\documentclass[12pt,a4paper]{report}

% dát pryč will, používat přítomnej čas

% Nejistý fráze který tam všude používám
% ... of a form ...
% Let us ...

%takle asi spíš psát ty anglický věty !!!!!!!!!!!!!!!!!!!!!! 
% takle nak to píše barendrecht :
%$\FV(M)$ the set of free variables of term M is defined inductively as follows.

% !!! místo we can atd psát one can ...
%      - ale ted to vypada že to neni tak žhavý pravidlo

% ujednotit i.e. , ty čaky kolem toho atd

% myslim že používam málo čárek, přečíst to a vygooglit kde mi příde že by mohla bejt

% ujednotit kurzívu a normální písmo v pseudocodu

% zčekovat: za any musí bejt plural, nejde any term ale musí bejt any terms atd
%           pro singular je a term.

% The general rule you can use is “that" is for restricting the meaning of the subject, whereas "which" is for decorating the subject without adding to the meaning. 
%http://www.wikihow.com/Use-%22That%22-and-%22Which%22-Correctly



\setlength\textwidth{145mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{15mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}


\usepackage[utf8]{inputenc}
\usepackage{qtree}

\usepackage{color}

\usepackage[ampersand]{easylist}

\usepackage{amssymb}

\usepackage[vlined]{algorithm2e}
%% \usepackage{algpseudocode}
\usepackage{framed}

\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}


\newcommand{\Lets}{Let us\xspace}
\newcommand{\lets}{let us\xspace}
\newcommand{\lterm}{$\lambda$-term\xspace}
\newcommand{\lterms}{$\lambda$-terms\xspace}
\newcommand{\lhead}{$\lambda$-head\xspace}
\newcommand{\lheads}{$\lambda$-heads\xspace}

\newcommand{\la}{\leftarrow\xspace}
\newcommand{\Lp}  {\Lambda^{\prime}\xspace}


\newcommand{\tur}[3]{#1\vdash{}#2:#3}

\newcommand{\turst}[3]{$#1\vdash{}#2:#3$\xspace}
\newcommand{\GMS}{\turst{\Gamma}{M}{\sigma}}

\newcommand{\atTree}{@-tree\xspace}


%\newcommand{\EA}{EA\xspace} % TODO nahradit správnym "evoluťionar algorithms"

\newcommand{\setDots}[2]{ 
	\lbrace #1 , \dots , #2 \rbrace
}

\newcommand{\lh}[1]{\lambda #1}


\newcommand{\Pseudokod}[2]{
	\begin{framed}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\SetKwProg{Fn}{function}{}{}
		\Fn{#1}{#2}
	\end{algorithm}
	\end{framed}
}

\newcommand{\pseudo}[1]{
	\begin{framed}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		#1
	\end{algorithm}
	\end{framed}
}

\newenvironment{enum}
{\begin{easylist}[itemize]}
{\end{easylist}}

\newenvironment{todo}
{ ~\\[0.5em]
  {\color{red}\textbf{TODO}}
  \begin{easylist}[itemize]}
{ \end{easylist}
  ~}



%% Balíček hyperref, kterým jdou vyrábět klikací odkazy v PDF,
%% ale hlavně ho používáme k uložení metadat do PDF (včetně obsahu).
%% POZOR, nezapomeňte vyplnit jméno práce a autora.
%\usepackage[ps2pdf,unicode]{hyperref}   % Musí být za všemi ostatními balíčky
%\hypersetup{pdftitle=Typed Functional Genetic Programming}
%\hypersetup{pdfauthor=Tomáš Křen}

% Tato makra přesvědčují mírně ošklivým trikem LaTeX, aby hlavičky kapitol
% sázel příčetněji a nevynechával nad nimi spoustu místa. Směle ignorujte.
\makeatletter
\def\@makechapterhead#1{
  {\parindent \z@ \raggedright \normalfont
   \Huge\bfseries \thechapter. #1
   \par\nobreak
   \vskip 20\p@
}}
\def\@makeschapterhead#1{
  {\parindent \z@ \raggedright \normalfont
   \Huge\bfseries #1
   \par\nobreak
   \vskip 20\p@
}}
\makeatother


\newcommand\Vtextvisiblespace[1][.3em]{%
  \mbox{\kern.06em\vrule height.3ex}%
  \vbox{\hrule width#1}%
  \hbox{\vrule height.3ex}}

\title{Typed Functional Genetic Programming}
\author{Tomáš Křen}
\date{Prague 2013}

\begin{document}

% Trochu volnější nastavení dělení slov, než je default.
\lefthyphenmin=2
\righthyphenmin=2

%%% Titulní strana práce

\pagestyle{empty}
\begin{center}

\large

Charles University in Prague 

\medskip

Faculty of Mathematics and Physics

\vfill

{\bf\Large MASTER THESIS}

\vfill

%%% \centerline{\mbox{\includegraphics[width=60mm]{../img/logo.eps}}}

%\begin{figure}[!ht]
%  \centering
%  \includegraphics{logo.eps}
%  \caption{Default}\label{fig:default}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stačí todle odkomentovat a dát nahoře LaTeX (F2) , pak DVI->PDF (F9)  a pak View PDF
%\includegraphics[scale=0.5]{logo.eps}
\includegraphics[scale=0.15]{logomff.png}

\vfill
\vspace{5mm}

{\LARGE Tomáš Křen}

\vspace{15mm}

% Název práce přesně podle zadání
{\LARGE\bfseries Typed Functional Genetic Programming}

\vfill

% Název katedry nebo ústavu, kde byla práce oficiálně zadána
% (dle Organizační struktury MFF UK)
%%%%Name of the department or institute
%Department of Theoretical Computer Science and Mathematical Logic\\
%{\small Department of Theoretical Computer Science and Mathematical Logic} \\
{\fontsize{0.46cm}{1em}\selectfont 
Department of Theoretical Computer Science and Mathematical Logic}

\vfill

\begin{tabular}{rl}

Supervisor of the master thesis: & RNDr. Petr Pudlák, Ph.D. \\
\noalign{\vspace{2mm}}
Study programme: & Computer Science \\ %Teoretická informatika \\
\noalign{\vspace{2mm}}
Specialization: & Theoretical Computer Science
%Neprocedurální programování a umělá inteligence \\
%{\fontsize{0.36cm}{1em}\selectfont 
%Neprocedurální programování a umělá inteligence} \\
%Non-procedural Programming and Artificial Intelligence} \\
%}\\
\end{tabular}

\vfill

% Zde doplňte rok
Prague 2013

\end{center}

\newpage

%%% Následuje vevázaný list -- kopie podepsaného "Zadání diplomové práce".
%%% Toto zadání NENÍ součástí elektronické verze práce, nescanovat.

%%% Na tomto místě mohou být napsána případná poděkování (vedoucímu práce,
%%% konzultantovi, tomu, kdo zapůjčil software, literaturu apod.)

%% on tam měl %% \openright %!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\noindent
%$<$dedication$>$
I would like to express my sincere gratitude to my supervisor RNDr. Petr Pudlák, Ph.D.
for the useful comments, remarks and willingness to help me with 
this thesis.
I would also like to thank my parents, who have supported me throughout
my studies and whole life. 

\newpage

%%% Strana s čestným prohlášením k diplomové práci

\vglue 0pt plus 1fill

\noindent
I declare that I carried out this master thesis independently, and only with the cited
sources, literature and other professional sources.

\medskip\noindent
I understand that my work relates to the rights and obligations under the Act No.
121/2000 Coll., the Copyright Act, as amended, in particular the fact that the Charles
University in Prague has the right to conclude a license agreement on the use of this
work as a school work pursuant to Section 60 paragraph 1 of the Copyright Act.

\vspace{10mm}

\hbox{\hbox to 0.5\hsize{%
In ........ date ............
\hss}\hbox to 0.5\hsize{%
signature of the author
\hss}}

\vspace{20mm}
\newpage


%%% Povinná informační strana diplomové práce

\vbox to 0.5\vsize{
\setlength\parindent{0mm}
\setlength\parskip{5mm}

Název práce:
Typed Functional Genetic Programming
% přesně dle zadání

Autor:
Tomáš Křen

Katedra:  % Případně Ústav:
Katedra teoretické informatiky a matematické logiky
% dle Organizační struktury MFF UK

Vedoucí diplomové práce:
RNDr. Petr Pudlák, Ph.D., 
Katedra teoretické informatiky a matematické logiky
%Jméno a příjmení s tituly, pracoviště
% dle Organizační struktury MFF UK, případně plný název pracoviště mimo MFF UK

Abstrakt:
% abstrakt v rozsahu 80-200 slov; nejedná se však o opis zadání diplomové práce
{\fontsize{0.35cm}{1em}\selectfont 
V této práci je představen design a implementace
systému řešícího úlohu genetického programování v
simply typed lambda kalkulu. Je zde představena
metoda inicializace počáteční populace založená 
na technice produkující typované lambda termy 
v dlouhé normální formě. Tato metoda je parametrizována
jednoduchou prohledávací strategií. Několik takových
prohledávacích strategií je představeno, jako například
strategie pro systematické generování nebo strategie
odpovídající standardní ramped half-and-half metodě.
Další z představených strategií, strategie jménem 
geometrická strategie je blíže podrobena experimentům,
které ukáží že má několik žádoucích efektů na průběh evoluce, 
jakými jsou zlepšení míry úspěšnosti, nižší časové nároky 
a menší průměrnou velikost termů v porovnání se standardní
ramped half-and-half metodou generování jedinců. Další výkonnostní
zlepšení jsou navržena a podpořena experimenty, jedná se o
$\eta$-normalizaci vygenerovaných jedinců a @-tree
reprezentaci jedinců. Použitý proces eliminace abstrakcí
umožňuje použití jednoduchého podstromy měnícího 
křížení.}


Klíčová slova:
genetické programovaní, funkcionalní programovaní
% 3 až 5 klíčových slov

\vss}\nobreak\vbox to 0.49\vsize{
\setlength\parindent{0mm}
\setlength\parskip{5mm}

Title:
Typed Functional Genetic Programming
% přesný překlad názvu práce v angličtině

Author:
Tomáš Křen

Department:
Department of Theoretical Computer Science and Mathematical Logic
% dle Organizační struktury MFF UK v angličtině

Supervisor:
RNDr. Petr Pudlák, Ph.D., 
Department of Theoretical Computer Science and Mathematical Logic
%Jméno a příjmení s tituly, pracoviště
% dle Organizační struktury MFF UK, případně plný název pracoviště
% mimo MFF UK v angličtině

Abstract:
{\fontsize{0.35cm}{1em}\selectfont 
In this thesis is presented design and implementation
of a system performing genetic programming in simply
typed lambda calculus. Population initialization
method based on term generating technique producing
typed lambda terms in long normal form is introduced. 
This method is parameterized by simple search strategy.
Several search strategies are presented, such as
strategy for systematic generation or strategy
corresponding to standard ramped half-and-half method.
Another such a strategies called \textit{geometric} strategy
is further examined in experiments and shown
to have various desirable effects such as
improved success rate, lesser time consumption and 
smaller average term size in comparison with standard 
ramped half-and-half generating method. Other performance
enhancements are proposed and supported by experiments
such as $\eta$-normalization of generated individuals and 
@-tree representation of individuals. Abstraction elimination
is utilized to enable use of simple tree-swapping crossover.
}
  
% abstrakt v rozsahu 80-200 slov v angličtině; nejedná se však o překlad
% zadání diplomové práce


Keywords:
genetic programming, functional programming
% 3 až 5 klíčových slov v angličtině

\vss}

\newpage


%%% Strana s automaticky generovaným obsahem diplomové práce. U matematických
%%% prací je přípustné, aby seznam tabulek a zkratek, existují-li, byl umístěn
%%% na začátku práce, místo na jejím konci.

%% on tam měl %% \openright %!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\pagestyle{plain}
\setcounter{page}{1}


\tableofcontents
	
%\chapter{Introduction}
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

%	Seznamte se s typovanými lambda kalkuly [1] 
%	a s jejich praktickou implementací [5]. 
%	Prostudujte standardní techniky genetického 
%	programování [2-4]. Navrhněte a implementujte 
%	prototyp systému řešící úlohu genetického 
%	programování nad typovaným funkcionálním 
%	jazykem. Porovnejte dosažené výsledky 
%	v závislosti na zvolených vstupních parametrech. 


Genetic programming (GP) is an AI technique, which falls into broader category 
of evolutionary algorithms  ---  mataheuristic  search algorithms inspired  by 
biological evolution by natural selection. I dare say many people perceive its beauty in the  fact that GP is a computer program  constructing other computer programs %(property sometimes called \textit{automatic programming}) 
with desired properties by breeding them. 
It is also pretty successful technique in a number of areas \cite{koza05}. 
And perhaps that is why GP has become very popular.    

GP constructs the population of solution programs from 
two sets of building blocks; set $F$ of functions 
and set $T$ of constants and variables. 
In standard GP \textit{closure} of those 
building blocks is required.
This means that all functions in $F$ must accept all 
constants and variables from $T$ as their arguments and produce 
such values that are also acceptable by all functions in $F$ as arguments.
Main motivation behind this thesis is to design and implement a system that
eliminates this constraint. 
This can be achieved by utilization of \textit{types}.
Utilization of types in GP is not a new idea; 
it was dealt with in various previous works 
\cite{yu01,montana95,haynes96,olsson94}.\\

%The contribution of this thesis can be seen 
This thesis deals with this task using concepts from
theory of typed $\lambda$-calculus and combinatory logic
and shows how to use them in GP. 
Hopefully, it is also shown that those two fields are
nicely linked together.
%, e.g., the concept of \textit{building blocks}
%corresponds to concept of \textit{context}.
Several enhancements are derived from this theoretical background
and shown to have positive effect on performance of our system.  
Our main focus lies in the process of generating the initial population.
 
Design of a GP system utilizing these methods is presented.
This design aims to be generalization of standard GP 
developed by Koza\cite{koza92}, rather than being 
specialized variant of GP using more recent techniques.
This decision is based on belief
that simplicity of design of standard GP plays nicely
with testing those $\lambda$-calculus concepts in 
the evolutionary context and on belief that modern specialized 
techniques can be incorporated later.\\
  
Notable highlights of the thesis are: 
\begin{itemize}
 \item \textit{Inhabitation tree} 
 %(a slight modification 
 %  of \textit{inhabitation machine}\cite{barendregt10}
 %  suited better for reasoning about implementation details) 
   is presented and used for \textit{generation of individuals}.
   This generation process is parameterized by simple 
   \textit{search strategy}. Several search strategies are presented,
   such as \textit{systematic} strategy performing exhaustive enumeration
   of individuals and strategy performing generalization of 
   standard GP term generating method \textit{ramped half-and-half}.  
 \item One of proposed search strategies, the \textit{geometric} strategy, is
   examined in a couple of experiments and shown to have 
   several desirable properties.
 \item The \textit{$\eta$-normalization} of generated individuals
       and \textit{\atTree} representation of individuals, 
       two enhancements of the system inspired from the theoretical
       background are examined and shown to have 
       improving effect on performance of the system.     
 \item \textit{Abstraction elimination} is utilized
   to enable use of simple \textit{mutation operator}.
   Both simple and optimize versions of this transformation are examined.
% \item \textit{Polymorphic} generalization of inhabitation tree,
%   enabling use of polymorphic functions as building blocks, 
%   is presented. 

\end{itemize}

%The implementation of our system aims 
Described system is implemented in purely functional language 
\textit{Haskell}. Evaluation of individual \lterms is 
performed by translating them to
Haskell programs and evaluating them in Haskell interpreter. 
The core part of the system runs as server, which is 
managed through user interface accessible via
web browser. Solution individuals are also translated
to \textit{JavaScript} expressions, which enables  
an interactive solution presentation.
This approach helps us with demonstrating 
that \lterms can be easily translated to a 
widely used programming language.\\

Several example problems for the designed system are presented 
and examined. Those examples are not intended to 
bring breakthrough results; they are intended to 
serve as demonstration of that the system is working properly,
as instrument for examining presented techniques and 
as demonstration of several directions in which the system
can be used.      


\chapter{Genetic Programming}
\label{GP}

\textit{Genetic programming} is a technique inspired by biological evolution
that for a given problem tries to find computer programs able to solve that problem. 
GP was developed by John Koza in 1992 \cite{koza92}.

A problem to be solved is given to GP in a form of \textit{fitness function}. 
Fitness function is a function which takes computer program as its input and 
returns numerical value called \textit{fitness} as output. 
The bigger fitness of a computer program is, the better solution of a problem.

GP maintains a collection of computer programs called \textit{population}. 
A member of population is called \textit{individual}. 
By running GP algorithm evolution of those individuals is performed.

Individuals are computer program \textit{expressions} kept as \textit{syntactic trees}. 
Basically those trees are rooted trees with a function symbol in each internal node 
and with constant symbol or variable symbol in each leaf node. 
Number of child nodes for each internal node corresponds to the number of arguments of a function whose symbol is in that node.

Another crucial input besides fitness function is a collection of 
\textit{building blocks}\footnote{Throughout this thesis, 
we use the term \textit{building blocks} 
differently than it is sometimes used in GP context. 
Hopefully it will not confuse the reader.}.
It is a collection of symbols (accompanied with an information about number of arguments).
Those symbols are used to construct trees representing individuals.\\

\Lets describe GP algorithm briefly.
At the beginning, initial population is generated from building blocks randomly.
A step of GP algorithm is stochastic transformation of the current population into 	
the next population.

This step consists of two sub steps:
\begin{itemize} 
	\item Selection of \textit{parents} for individuals of the next population based on the fitness.
	      The bigger fitness of an individual of the current population is, 
	      the better chance of success being selected as parent it has.  
	\item Application of genetic operators (such as \textit{crossover}, 
	      \textit{reproduction} and \textit{mutation}) 
		  on parent individuals producing new individuals of the next population.  
\end{itemize}	  
This transformation is repeatedly applied for a predefined number of steps (which is called 
number of \textit{generations}) or until some predefined criterion is met.	
\\\\
\Lets now look on GP in greater detail. 

\newpage
\section{Program trees}
\label{GP-prog-trees}

In GP programs are represented as \textit{S-expressions}. 
\Lets define S-expression inductively:

\begin{samepage}
\begin{enumerate}
	\item Constant or variable symbol $s$ is S-expression.\footnote{
	      By constants we also mean procedures with zero arguments.}
	\item Let there be a function with symbol $f$ which has $n$ arguments.
	 
	      And let there be S-expressions $e_{1}, ..., e_{n}$. 
	      
	      Then ( $f$ $e_{1}$ ... $e_{n}$ ) is expression.\footnote{
	      This notation comes from Lisp programming language, 
	      a more standard notation would be $f(e_{1}, ... ,e_{n})$.}   
\end{enumerate}
\end{samepage}~\\
There is straightforward tree representation corresponding to these two cases:

\begin{enumerate}
	\item One node tree $s$.
    \item \mbox{\Tree
			[.$f$	
		 		\text{$t_{1}$}
		 		\text{...}
		 		\text{$t_{n}$} ]}\\\\
		 where $t_{1}, ..., t_{n}$ are trees corresponding 
		 to S-expressions $e_{1}, ..., e_{n}$.	   
\end{enumerate}

Computer program with inputs $x_{1}, ..., x_{n}$ is realized as expression in which 
may occur variables $x_{1}, ..., x_{n}$.



\section{Building blocks}
\label{building-blocks}

Set of building blocks consists of two sets.

\begin{itemize}
	\item \textit{Terminal set $T$}: Set of symbols used as leaf nodes of 	               
	      program trees standing for constants and variables.
	\item \textit{Function set $F$}: Set of symbols used as internal nodes 
	      of program trees standing for functions.
\end{itemize}

\newcommand{\TuF}{$T \cup F$\xspace}

Therefore $building$ $blocks = $ \TuF.\\

For each symbol in \TuF that is not variable 
there must also be an implementation.
And for every function symbol from $F$ there must be specified 
number of arguments.\\

There is one important constrain on function implementations for functions from F:
There must be type $A$ that for every function symbol $f \in F$ the corresponding function implementation standing behind this symbol must be of a type 
$A \times ... \times A \rightarrow A$ and must be total (defined on all
combinations of inputs). And analogically for $t \in T$ being of a type A.  

Satisfying this constrain ensures that every program tree build 
from \TuF will be total. We will refer to this constrain as to \textit{closure}.\\

We can say that the motivation behind this thesis is to construct system,
where building blocks may be of any type. 

\section{Generating individuals}
\label{GPgene}

We will describe tree generating method described by Koza \cite{koza92}
called \textit{ramped half-and-half}. \\

An individual tree is constructed by random (uniform) selection of symbol from 
subset of \TuF for the root node and after that by generation of its 
subtrees recursively. 
Number of subtrees of a node corresponds 
to the number of arguments for the selected symbol. 

There are two generating sub-methods called \textit{full} and 
\textit{grow}; each of them restricting differently the subset of \TuF. 
This restriction depends on depth of the
node for which the symbol is being selected. 

In order to perform \textit{ramped half-and-half} method
one of those two sub-methods is randomly (uniformly) 
selected and the selected one is performed.
This selection is done for each generated tree, so there
is 50\% chance for tree to be generated by \textit{full} 
and 50\% chance for tree to be generated by \textit{grow}.\\


A maximum depth $D_{initial}$ is defined (e.g. $D_{initial} = 6$).

For each tree is selected its 
depth $d$ from $\setDots{2}{D_{initial}}$ randomly (uniformly).\\


In the root (\textit{depth} = 0) both \textit{full} and \textit{grow}
select the symbol from the set $F$.

In nodes with \textit{depth} $< d$ the \textit{full}
method selects from the set $F$, whereas the \textit{grow} method 
selects from whole \TuF.

In nodes with \textit{depth} $= d$ both methods select from the
set $T$.

Generated individual is added into the initial population only if 
the initial population does not yet contain this individual.
This way it is guaranteed that every individual is unique 
in the initial population.
  
\section{Evaluation}

Evaluation is process of assigning fitness value to all 
individuals in the population. It is also usual to check
individuals for being correct solution.
If such a correct solution is find, the algorithm terminates. 
In our implementation fitness function returns
pair of real number (fitness value) and boolean value (indicating
whether tested individual is correct solution). 

 
\newpage

\section{Selection}

In the field of evolutionary algorithms there is plenty of 
options for selection mechanisms 
for us to choose from. Again we will describe mechanism Koza
used in his first book on GP. It is the \textit{Roulette selection}.
It uses fitness value of each individual in straightforward way to determine probability of selecting this individual.\\
Let there be $popSize$ individuals in the population.\\
And let $f_{i}$ be fitness value for individual i 
where $i \in \setDots{1}{popSize}$. 

Then $p_{i}$ probability of selection of individual $i$ is computed
as follows:

$$ p_{i} = \dfrac{ f_{i}  }{ \sum\limits_{j=1}^{popSize}{f_{j} }  } $$

\section{Crossover}
\label{GPxover}

For Koza the most important genetic operator in GP is 
crossover. It is operator inspired by sexual reproduction
occurring in the nature. Generally speaking crossover takes
two (parent) individuals and combines theirs genomes to produce 
two possibly new (offspring) individuals.   

In GP the most common crossover is \textit{Subtree swapping crossover}.
This crossover randomly selects one node in each parent tree.

With probability $p_{ip}$ (e.g. $p_{ip} = 90\%$) the crossover node
is randomly (uniformly) selected from internal nodes (i.e. function nodes) and
with probability $1 - p_{ip}$ (e.g 10\%) from leaf nodes.
This is done for both parents.
   

Two new child individuals are constructed by swapping subtrees 
that have roots in those selected nodes.\\

Example should clarify this process. Here are two parent trees with 
selected nodes in bold:

\Tree [.$ifneq$ $1$
		 	   [.\textbf{iflt} $0$ $x$ [.$-$ $0$ $x$ ] $1$ ]
		 	   [.$+$ \text{$x$} \text{$2$} ]
		 	   $1$ ]
\Tree [.$\%$ \text{$x$}
         	 [.\textbf{ifeq} \text{$1$} \text{$x$} \text{$x$} \text{$0$} ] ]\\

And here are two child trees with swapped subtrees:\\

\Tree [.$ifneq$ $1$
		 	   [.\textbf{ifeq} \textbf{1} \textbf{x} \textbf{x} 
		 	     \textbf{0} ]
		 	   [.$+$ \text{$x$} \text{$2$} ]
		 	   $1$ ]
\Tree [.$\%$ \text{$x$}
         	 [.\textbf{iflt} \textbf{0} \textbf{x} 
         	   [.\textbf{-} \textbf{0} \textbf{x} ] \textbf{1} ] ]\\


A maximum permissible depth $D_{created}$ 
for offspring individuals is defined (e.g. $D_{created} = 17$).
If one of the offspring has greater depth than this limit, then 
this offspring is replaced by the first parent in the result of 
the crossover operator. If both offspring exceeds this limit, than 
both are replaced by both parents.  



\section{Reproduction}

Reproduction is simple mechanism providing preservation of solutions
from the current population to the next one. It simply copies 
one individual to the next population.

\section{Mutation}

Mutation is genetic operator modifying one individual.
There are many options for mutation mechanisms 
for us to choose from. Here will be described 
\textit{Subtree generating mutation} 
witch uses mechanism for generating individuals.

This mutation randomly selects one node in the individual tree.
New mutant individual is constructed by replacement of 
subtree with root in the selected node by new generated
tree. This new tree is generated by mechanism for generating 
individuals.

Analogically as for crossover, a maximum permissible depth for mutant individual 
is defined.

Similarly as Koza in \cite{koza92}, we do not use the mutation operator
in this thesis. This does not mean that I do not like it. 
Conversely, I am looking forward too its enhancing effect in the future work.

\section{Construction of next population}

Let $pop_{t}$ be the current population that we want to 
transform into the next population $pop_{t+1}$. 
We start by initializing $pop_{t+1}$ by empty population.

Then we iteratively fill $pop_{t+1}$ by individuals 
returned by genetic operators.

In each iteration one genetic operator is randomly selected.
Then required amount of individuals for the operation is selected
by individual selection mechanism described above (one or two
individuals for our genetic operators).
And those selected individuals are used as input for selected
genetic operator which produces some new individuals.
Those new individuals are inserted into $pop_{t+1}$.

This process continues until $pop_{t+1}$ is filled with
$popSize$ individuals.  

Selection of genetic operation in each operation is 
controlled by probabilities for each genetic operator.\\

Koza\cite{koza92} uses as default probabilities those values:

\begin{itemize}
	\item \textit{Crossover}    : $90\%$
	\item \textit{Reproduction} : $10\%$
	\item \textit{Mutation}     :  $0\%$
\end{itemize}



\newpage
\section{GP algorithm in pseudocode}

Bellow is described GP algorithm in pseudocode. 
It is slight modification of the standard GP algorithm, 
because the best individual of the 
generation is preserved unchanged into the next generation.

\Pseudokod{GP( fitness, \TuF, popSize, numGens, probabs )}{
  $gen \leftarrow$ 0 \;
  $pop \leftarrow generateInitialPopuletion( $ \TuF$ ) $ \;
  ($popWithFitness,terminate,best) \leftarrow$ 
  evaluate($fitness$, $pop$)\;\; 	
	
  \While{ $gen < numGens$ $\wedge$ $\neg terminate$  }{  	
	$newPop \leftarrow$ empty population \;
	$newPop$.insert( $best$ )\;\;
	
	$i \leftarrow 1$ \;
	\While{ $i < popSize$ }{	
		$op \leftarrow probabilisticallySelectOperation(probabs)$ \;\;
				
		\Switch{op}{
		
			\Case{Crossover}{
				$parent1 \leftarrow$ selection( $popWithFitness$ ) \;
				$parent2 \leftarrow$ selection( $popWithFitness$ ) \;\;
				
				($child1$,$child2$) = crossover( $parent1$ , $parent2$ )\;\;
				
				$newPop$.insert( $child1$ ) \;
				$newPop$.insert( $child2$ ) \;
				$i \leftarrow i + 2$ \;
			}
			\Case{Reproduction}{
				$indiv \leftarrow$ selection( $popWithFitness$ ) \;
				$newPop$.insert( $indiv$ ) \;
				$i \leftarrow i + 1$ \;
			}
			\Case{Mutation}{
				$indiv \leftarrow$ selection( $popWithFitness$ ) \;
				$mutant \leftarrow$ mutate( $indiv$ , \TuF ) \;
				$newPop$.insert( $mutant$ ) \;
				$i \leftarrow i + 1$ \;	
			}		
		}
	}\;
	
	$pop \leftarrow newPop$  \;
	($popWithFitness,terminate,best) \leftarrow$ 
	evaluate($fitness$, $pop$)\;
	$gen \leftarrow gen + 1$ \; 
 }\;
 
 \Return $popWithFitness$ \;
}

\begin{samepage}
\Lets clarify the input arguments:
\begin{itemize}
	\item \textit{fitness} - Fitness function.
	\item \textit{\TuF} - Building blocks accompanied with an  	
	      information about implementations and argument numbers.
	\item \textit{popSize} - Population size.
	\item \textit{numGens} - Number of generations.
	\item \textit{probabs} - Genetic operators probabilities.
\end{itemize} 
\end{samepage}

%In order to clarify the code let us describe in greater detail inputs and

Behaviors of contained procedures are hopefully clear from verbal description.
\\[2em]

For more detailed information on subject of GP and its many variants
see \cite{fg} containing many references to other works. 
  




\chapter{Mathematical background}

\newcommand{\then}{\Rightarrow\xspace}

\newcommand{\lamb}[2]{( \lambda \, #1 \, . \, #2 )}
\newcommand{\lam}[2]{\lambda \, #1 \, . \, #2}

\newcommand{\ST}{\mathop{\mathrm{ST}}}
\newcommand{\FV}{\mathop{\mathrm{FV}}}

\newcommand{\Scomb }{\mathbf{S}}
\newcommand{\Kcomb }{\mathbf{K}}
\newcommand{\Icomb }{\mathbf{I}}


\newcommand{\bbarr}{\twoheadrightarrow_\beta}
\newcommand{\barr}{\rightarrow_\beta}
\newcommand{\beq}{=_\beta}

\newcommand{\eearr}{\twoheadrightarrow_\eta}
\newcommand{\earr}{\rightarrow_\eta}
\newcommand{\eeq}{=_\eta}

\newcommand{\bearr}{\rightarrow_{\beta\eta}}
\newcommand{\bbeearr}{\twoheadrightarrow_{\beta\eta}}
\newcommand{\beeq}{=_{\beta\eta}}


\newcommand{\etar}{\twoheadrightarrow_\eta}
\newcommand{\ered}{$\eta$-reduction\xspace}
\newcommand{\bnf}{$\beta$-\textit{nf}\xspace}
\newcommand{\enf}{$\eta$-\textit{nf}\xspace}
\newcommand{\eenf}{$\eta^{-1}$-\textit{nf}\xspace}
\newcommand{\beenf}{$\beta\eta^{-1}$-\textit{nf}\xspace}
\newcommand{\benf}{$\beta\eta$-\textit{nf}\xspace}
\newcommand{\bredex}{$\beta$-redex\xspace} 

\newcommand{\lnf}{\textit{lnf}\xspace}


In this chapter we briefly describe key concepts
of simply typed $\lambda$-calculus.
We try to favor readability over formality,
for more detailed and formal treatment of this subject see 
\cite{barendregt92,barendregt10}.

After introductory description, an analysis of term generation follows; 
a path from inference rules to term generating grammar rules is shown,
\textit{long normal form (lnf)} is described,
followed by term generating grammar for \textit{lnf}
and analysis of its relevant properties (such as its relation to \benf),
then \textit{inhabitation tree} is presented --- 
an implementation friendly refinement of 
\textit{lnf} term generating grammar.

	
\theoremstyle{plain} 
\newtheorem{theorem}{Theorem} 
\newtheorem{proposition}{Proposition} 
\newtheorem{lemma}{Lemma} 
\newtheorem*{corollary}{Corollary}

\theoremstyle{definition} 
\newtheorem*{definition}{Definition} 
\newtheorem{conjecture}{Conjecture}
 \newtheorem*{example}{Example} 

\theoremstyle{remark} 
\newtheorem*{remark}{Remark} 
\newtheorem*{note}{Note} 
\newtheorem{case}{Case}

		
\section{\lterms}
\label{deflam}

Here we  %formally 
describe programming language, 
in which we generate individual programs --- so called \lterms.  






\begin{definition}

Let $V$ be infinite countable set of {\it 
variable names}. Let $C$ be set of {\it constant names}, 
$V \cap C = \emptyset$.	 	
Then $\Lambda$ is set of {\it \lterms} defined inductively as follows.	
\begin{align*}
x   \in V \cup C  &\then x     \in \Lambda \\
M,N \in \Lambda   &\then (M~N) \in \Lambda 
\textit{~~~~~~(Function application)} \\
x   \in V , M \in \Lambda &\then \lamb{x}{M} \in \Lambda
\textit{~~~~($\lambda$-abstraction)} 
\end{align*}~
\end{definition}



\textit{Function application} and 
\textit{$\lambda$-abstraction} are concepts
well known from common programming languages. 
For example in JavaScript 
$(M~N)$ translates to expression \texttt{$M$($N$)} and
$\lamb{x}{M}$ translates to expression \texttt{function($x$)\{return $M$;\}}.
In other words, the function application 
corresponds to the act of supplying a function 
with an argument and
the $\lambda$-abstraction is equivalent to 
\textit{anonymous function}. \\

It is usual to not distinguish between $V$ and $C$ and just use
one set of variable names. But because this distinction is quite convenient
for us in the implementation, we use it even here. 
Hopefully, it will not confuse the reader.
 

We can use $V = \Sigma^+$, set of all non-empty finite strings of symbols 
from $\Sigma$ where $\Sigma$ is some alphabet set, e.g.  
$
\Sigma =
\setDots{a}{z}$.

And $C$ is set supplied by "the user" to enrich 
the language with constant names standing
for some predefined behavior.

A constant may stand just for another \lterm
or it may stand for some predefined constant 
such as 0,1,2,... and primitive operations on
them, e.g. addition. 
But the specific implementations 
of these constants will not be ours big concern 
while reasoning about methods for generating 
\lterms .\\

We use upper case symbols such as 
$M,N,M_1,M_2,...,N_1,N_2,...$
to denote arbitrary lambda terms in contrast with
those in lower case such as
$a,b,c,...,x,y,z,$
$f_1,f_2,...,g_1,g_2,...$
denoting variable names, elements of $V$.

Sometimes we will use lower case symbols
to denote arbitrary variable names (such as
$x$ in $\lamb{x}{M}$),
and at other times, we will use them
as specific variable names in specific terms 
(such as $x$ in $\lamb{x}{x}$).
The distinction between the two should be
clear from the context.

We use bold upper case symbols such as 
$\Scomb, \Kcomb, \Icomb$ as abbreviation
for specific fixed terms, e.g. 
$\Icomb$ always means $\lamb{x}{x}$.  
\\[1em]

We use following notation
abbreviations for better readability. \vspace{3mm}

\begin{easylist}[enumerate]
& $M_1~M_2~M_3~\dots~M_n$ for 
  $(\dots((M_1~M_2)~M_3)~\dots~M_n)$ 
& $\lam{x_1 x_2 \dots x_n }{M}$ for
  $\lamb{x_1}{\lamb{x_2}{\dots\lamb{x_n}{M}\dots}}$
\end{easylist}~
  
For example we can write
$\lamb{f}{\lamb{g}{\lamb{x}{((f~x)\,(g~x))}}}$
as $\lam{f\,g\,x}{f~x\,(g~x)}$.

\newcommand{\ods}{\hspace{3mm}}

~

Here follow some examples of \lterms. \vspace{1mm}

\ods $x$

\ods $\lamb{x}{x}$

\ods $\lamb{x}{f}$

\ods $(\lamb{x}{(x~x)}~\lamb{x}{(x~x)})$

\ods $\lamb{x}{\lamb{y}{x}}$

\ods $\lamb{f}{\lamb{g}{\lamb{x}{((f~x)~(g~x))}}}$


\subsection{ Subterms, free variables and substitution }

In order to demonstrate what is a subterm of a term
\lets define inductively $\ST(M)$ set of all subterms of a term $M$.
\begin{align*}
\ST(x)          &= \{x\} \\
\ST((P~Q))      &= \{(P~Q)\} \cup \ST(P) \cup \ST(Q) \\
\ST(\lam{x}{P}) &= \{\lam{x}{P}\} \cup \ST(P) 
\end{align*}

%takle asi spíš psát ty anglický věty !!!!!!!!!!!!!!!!!!!!!! 
% takle nak to píše barendrecht :
 
$\FV(M)$ the set of free variables of a term M is defined inductively as follows.
\begin{align*}
\FV(x)          &= \{\}                        &\textbf{if } x \in C  \\
\FV(x)          &= \{x\}                       &\textbf{if } x \in V  \\
\FV((P~Q))      &= \FV(P) \cup \FV(Q)          &          \\
\FV(\lam{x}{P}) &= \FV(P) \smallsetminus \{x\} &
\end{align*}

A variable in \lterm $M$ is called \textit{bound} if it is not free.

\lterm M is called \textit{combinator} if $\FV(M)=\emptyset$.\\


Notable examples of combinators are combinators
$\Scomb$, $\Kcomb$ and $\Icomb$.
\begin{align*}
\Scomb &= \lam{f\,g\,x}{f\,x\,(g\,x)} \\
\Kcomb &= \lam{x\,y}{x} \\
\Icomb &= \lam{x}{x} 
\end{align*}

As will be shown in \ref{toSKI}
these combinators will help us in performing crossover of \lterms.\\

$M[x:=N]$ the substitution of a term $N$ for the free occurrences of 
a variable $x$ in a term $M$ is defined inductively as follows.
\begin{align*}
x[x:=N]           &= N \\
y[x:=N]           &= y &\text{ where } x \not= y  \\
(P~Q)[x:=N]       &= ( P[x:=N]  ~ Q[x:=N] ) \\
\lamb{x}{P}[x:=N] &= \lam{x}{P}\\
\lamb{y}{P}[x:=N] &= \lam{y}{( P[x:=N] )} &\text{ where } x \not= y, y\not\in \FV(N)\\
\end{align*}

In order to satisfy the $y\not\in \FV(N)$ condition, names of bound variables
are chosen such that they differ from the free ones in the term.



\subsection{$\beta$-reduction}

In order to perform computation there must be some
mechanism for term evaluation. In $\lambda$-calculus there
is $\beta$-reduction for this reason.\\

\newcommand{\bRedex}{$\beta$-redex\xspace}
\newcommand{\bRedexes}{$\beta$-redexes\xspace}
\newcommand{\bArrow}{\rightarrow_\beta\xspace}
\newcommand{\eArrow}{\rightarrow_\eta\xspace}
\newcommand{\eeArrow}{\rightarrow_{\eta^{-1}}\xspace}

A term of a form $\lamb{x}{M}N$ is called \textit{\bRedex}.
A \bRedex can be $\beta$-reduced to term $M[x:=N]$. 
This fact is written as \textit{relation} $\bArrow$ 
of those two terms:
\begin{equation} \label{eq:bRed}
\lamb{x}{M}N \bArrow M[x:=N]
\end{equation}
It is also possible to reduce \textit{subterm \bRedexes} 
which can be formally stated as:
\begin{align*}
P \bArrow Q &\then (R~P)      \bArrow (R~Q) \\
P \bArrow Q &\then (P~R)      \bArrow (Q~R) \\
P \bArrow Q &\then \lam{x}{P} \bArrow \lam{x}{Q}  
\end{align*}

In other words, $\beta$-reduction is the process 
of insertion of arguments supplied to a function into 
its body. \\

Another useful relations are $\bbarr$ and $\beq$ defined as follows. 

\begin{enumerate}
 \item \begin{enumerate}
 	\item $M \bbarr M$
 	\item $M \barr N \then M \bbarr N$
 	\item $M \bbarr N , N \bbarr L \then M \bbarr L$ 	
 \end{enumerate}
 \item \begin{enumerate}
 	\item $M \bbarr N \then M \beq N$
 	\item $M \beq N \then N \beq M$
 	\item $M \beq N , N \beq L \then M \beq L$
 \end{enumerate}

\end{enumerate}

We read those relations as follows.
\begin{enumerate}
 	\item $M \bbarr N$ --- "$M$ $\beta$-reduces to $N$."  
 	\item $M \barr N$  --- "$M$ $\beta$-reduces to $N$
 	      in one step."
 	\item $M \beq N$ --- "$M$ is $\beta$-convertible to $N$."	
 \end{enumerate}



\subsection{$\eta$-reduction}

Similarly as for $\beta$-reduction we can define $\eta$-reduction 
except that instead of \ref{eq:bRed} we use:  

$$\lamb{x}{(M~x)} \eArrow M \textbf{ ~~~~if } x \not\in FV(M) $$~

Analogically, a term of a form $\lamb{x}{(M~x)}$ is called 
\textit{$\eta$-redex}.

Relation $\bearr\;=\;\barr \cup \earr$. 
(Relation $R = \{\;(a,b)\;|\;a\;R\;b\;\}$.)

Similarly as for $\bbarr$ and $\beq$ we can define relations 
$\eearr$, $\eeq$,$\bbeearr$ and $\beeq$.


\subsection{$\eta^{-1}$-reduction}

$\eta^{-1}$-reduction (also called $\eta$-expansion) is 
the reduction converse to $\eta$-reduction.
Again it may be obtained by replacing \ref{eq:bRed}, now with:  

$$M \eeArrow \lamb{x}{(M~x)} \textbf{ ~~~~if } x \not\in FV(M) $$




\subsection{$\beta$-normal form}

~\begin{easylist}[enumerate]
& A \lterm is a \textit{$\beta$-normal form} (\bnf) 
if it does not have a $\beta$-redex as subterm.
& A \lterm M \textit{has} a \bnf if $M \beq N$
and $N$ is a \bnf.\\
\end{easylist}
A normal form may be thought of as a result of a term evaluation. 

Similarly we can define \enf and \benf.

\subsection{Tree representations of \lterms}
\label{tree-reps}

\newcommand{\sexprTree}{sexpr-tree\xspace}
\newcommand{\SexprTree}{Sexpr-tree\xspace}



From the definition of \lterm we can straightforwardly derive 
the standard tree representation for \lterms, which we call 
\textit{\atTree}. Term M is 
translated into \atTree $T_@[M]$ by following rules.


\begin{enumerate}
	\item $T_@[x] := x$ (i.e. $x \in V \cup C$ becomes one node tree $x$)
	\item \mbox{$T_@[(P$ $Q)] := $ \Tree
			[.@	
		 		\text{$T_@[P]$}
		 		\text{$T_@[Q]$}		 			
			] }
	\item \mbox{$T_@[\lambda x . P] := $ \Tree
			[.\text{$\lambda x$}	
		 	 	\text{$T_@[P]$}	
			] }
\end{enumerate}


We can enhance this representation by replacing third rule
by following one. It compressing consecutive 
$\lambda$-abstractions into one \atTree node. \\

\mbox{$T_@[\lambda x_1 \dots x_n . P] := $ \Tree
	[.\text{$\lambda x_1 \dots x_n$ }	
 	 	\text{$T_@[P]$}	
	] } 
	
	~\\

Since this representation comes directly from definition, 
it is evident that it covers all possible \lterms.\\
 
Following tree representation is extension of 
\textit{S-expression trees} described in~\ref{GP-prog-trees},
thus we call it \textit{\sexprTree}.
Unlike \atTree, it is not able to cover all \lterms, but only those
in \bnf. 
Term M is translated into \sexprTree $T_S[M]$ by following rules.

%It will also be the representation suitable for showing 
%that \textit{solving} Inhabitation tree generates wanted \lterm.


\begin{enumerate}
    \item $T_S[x] := x$
	\item \mbox{ $T_S[(f M_1 M_2 \dots M_m)] := $ 
	%where $f \in V \cup C, n \geq 1$ translates into tree\\
		\Tree
			[.f	
		 		\text{$T_S[M_1]$}
		 		\text{$T_S[M_2]$}
		 		\text{$\dots$}
		 		\text{$T_S[M_m]$}		 				 			
			] }
	\item \mbox{$T_S[\lambda x_1 \dots x_n . M] := $
	 	\Tree
			[.\text{$\lambda x_1 \dots x_n$ }	
		 	 	\text{$T_S[M]$}	
			] }
\end{enumerate}~




\newpage
\section{Types}
\label{deftype}

\newcommand{\ar}{\rightarrow\xspace}
\newcommand{\T}{\mathbb{T}\xspace}

A \lterm as described above
corresponds to a program expression with no type information
included. Now we will describe \textit{types} (or \textit{type terms}).
After putting those two pieces 
(\textit{\lterms} and \textit{types}) together 
we will get system called \textit{simply typed $\lambda$-calculus}.


\begin{definition}
Let $A$ be set of {\it atomic type names}. 
Then $\mathbb{T}$ is set of {\it types} inductively defined as follows.
\begin{align*}
\alpha      \in A  &\then   \alpha \in \T \\
\sigma,\tau \in \T &\then ( \sigma \ar  \tau ) \in \T 
\end{align*}~

\end{definition}

Type $\sigma \ar \tau$ is type for functions taking as input
something of a type $\sigma$ and returning 
as output something of a type $\tau$. 

Usually A is called \textit{set of type variables}, 
but since it might be source of confusion with polymorphic type variables
we prefer this name, which also better suits our usage. 

Specific definition of $A$ is not so important, but for practical purposes 
we can think of it as the set of all non-function types relevant in our
situation, e.g., $A = \{ Int , IntList , Bool, BoolList , Char, String \}$.
Or we can think of $A$ more generally as of set of all non-empty strings
of symbols of some alphabet. \\

We use following notation
abbreviation for better readability:

$\tau_1 \ar \tau_2 \ar \dots \ar \tau_n$ means 
$\tau_1 \ar (\tau_2 \ar (\dots \ar (\tau_{n-1} \ar \tau_n)\dots))$.\\

In other words, the operator $\ar$ is right-associative.\\

It is easy to see that every $\sigma \in \T$ may be expressed as 
$\tau_1 \ar \dots \ar \tau_n \ar \alpha$ 
where $\alpha \in A$ and $n \geq 0$.\footnote{ 
We can prove it by induction on size of $\sigma$. 
For $\sigma = \alpha \in \T$ is $n = 0$ and for $\sigma = \tau_0 \ar \rho$
we have from induction hypothesis that $\rho = \tau_1 \ar \dots \ar \tau_n \ar \alpha$,
therefore
$\sigma = \tau_0 \ar \tau_1 \ar \dots \ar \tau_n \ar \alpha$.}

Technically speaking, we have types only for functions with one argument, but
this property suggests how we can "simulate" function which takes $n$ arguments.

This technique (or trick) is called \textit{Currying}.\footnote{More formally,
currying is process of transforming a
function which takes multiple inputs
into function that takes one argument and
returns function with one less argument.}   

	
\subsection{Statements of the form $M : \sigma$}

\begin{definition}
	Let $\Lambda$ be set of {\it \lterms}. 
	Let $\mathbb{T}$ be set of {\it types}.       
	A {\it statement} $M : \sigma$ is a pair 
	$(M,\sigma) \in \Lambda \times \mathbb{T}$.
	\\ 
\end{definition}
	
Statement $M : \sigma$ is vocalized as {\it "$M$ has type $\sigma$"}.
The type $\sigma$ is called the {\it predicate} and 
the term $M$ is called the {\it subject} of the statement $M : \sigma$. \\

We can look on this relation in tree different ways.

\begin{description}
  \item[Programming point of view.] 
   Statement $M : \sigma$ means that value $M$ is 
   an instance of data type
   $\sigma$. So statement $M : \sigma$ is
   somehow analogous to \textit{Java}
   expression \texttt{$M$ instanceof $\sigma$}.\footnote{
   This analogy is inadequate in that instanceof is
   construct of the language, whereas : is symbol
   out of the language, it enables us to talk about
   terms from outside perspective.  }    
   
   Type \textit{constrains} the value 
   and gives \textit{information} about it, 
   e.g., function $f : \sigma \ar \tau$ 
   is constrained to only accept values of a type $\sigma$
   as input and must return value of a type $\tau$ as output.
     
  \item[Set point of view.] 
     Statement $M : \sigma$ can also be viewed as relation
     $M \in \sigma$. This makes us look on types as on
     collections of their instances.
      
  \item[Logic point of view.] According to 
  the \textit{Curry–Howard correspondence}
  we can read statement $M : \sigma$ as 
  \textit{"$M$ is a proof of formula $\sigma$"}.
  We will not treat this interesting subject in much detail,
  \lets just say that $\ar$ corresponds to the
  implication $\then$ and \textit{atomic types} correspond to
  \textit{propositional variables}. The whole system of
  \textit{simply typed $\lambda$-calculus} then corresponds
  to the \textit{implicational fragment of propositional 
  intuitionistic logic}.
   
  Intuition is that the proof of $A \then B$ 
  can be imagined as function expecting proof of $A$
  and utilizing it in production of the proof of $B$
  that is returned as output of the function. 
    
  We are mentioning this beautiful
  correspondence in hope that it will help clarify 
  use of \textit{inference rules}, which are key concept
  in the process of deriving valid statements about
  terms and types.  
      
\end{description}  	
	


	
\subsection{Context}


\begin{definition}
~
\begin{enumerate}
 \item A \textit{declaration} is a statement 
 $x : \sigma$ where $x \in V \cup C$.
  
 \item A \textit{context} (or \textit{basis}) 
 is set of declarations with distinct variables as subjects.
\end{enumerate}~
\end{definition}
    

Again our definition slightly differs from the standard one
in that it permits more than one declaration with the same 
\textit{constant}, which enables us to simulate very primitive
kind of ad-hoc polymorphism by \textit{"constant overloading"}.  
And again it does not affect the theory; it just seems less
confusing to me to state these definitions as they are later 
used in the implementation.   \\

In our system, the set of \textit{building blocks} 
(the equivalent of the set $T \cup F$ in standard GP)
will be a context with all declarations having \textit{constants}
as subjects, i.e., a set of constants together with their types.

In the process of term generation, 
this initial context is being further extend with variables
 --- by "entering inner scopes of bodies of lambda functions".\\   
	

Notation $\Gamma,x:\sigma $ denotes $ \Gamma\cup\{(x:\sigma)\}$ 
such that $\Gamma$ does not contain any declaration with $x$ as subject.

		
\subsection{Simply typed $\lambda$-calculus}

\begin{definition}
A statement $M\colon\sigma$ is \textit{derivable from}
a context $\Gamma$ (notation $\Gamma\vdash{}M\colon\sigma$) 
if it can be produced by the following rules.
\begin{align*}
(x,\sigma) \in \Gamma &~\then~ \tur{\Gamma}{x}{\sigma}\\
\tur{\Gamma}{M}{\sigma \ar \tau}~,~\tur{\Gamma}{N}{\sigma} 
&~\then~ \tur{\Gamma}{(M~N)}{\tau}\\  
\tur{\Gamma \cup \{(x,\sigma)\}}{M}{\tau}
&~\then~ \tur{\Gamma}{\lamb{x}{M}}{\sigma \ar \tau} 
\end{align*}~
\end{definition}

Those rules are usually stated in the form of \textit{inference rules}.

\newcommand{\ruleI}[2]{\dfrac{#1}{#2}}
\newcommand{\ruleII}[3]{\dfrac{#1 \qquad #2}{#3}}
\newcommand{\ruleIIs}[3]{\dfrac{#1 ~ #2}{#3}}
\newcommand{\ruleIII}[4]{\dfrac{#1 \qquad #2 \qquad #3}{#4}}
\newcommand{\ruleIV}[5]{\dfrac{#1 \qquad #2 \qquad #3 \qquad #4}{#5}}


\begin{align*}
\ruleI { (x:\sigma) \in \Gamma }
       { \tur{\Gamma}{x}{\sigma} }
&& \textit{(axiom)} \\[1em]
\ruleII{ \tur{\Gamma}{M}{\sigma \ar \tau} }
       { \tur{\Gamma}{N}{\sigma} }
       { \tur{\Gamma}{(M~N)}{\tau} }
&& \textit{($\ar$-elimination)} \\[1em]
\ruleI { \tur{\Gamma,x:\sigma}{M}{\tau} }
       { \tur{\Gamma}{\lamb{x}{M}}{\sigma \ar \tau} }
&& \textit{($\ar$-introduction)} \\[1em]
\end{align*}

\Lets consider following example statement of a form \GMS.

$$
	\{\} \vdash (\lambda f . (\lambda x . (f x) )) : 
	(\sigma \rightarrow \tau) \rightarrow ( \sigma \rightarrow \tau ) 
$$~
	
This statement is derived as follows. 

\begin{equation*}
\dfrac{
	\dfrac{ (f,\sigma \rightarrow \tau) \in \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \}  }
	     { \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \} \vdash f : \sigma \rightarrow \tau }
	\qquad
	\dfrac{ (x,\sigma) \in \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \}  }
	     { \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \} \vdash x : \sigma }
	 }
	 {
		\dfrac{		 	
	 		\{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \} \vdash (f x) : \tau
	 	}{
			\dfrac{\{ (f,\sigma \rightarrow \tau) \} \vdash (\lambda x . (f x) ) : 
			\sigma \rightarrow \tau}
			{ \{ \} \vdash (\lambda f . (\lambda x . (f x) ) ) 
			  : (\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau) }
	 	}
	 }
\end{equation*}		

If we turn the derivation upside down, we can see that it is a tree with 
desired statement in the root and \textit{axioms} in the leafs. \\


New valid inference rules may be produced by finite composition of 
other valid inference rules. 

\Lets consider following rule.

$$
\ruleII
 { \tur{\Gamma,x:\sigma}{M}{\tau} }
 { \tur{\Gamma}{N}{\sigma} }
 { \tur{\Gamma}{(\lamb{x}{M}~N)}{\tau} }
$$

Its validity is shown by the following rule composition.

$$
\ruleII
	{ \ruleI 
	     { \tur{\Gamma,x:\sigma}{M}{\tau} }
	     { \tur{\Gamma}{\lamb{x}{M}}{\sigma \ar \tau} }
	}
	{ \tur{\Gamma}{N}{\sigma} }
	{ \tur{\Gamma}{(\lamb{x}{M}~N)}{\tau} }
$$~






\Lets consider following strange rule.

$$
\ruleII
 { \tur{\Gamma,x:\rho \ar \pi}{M}{\tau} }
 { \tur{\Gamma,y:\rho}{N}{\pi} }
 { \tur{\Gamma}{(\lamb{x}{M}~\lamb{y}{N})}{\tau} }
$$

Its validity is shown by the following rule composition.

$$
\ruleII
	{ \ruleI 
	     { \tur{\Gamma,x:\rho \ar \pi}{M}{\tau} }
	     { \tur{\Gamma}{\lamb{x}{M}}{(\rho \ar \pi) \ar \tau} }
	}
	{ \ruleI 
	     { \tur{\Gamma,y:\rho}{N}{\pi} }
	     { \tur{\Gamma}{\lamb{y}{N}}{\rho \ar \pi} }
	}	 { \tur{\Gamma}{(\lamb{x}{M}~\lamb{y}{N})}{\tau} }
$$~




	
	
Generally speaking, inference rules are used for deriving statements of a form 
\GMS from yet derived statements of such a form (with exception of 
\textit{axiom} rule, which serves as a "starting point").
Those inference rules are written in the following form.

$$ 
\ruleIV
  { \tur{\Gamma_1}{M_1}{\sigma_1} }
  { \tur{\Gamma_2}{M_2}{\sigma_2} }
  { \dotsm }
  { \tur{\Gamma_n}{M_n}{\sigma_n} }
  { \tur{\Gamma_{n+1}}{ \mathbf{T}(M_1,M_2,\dots,M_n)}{\sigma_{n+1}} } 
$$

Where $\mathbf{T}(M_1,M_2,\dots,M_n)$ stands for some 
specific \lterm that may contain \textit{"meta-variables"}
$M_1,M_2,\dots,M_n$. 

In the example above $\mathbf{T}(M,N) = (\lamb{x}{M}~\lamb{y}{N})$

Suppose we have derived statements 
$\Gamma_1 \vdash M_1 : \sigma_1 ,
 \Gamma_2 \vdash M_2 : \sigma_2 ,
 \dots ,
 \Gamma_n \vdash M_n : \sigma_n$. 
Then we are allowed to use the inference rule to derive statement
\mbox{ $\Gamma_{n+1} \vdash \mathbf{T}(M_1,M_2,\dots,M_n) : \sigma_{n+1}$ }.\\
 
 
 
 
%For deriving statements including types of a form 
%$(\sigma \rightarrow \tau)$ are essential those two 
%inference rules:
%
%\begin{equation*}
%	\frac{\Gamma \vdash M : \sigma \rightarrow \tau \qquad
%		  \Gamma \vdash N : \sigma }
%	     {\Gamma \vdash (M N) : \tau }
%\end{equation*}	
%
%\begin{equation*}
%	\frac{\Gamma \cup \{ ( x,\sigma ) \} \vdash M : \tau }
%	     {\Gamma \vdash (\lambda x . M) : \sigma \rightarrow \tau }
%\end{equation*}		 
% 
%This kind of inference rules allows us to derive new statements from yet derived statements, but 
%what if we do not have any statement yet? 
%For this purpose we have other kinds of inference rules such as {\it axiom} inference rule:   
%	
%\begin{equation*}
%	\frac{( x , \sigma )  \in \Gamma}
%	     {\Gamma \vdash x : \sigma}
%\end{equation*}	


Nice thing about \textit{simply typed lambda calculus} is that
program $M$ such that \GMS always terminates 
(provided that implementations of function symbols in $\Gamma$ 
also always terminate). This fact is particularly handy for
GP --- we can evaluate bred programs without worrying about hanging
in the infinite loop forever.
 



%\begin{todo}
%   & říct že se vždycky zastavěj (strong normalization)
%     && a jaký to má výhody pro nás v GP (duležitý),
%     && potřebný definice pro to uvést v \lterms sekci
%     && musí se říct co je strongly normalizing, 
%        tzn simpli typed $lambda$-kalkulus termy a $\beta$-redukce /
%        $\beta\eta$-redukce
%%     && řekl bych že eta redukce je SN vždycky (tzn i untyped lk)
%%        pač plati (CR+terminující pak SN) a je CR vzhledem k barendrecht-stará-žlutá
%%        a terminující je rpotože zmenšuje term s nemuže zmenšovat donekonečna    
%\end{todo}


\subsection{Tree representations of typed \lterms}
\label{typed-tree-reps}

\newcommand{\lsCurry}{$\lambda^{\ar}$\nobreakdash-Curry\xspace}
\newcommand{\lsChurch}{$\lambda^{\ar}$\nobreakdash-Church\xspace}


Simply typed $\lambda$-calculus as described above is formally
called the \textit{system \lsCurry}. 
Other slightly different formalization is called 
\textit{system \lsChurch}. 
They differ in that \lsCurry uses as terms above described
untyped terms, whereas \lsChurch uses slightly modified
terms with information about types of variables written in
lambda abstraction along with variable name.
Since those systems are proven to be equivalent,
we are not giving this subject much attention here.
For more detailed information on this subject see \cite{barendregt92}.\\

In our implementation we also use modified representation of \lterms
storing information about types. But our representation is more 
radical than that of \lsChurch; it stores information about types
of all subterms.

Regarding tree representation of such a typed
\lterm, most of the time it is more convenient to 
understand this information as invisibly sitting
in the tree nodes alongside with visible
term symbols.

But as we will see below in the section about \textit{inhabitation trees},
there are situations when the explicit type information in the tree representation
comes in handy. Thus we are presenting following tree representation.\\


Straightforward approach would be to add to each node a visible type entry 
that would be the type of the subterm corresponding to subtree having this
node as the root node. 

Approach more suitable for our further purpose 
is to add a special type node \textit{above} each node.

More specifically,
\lets consider \textit{\sexprTree} $t$ corresponding to a \lterm of the type
$\sigma$ with root $r$ and subtrees $s_1 , \dots , s_n$. 
Then corresponding \textit{typed \sexprTree} $TT[t]$ for typed \lterm is 
obtained from the tree $t$ as follows.  

\begin{equation*}
\mbox{ 
TT[ 
\Tree
	[.r 	
	  	  \text{$s_1$}
		  \text{$s_2$}
		  \text{$\dots$}
		  \text{$s_n$}
	] 
]} =
\mbox{
\Tree
	[.\text{$\sigma$ }
	    [.r 	
	  	  \text{$TT[s_1]$}
		  \text{$TT[s_2]$}
		  \text{$\dots$}
		  \text{$TT[s_n]$}
		]	  	
	] 
}
\end{equation*}

We silently suppose here that type information about every subterm is available.

%And analogically we can define transformation $TTtoTerm[]$ as follows.



\section{Term generating grammar}
\label{Term-generating-grammar}

\newcommand{\gar}{\longmapsto}

Inference rules are good for deriving statements of the form \GMS, but our
goal is slightly different; we would like to generate many \lterms M for a given type 
$\sigma$ and context $\Gamma$.\\

\Lets start our reasoning about generation of \lterms with
notion of term generating grammar. We will show how to
transform an inference rule into a rule of term generating grammar.
With the appropriate term generating grammar 
in hands it is just a simple step to \textit{inhabitation trees}.\\

It is not a grammar in classical sense, because we will be operating with infinite sets of nonterminal symbols and rules. Technically it it
2-level grammer \cite{barendregt10}.\\

Our terminals are symbols used for writing \lterms 
.%,i.e., parentheses,  . 

Our non-terminals are pairs $(\tau,\Gamma)$ where 
$\tau \in \T$ and $\Gamma$ is a \textit{context}.

Our initial non-terminal is $S = (\sigma_S,\Gamma_S)$
where $\sigma_S$ is desired type of generated \lterms and
$\Gamma_S$ is \textit{building blocks} context. \\

\Lets begin with \textit{axiom} inference rule.

$$
\ruleI { (x:\sigma) \in \Gamma }
       { \tur{\Gamma}{x}{\sigma} }
$$~

%It corresponds to following grammar rule schema.
%$$ ( \sigma , \Gamma )  \gar \mbox{ {\Large x}} 
%~~~~~~~~~~~~~ \textbf{if } (x,\sigma) \in \Gamma$$ 

For every non-terminal $(\sigma,\Gamma)$ 
and for every $x$ such that $(x:\sigma) \in \Gamma$,
there is following grammar rule.
$$ ( \sigma , \Gamma )  \longmapsto \mbox{ {\Large x}} $$ \\

Next is \textit{$\ar$-elimination}.

$$
\ruleII{ \tur{\Gamma}{M}{\sigma \ar \tau} }
       { \tur{\Gamma}{N}{\sigma} }
       { \tur{\Gamma}{(M~N)}{\tau} }
$$~

For every non-terminal $(\tau,\Gamma)$ 
and for every type $\sigma \in \T$ 
there is following grammar rule.\footnote{ 
Terminal symbols for parenthesis and normally {\it space} 
now \textvisiblespace \quad (for {\it function application} operator) 
are visually highlighted. } 
$$
	( \tau , \Gamma )  \gar
	\bigg( ( \sigma \rightarrow \tau , \Gamma ) 
	\mbox{ \Vtextvisiblespace[1em] } ( \sigma , \Gamma ) \bigg)
$$\\

And finally {\it $\ar$-introduction}. 
$$
\ruleI { \tur{\Gamma,x:\sigma}{M}{\tau} }
       { \tur{\Gamma}{\lamb{x}{M}}{\sigma \ar \tau} }
$$

For every non-terminal $(\sigma \ar \tau,\Gamma)$
and for every variable $x$ such that $\Gamma$ does not contain any 
declaration with $x$ as subject
there is following grammar rule.
 

%and let $x$ be variable name that is
%not present in any declaration in $\Gamma$.
%(since we want to generate "fresh" variables). 

$$ 
	( \sigma \rightarrow \tau , \Gamma )  \gar
	\bigg( \mbox{ {\Large $\lambda$ x . }}( \tau ; \Gamma ,x:\sigma ) \quad \bigg)
$$\\

We will demonstrate \lterm generation on example. 
Again on $(\lambda f . (\lambda x . (f x) ))$. 
We would like to generate \lterm of a type 
$(\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau)$
with $\Gamma = \{\}$.
\begin{align*}
	& ((\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau),\{\}) \\ 
	\gar & \Big( \mbox{ {\Large $\lambda$f.}}
	  ( \sigma \rightarrow \tau , \{ (f,\sigma \rightarrow \tau) \} ) 
	~ \Big)
	\\
	\gar & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}
	  	 	( \tau , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) 
		~ \Big)  	 
	~ \Big)
	\\
	\gar & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
	  	 	\Big( 
	  	 	  ( \sigma \rightarrow \tau , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) 
			  \mbox{ \Vtextvisiblespace[1em] } 
			  ( \sigma , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} )  \Big) 
		~ \Big)  	 
	 ~ \Big)
	\\
	\gar & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
	  	 	\Big( 
	  	 	  \mbox{ {\Large f}} 
			  \mbox{ \Vtextvisiblespace[1em] } 
			  ( \sigma , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) \Big) 
		~ \Big)  	 
	~ \Big)		
	\\
	\gar & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
	  	 	\Big( 
	  	 	  \mbox{ {\Large f}} 
			  \mbox{ \Vtextvisiblespace[1em] } 
			  \mbox{{\Large x}} \Big) 
		~ \Big)  	 
	~ \Big)
\end{align*}~\\


\Lets consider a general inference rule again.
$$ 
\ruleIV
  { \tur{\Gamma_1}{M_1}{\sigma_1} }
  { \tur{\Gamma_2}{M_2}{\sigma_2} }
  { \dotsm }
  { \tur{\Gamma_n}{M_n}{\sigma_n} }
  { \tur{\Gamma_{n+1}}{ \mathbf{T}(M_1,M_2,\dots,M_n)}{\sigma_{n+1}} } 
$$

The corresponding grammar rule can be summarized as following schema.

$$
 (\sigma_{n+1},\Gamma_{n+1})
 \gar
 \mbox{ {\Large \textbf{T}}} 
 \bigg( 
 (\sigma_1,\Gamma_1)
 \mbox{ {\Large ,}}
 (\sigma_2,\Gamma_2)
 \mbox{ {\Large ,}}
 ~\dots
 \mbox{ {\Large ,}}
 (\sigma_n,\Gamma_n)
 \bigg)  
$$


	


\section{Long normal form}
\label{lnf}

%v barendrechtovi je to v definici lnf
%$\lam{x_1 \dots x_n}{f~M_1~\dots~M_n}$ místo $M_m$ 
%ale to musí bejt určitě překlep...

\begin{definition}
Let \GMS where 
$\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha, n \geq 0$.
	\begin{enumerate}
	  \item	
		Then $M$ is in \textit{long normal form} (\lnf) if following 
		conditions are satisfied.
		\begin{enumerate}
		 \item $M$ is term of the form $\lam{x_1 \dots x_n}{f~M_1~\dots~M_m}$\\
		  (specially for $n = 0$, $M$ is term of the form $f$).
		 \item Each $M_i$ is in \lnf.
		\end{enumerate}	
	  \item 
	    $M$ has a \lnf if $M =_{\beta\eta} N$ and $N$ is in \lnf.
	\end{enumerate}
\end{definition}~

As is shown in \cite{barendregt10}, \lnf has following nice properties.

\begin{proposition}
If $M$ has a \bnf, 
%which according to Theorem 2B.4 is always the case, 
then it also has a unique \lnf, 
which is also its unique \beenf.
\end{proposition}

\begin{proposition}
Every $B$ in \bnf has a \lnf 
$L$ such that $L \twoheadrightarrow_{\eta} B$.
\end{proposition}








\section{Grammar producing \lterms in \lnf}
\label{barlike}

In \cite{barendregt10} is shown term generating grammar with 
following rules (our notation is used, but we will not 
highlighted terminals anymore).\footnote{
I was using term generating grammars for term generation
on my own before I encountered with \cite{barendregt10},
where i happily discovered that Barendregt is using
almost identical notation. 
But his \lnf grammar was more clever then my grammar
that was using grammar rules corresponding to the three basic
inference rules. 
}
\begin{align*}
( \alpha , \Gamma )  
&\gar
(~f~( \rho_1 , \Gamma )~\dots~( \rho_m , \Gamma )~)
& \textbf{if } \alpha \in A,
(f : \rho_1 \ar \dots \ar \rho_m \ar \alpha) \in \Gamma
\\ 
( \sigma \rightarrow \tau , \Gamma )  
&\gar
(~\lambda~x~.~( \tau ; \Gamma,x:\sigma )~)
&   
\end{align*}

The second rule can be replaced by more effective one.
\[ 
( \tau_1 \ar \dots \ar \tau_n \ar \alpha , \Gamma )  
\gar
(~\lambda~x_1~\dots~x_n~.~
( \alpha ; \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n  )~)
~~~~ \textbf{if } n > 0
\] 

This rule packs consecutive uses of the second rule into one use.
This is valid since the use of the second rule is deterministic;
it is used if and only if the non-terminal's type is not atomic.\\

Following proposition about this grammar is stated (but not proven)
in~\cite{barendregt10}. Since it is key proposition for our 
term generating method, we also provide a proof.


\newcommand{\garr}{\leadsto}%{\Rrightarrow}%
\newcommand{\Gp}{\Gamma^\prime}

\begin{proposition}
\label{gram-lnf-prop}
Let context $\Gamma$, \lterm $M$ and type $\sigma$ be given.\\
Let $\garr$ be transitive reflexive closure of $\gar$. 
Then
$$ 
(\sigma,\Gamma) \garr M 
\Leftrightarrow
\tur{\Gamma}{M}{\sigma} \text{ and $M$ is in lnf}.   
$$
\end{proposition}

\begin{proof}~

\Lets start with proving 

$ (\sigma,\Gamma) \garr M 
\then
\tur{\Gamma}{M}{\sigma} \text{ and $M$ is in \textit{lnf}}$

by induction on the length of rewriting chain of $(\sigma,\Gamma) \garr M$.\\

Let $\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha (n \geq 0)$,

$\Gp = \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n$.\\

$(\tau_1 \ar \dots \ar \tau_n \ar \alpha,\Gamma), n>0$ must rewrite to
$(~\lambda~x_1~\dots~x_n~.~( \alpha , \Gp )~)$, 

thus (even for $n = 0$) the chain must go through
$(~\lambda~x_1~\dots~x_n~.~( \alpha , \Gp )~)$.\\

$(\alpha,\Gp)$ must rewrite to 
$(~f~( \rho_1 , \Gp )~\dots~( \rho_m , \Gp )~)$ 

for some $(f,\rho_1\ar\dots\ar\rho_m\ar\alpha) \in \Gp$,

thus the chain must go through 
$(~\lambda~x_1~\dots~x_n~.~(~f~( \rho_1 , \Gp )~\dots~( \rho_m , \Gp )~)~)$.\\

After that each $( \rho_i , \Gp ) \garr M_i$
with shorter chain than $(\sigma,\Gamma) \garr M $.

Thus we have from the induction hypothesis for each $M_i$ that
 
$\tur{\Gp}{M_i}{\rho_i}$ and $M_i$ is in \textit{lnf}.\\

$M = (~\lambda~x_1~\dots~x_n~.~(~f~M_1~\dots~M_m~)~)$ 
and each $M_i$ is in \lnf, 

thus M is in \lnf.\\

Following derivation shows how to derive \GMS.\\

$
\ruleI{
\ruleI{ 
\ruleI{
\ruleI{
\ruleIIs{
\ruleIIs{
\ruleIIs{
\ruleIIs{ 
\ruleI
{\boxed{(f,\rho_1\ar\dots\ar\rho_m\ar\alpha) \in \Gp} }
{\tur{\Gp}{f}{\rho_1\ar\dots\ar\rho_m\ar\alpha}}}
{\boxed{
 \tur{\Gp }
     {M_1 }
     {\rho_1 }}}
{
\tur{\Gp}{(~f~M_1~)}{\rho_2\ar\dots\ar\rho_m\ar\alpha} 
}}
{\boxed{
 \tur{\Gp }
     {M_2 }
     {\rho_2 }}}
{\vdots}}
{\ddots}
{\tur{\Gp }
     {(~f~M_1~\dots~M_{m-1}~) }
     {\rho_m \ar \alpha}}}
{\boxed{
 \tur{\Gp }
     {M_m }
     {\rho_m}} }
{\tur{\Gp }
     {(~f~M_1~\dots~M_m~) }
     {\alpha}}}
{\tur{\Gamma,x_1:\tau_1,\dots,x_{n-1}:\tau_{n-1} }
     {\lam{x_n}{(~f~M_1~\dots~M_m~)} }
     {\tau_n \ar \alpha} }}
{\vdots}}
{\tur{\Gamma,x_1:\tau_1 }
       {\lam{x_2~\dots~x_n}{(~f~M_1~\dots~M_m~)} }
       {\tau_2 \ar \dots \ar \tau_n \ar \alpha} } }
{\boxed{
 \tur{\Gamma }
     {\lam{x_1~x_2~\dots~x_n}{(~f~M_1~\dots~M_m~)} }
     {\tau_1 \ar \tau_2 \ar \dots \ar \tau_n \ar \alpha} }}
$

~\\~\\

Now \lets prove the opposite direction 

$\tur{\Gamma}{M}{\sigma} \text{ and $M$ is in \textit{lnf}}
\then
(\sigma,\Gamma) \garr M$
   
by induction on the size of M.\\


Let $M = \lam{x_1 \dots x_n}{f~M_1~\dots~M_m}$,

$\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha$,

$\Gp = \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n$.\\

Then

$
( \sigma , \Gamma ) =
( \tau_1 \ar \dots \ar \tau_n \ar \alpha , \Gamma )  
\gar
(~\lambda~x_1~\dots~x_n~.~
( \alpha , \Gp )~)
$

by using the updated second grammar rule.\\

From \GMS we have that $(f,\rho_1\ar\dots\ar\rho_m\ar\alpha) \in \Gp$
and $\tur{\Gp}{M_i}{\rho_i}$.

(This can be easily observed from definition of \GMS.)\\

Therefore 

$
(~\lambda~x_1~\dots~x_n~.~( \alpha , \Gp )~)
\gar
(~\lambda~x_1~\dots~x_n~.~
	(~f~( \rho_1 , \Gp )~\dots~( \rho_m , \Gp )~)
~)
$

by using the first rule.\\

Each $M_i$ is in \lnf, 
is smaller in size then $M$
and $\tur{\Gp}{M_i}{\rho_i}$; 

therefore we have from the induction hypothesis that 
$(\rho_i,\Gp) \garr M_i$.\\

Therefore 
$
( \sigma , \Gamma )
\garr
(~\lambda~x_1~\dots~x_n~.~
	(~f~M_1~\dots~M_m~)
~)
$.
 
\end{proof}~


This means that we can use this grammar to generate all \lterms $M$ in \lnf
with desired type $\sigma$ from \textit{building blocks} context $\Gamma$.\\



\subsection{Benefits of generating \lterms in \lnf}
\label{benefits}

By generating \lterms in \textit{lnf} we avoid generating 
\lterms $M$,$N$ such that $M \not= N$, but $M =_{\beta\eta} N$.
In other words, we avoid generating two programs with different 
source codes, but performing the same computation.

Every \lterm $M$ such that \GMS has its unique \lnf $L$, 
for which $L =_{\beta\eta} M$.
Therefore the computation performed by \lterm $M$ 
is not omitted, because it is the same computation
as the computation performed by \lterm $L$. \\

Generating \lterms in \lnf is even better than generating 
\lterms in \bnf. Since \lnf is the same thing as \beenf,
every \lterm in \lnf is also in \bnf. 

This comes straight from the definition of \beenf, 
but one can also see it from observing method for generating
terms in \bnf. As is shown in \cite{barendregt10}, 
this method is obtained simply by replacing  
the first grammar rule by slightly modified rule,
resulting in the following grammar.
\begin{align*}
( \pi , \Gamma )  
&\gar
(~f~( \rho_1 , \Gamma )~\dots~( \rho_m , \Gamma )~)
& \textbf{if } (f : \rho_1 \ar \dots \ar \rho_m \ar \pi) \in \Gamma
\\ 
( \sigma \rightarrow \tau , \Gamma )  
&\gar
(~\lambda~x~.~( \tau ; \Gamma,x:\sigma )~)
&   
\end{align*}

The difference lies in that $f$'s type is no longer needed to be fully expanded
($\pi \in \T$ instead of $\alpha \in A$). This makes the grammar less deterministic,
resulting in a bigger search space. The new rule is generalization of the old one,
thus all terms in \lnf will be generated, along with many new terms in \bnf that 
are not in \lnf. 
    
By generating \lterms in \lnf we avoid generating 
\lterms $M$,$N$ such that $M \not= N$ and $M =_{\eta} N$; 
but by generating in \bnf we do not avoid it.\\


The disadvantage of the \lnf, as the name suggests, is that it is long.
Terms in \lnf are said to be \textit{fully $\eta$-expanded} \cite{barendregt10}. 
Relevant property of $\eta$-reduction is that it always shortens the term
that is being reduced be it. And conversely, $\eta$-expansion prolongs.
$$\lamb{x}{(M~x)} \eArrow M \textbf{ ~~~~if } x \not\in FV(M) $$

Now we show that for every \lterm $M$ 
every sequence of $\eta$-reductions is finite and
leads to unique \enf $N$.

\begin{enumerate}
 \item Every application of \ered shortens the term.
       Since every term has finite size, this process must 
       end at some point. Thus every \lterm has \enf.
 \item Since \ered is \textit{Church–Rosser}, \enf is unique (see \cite{barendregt84}). 
\end{enumerate}

So we can take every generated \lterm $M$ in 
\lnf and transform it to shorter term in \enf. 
The question is whether it remains in \bnf, thus being in \benf.
The answer is yes; it can be proven by showing that no 
new \bredex is created by \ered.  

\begin{proposition}
Let $P$ be in \bnf and $P \eArrow Q$. Then $Q$ is in \bnf.    
\end{proposition}
\begin{proof}

For better clarity \lets show the \ered and \bredex using trees.\\
\ered: \\

\Tree [.$\lh{x}$ [.@ $M$ $x$ ] ] 
~~~~$\eArrow$~~~~
$M$ 

And \bredex:

\Tree [.@ [.$\lh{x}$ $M$ ] $N$ ] \\

\Lets assume that $P \eArrow Q$ creates a new \bredex $B$ in $Q$.

Since \ered only destroys and never creates \textit{function applications} (i.e. @),
the root @ of $B$ must be present in $P$.  
But since $P$ contains no \bredex, the left subterm $L$ of this root @
is not $\lambda$-abstraction.
Only possible way for $L$ to be changed by $\eArrow$ into 
a $\lambda$-abstraction is that $L$ is the reduced subterm (so that
$L$ is changed for its subterm).
But that is in contradiction with $P$ not containing any \bredex,
because it would cause $L$ be a $\lambda$-abstraction.

\end{proof}

Notable property of \lnf and \benf is that there is \textit{bijection} 
(i.e. one-to-one correspondence) of 
the set of simply typed \lterms in \lnf and 
the set of simply typed \lterms in \benf.

\begin{proposition}

Reduction to \enf is bijection between  
the set of simply typed \lterms in \lnf and 
the set of simply typed \lterms in \benf.
\end{proposition}
\begin{proof}

Since reduction to \enf always leads to an unique term, it is a function.
In previous proposition is shown that \ered of \lnf
leads to a term in \benf.\\

In order to show that a function is bijection it is sufficient to show that it is
both \textit{injection} and \textit{surjection}.\\

Suppose it is not injection.

So there must be $M_1,M_2$ in \lnf such that $M_1 \not= M_2$
and $N$ in \benf such that $M_1 \etar N$, $M_2 \etar N$.
Therefore $M_1 =_\eta M_2$, 
so $M_1 =_{\beta\eta^{-1}} M_2$.
This contradicts with $M_1,M_2$ being distinct \lnf{}s.\\

Every $M$ in \bnf has a \lnf $N$ such that 
$N \twoheadrightarrow_{\eta} M$ (proposition from \ref{lnf}).
Term $M$ in \benf is in \bnf, thus it has desired \lnf $N$
which reduces to it. 

Therefore it is surjection. 
\end{proof}

Suppose we have systematic (i.e. gradually generating all terms, 
but no term twice) method for generating terms in \lnf,
we may transform it to systematic method for generating terms in \benf
by simply reducing each generated term to its \enf.  




\newpage
\section{Inhabitation tree}

Now we introduce \textit{inhabitation tree}; structure slightly different from
\textit{inhabitation machine}, which was introduced by Barendregt\cite{barendregt10}.
We can think about inhabitation tree as about unfolded inhabitation machine.
Inhabitation tree approach is further refinement 
of the term generating grammar approach, which is too much textual.
Whereas inhabitation tree approach aims at tree structure of \lterms.

~\\


\newcommand{\IT}{\mathop{\mathrm{IT}}}

\begin{definition}

Let context $\Gamma$ and type $\sigma$ be given.
\textit{Inhabitation tree} for $(\sigma,\Gamma)$, denoted as $\IT(\sigma,\Gamma)$,  
is a rooted possibly infinite tree that is recursively defined as follows.\\

If $\sigma = \alpha$ such that $\alpha \in A$, then 

$\IT(\sigma,\Gamma) := $ \\

\Tree 
[.$\alpha$ 
   [.$f_1$ $\IT(\rho^1_1,\Gamma)$ $\dots$ $\IT(\rho^1_{m_1},\Gamma)$ ]
   [.$\dots$ ]
   [.$f_k$ $\IT(\rho^k_1,\Gamma)$ $\dots$ $\IT(\rho^k_{m_k},\Gamma)$ ]           
]\\[1em]

~~~~~~where 

$~~~~~~~~~~\{ \,
f_1 : \rho^1_1 \ar \dots \ar \rho^1_{m_1} \ar \alpha ~,~\dots~,~
f_k : \rho^k_1 \ar \dots \ar \rho^k_{m_k} \ar \alpha \} \\ ~\,~~~~~~~~~~= 
\{\, f \: : \rho_1 \ar \dots \ar \rho_m \; \ar \alpha ~|~ 
  (f,\rho_1 \ar \dots \ar \rho_m \ar \alpha ) \in \Gamma \,\}$.\\

Otherwise, for $\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha$ 
such that $n> 0, \alpha \in A$

$\IT(\sigma,\Gamma) := $\\
 
\Tree [.$\tau_1\ar\dots\ar\tau_n\ar\alpha$ 
 [.$\lh{x_1~\dots~x_n}$ [.$\IT(\alpha;\Gamma,x_1:\tau_1,\dots,x_n:\tau_n)$ ] ] ]    


\end{definition}~\\[1em]

\newpage

\Lets discuss this definition in grater detail. 
An inhabitation tree has two kinds of nodes.

\begin{samepage}
\begin{enumerate}
  \item \textit{Type node} (or \textit{OR-node}).
  
  	    This kind of node contains type. %$\sigma \in \mathbb{T}$  
  \item \textit{Symbol node} (or \textit{AND-node}).
  
  		This kind of node contains 
  		constant name, variable name 
  		\\or \textit{$\lambda$-head} 
  	    --- non-empty finite sequence of variable names. 
\end{enumerate}
\end{samepage}~

Following observation clarifies the alternation of node kinds.
\begin{enumerate}
\item The root of Inhabitation tree for $(\sigma,\Gamma)$ is 
      \textit{type node} with $\sigma$ as type.
\item All \textit{type nodes} have as child nodes only \textit{symbol nodes}. 
\item All \textit{symbol nodes} have as child nodes only \textit{type nodes}. 
\end{enumerate}~

%~\\~\\~\\


There are two cases of $\sigma$ --- 
\textit{atomic type} and \textit{function type}. \\

First case is \textbf{atomic type}, i.e., $\sigma = \alpha$ where $\alpha \in A$.

For every $(f,\rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha) \in \Gamma$
there is a child \textit{symbol node} containing constant or variable name $f$.
This symbol node containing $f$ has $m$ child subtrees corresponding to  
$\IT(\rho_1,\Gamma),\dots,\IT(\rho_m,\Gamma)$.  

Therefore specially for $m = 0$ node containing $f$ is 
a leaf of the inhabitation tree.

~

Compare this case with corresponding grammar rule.
\[ 
	( \alpha , \Gamma )  \gar
	\bigg( \mbox{ {\Large f }}
	  \mbox{ \Vtextvisiblespace[1em] } 
	  ( \rho_1 , \Gamma )
	  \mbox{ \Vtextvisiblespace[1em] } 
	  \dots
	  \mbox{ \Vtextvisiblespace[1em] } 
	  ( \rho_m , \Gamma )
	  \quad \bigg)
\]

Second case is \textbf{function type}, i.e., 
$\sigma = \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$
where $n~\geq~1, \alpha~\in~A$.\\
For every  $i \in \{1,\dots,n\}$ we create new \textit{variable name} $x_i$ 
that is not yet included in context $\Gamma$ as variable or constant name.
 
There is one and only one child \textit{symbol node} 
of function type node containing $\lambda$-head
$\lambda x_1 \dots x_n$, which stands for 
sequence of variable names $(x_1,\dots,x_n)$.
This symbol node containing $\lambda x_1 \dots x_n$
has one and only one child subtree corresponding to  
$\IT(\alpha;\Gamma,x_1:\tau_1 , \dots , x_n:\tau_n )$.   

~

Compare this case with corresponding grammar rule.
\[ 
	( \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha , \Gamma )  \gar
	\bigg( \mbox{ {\Large 
	$\lambda x_1 \dots x_n .$ 
	}}( \alpha ; \Gamma , x_1:\tau_1,\dots,x_n:\tau_n ) \quad \bigg)
\]
~\\
\Lets consider following $(\sigma,\Gamma)$ as a simple example:

\begin{align*}
\sigma =& ~ \mathbb{B} \rightarrow  \mathbb{B} \rightarrow  \mathbb{B} \\ 
\Gamma =& \{ ~ true : \mathbb{B}  \\
        &  , ~ nand :  \mathbb{B} \rightarrow \mathbb{B} \rightarrow \mathbb{B} ~ \}
\end{align*}

This particular $(\sigma,\Gamma)$ results in the following tree:\\

\Tree
[.\text{ $\mathbb{B} \rightarrow \mathbb{B} \rightarrow \mathbb{B}$ }  
	[.\textbf{$\lambda$x_1 x_2 } 
		[.\text{ $\mathbb{B}$ } 
			\textbf{true}  
			[.\textbf{nand} 
				\qroof{ ~~ $\dotsb$ ~~ }.\text{ $\mathbb{B}$ }
				\qroof{ ~~ $\dotsb$ ~~ }.\text{ $\mathbb{B}$ } 
			]
			\textbf{x_1}
			\textbf{x_2}
		]
	]
]

~

Second example features our well known example:

\begin{align*}
\sigma =& ~ (\sigma \rightarrow \tau) \rightarrow \sigma \rightarrow \tau \\ 
\Gamma =& \{ \}
\end{align*}

Which results in following tree:

\Tree
[.\text{ $(\sigma \rightarrow \tau) \rightarrow \sigma \rightarrow \tau $ } 
	[.\textbf{$\lambda$ f x }	
		[.\text{ $\tau$ }		
			[.\textbf{f} 
				[.\text{ $\delta$ }
					\textbf{x}					
				]
			]
		]
	]
] 

\newpage
\subsection{And-or tree and searching in Inhabitation tree}

\Lets consider following definition of 
\textit{and-or tree}.
%\footnote{
%\textbf{TODO}: Mention that on [WIKI] there is more general definition, but 
%for our purposes is this one sufficient.}

\begin{definition}
~
	\begin{enumerate}
	\item \textit{And-or tree} is a rooted possibly infinite 
	      tree with every node labeled as either \textit{and-node} 
	      or
	      %\footnote{xor} 
	      \textit{or-node}.
	
%	\item By \textit{solving and-or tree} $T$ is meant finding finite tree $T'$
%	      subgraph of $T$ such that following conditions are met. 
    \item $T'$ \textit{solves} and-or tree $T$ if $T'$ satisfy following conditions.
	    \begin{enumerate}
	      \item $T'$ is a finite tree, subgraph of $T$. 
		  \item The root of $T'$ is the root of $T$.
		  \item Each \textit{and-node} in $T'$ has all the child nodes as in $T$.
		  \item Each \textit{or-node}  in $T'$ has precisely one child 
		      node.
		      %\footnote{\textbf{TODO}: MENTION why precisely one 
		      %and not at least one ..or CHANGE the def. }   
	    \end{enumerate}
	
	\end{enumerate}

\end{definition}

~\\[1em]
\Lets now consider following labeling of Inhabitation tree.

\begin{itemize}
  \item \textbf{Type nodes}   are labeled as \textbf{or-nodes}.   
  \item \textbf{Symbol nodes} are labeled as \textbf{and-nodes}.
\end{itemize}~

This labeling has following justification.\\

Selection of exactly one
child node in \textit{type node} corresponds to selection of exactly one
grammar rule in order to rewrite nonterminal symbol.  \\

Selection of all the child nodes in \textit{symbol node} corresponds to 
rewriting all the nonterminal symbols in string that is being generated.
Or put in different words, it corresponds to filling all the slots
for arguments with values.\\  

\newpage
\subsection{ Generating terms in \lnf by solving inhabitation tree }

The motivation for defining \textit{solving} and-or tree the way we did is that
solution trees of $\IT(\sigma,\Gamma)$ are  
\textit{typed \sexprTree{}s} representing all \lterms M in \lnf such that \GMS.


\begin{proposition}
\label{gram-it-prop}
(Correctness)\\
If \sexprTree $M'$ solves $\IT(\sigma,\Gamma)$,\\ 
then $M'$ represents \lterm $M$ in \lnf such that \GMS. 
\end{proposition}
\begin{proof}

We will prove following statement from which follows 
the rest by proposition \ref{gram-lnf-prop}:
If \sexprTree $M'$ solves $\IT(\sigma,\Gamma)$,
then $M'$ represents \lterm $M$ such that $(\sigma,\Gamma) \garr M$.\\

By induction on size of $M'$.\\

\textbf{If} $\sigma = \alpha$ such that $\alpha \in A$, then 

\mbox{
$M' = $ 
\Tree 
[.$\alpha$ 
   [.$f$ $M'_1$ $\dots$ $M'_m$ ]
]}\\

For some $f : \rho_1 \ar \dots \ar \rho_m \ar \alpha \in \Gamma$.\\

One can easily see that $M'_i$ solves $\IT(\rho_i,\Gamma)$. 

Thus by induction hypothesis we get that 

$M'_i$ represents \lterm $M_i$ such that $(\rho_i,\Gamma) \garr M_i$.\\

$(\sigma,\Gamma) \gar (~f~(\rho_1,\Gamma)~\dots~(\rho_m,\Gamma)~)$,
thus $(\sigma,\Gamma) \garr (~f~M_i~\dots~M_i~)$.

Since $M'$ represents $(~f~M_i~\dots~M_i~)$, the proof for the first case is complete.\\

\textbf{Otherwise}, for $\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha$ 
such that $n> 0, \alpha \in A$

\mbox{$M' = $
\Tree [.$\tau_1\ar\dots\ar\tau_n\ar\alpha$ 
 [.$\lh{x_1~\dots~x_n}$ [.$N'$ ] ] ]    }

Again, one can easily see that $N'$ solves 
$\IT(\alpha;\Gamma,x_1:\tau_1,\dots,x_n:\tau_n)$.

Thus by induction hypothesis we get that 

$N'$ represents \lterm $N$ such that 
$( \alpha ; \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n  ) \garr N$.\\

$
( \tau_1 \ar \dots \ar \tau_n \ar \alpha , \Gamma )  
\gar
(~\lambda~x_1~\dots~x_n~.~
( \alpha ; \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n  )~)
$,

thus $(\sigma,\Gamma) \garr (~\lambda~x_1~\dots~x_n~.~N~)$.

Since $M'$ represents $(~\lambda~x_1~\dots~x_n~.~N~)$, the proof is complete.


\end{proof}  	

\newpage
\begin{proposition}
(Completeness)\\
If $M$ is \lterm in \lnf such that \GMS,\\
then there exists exactly one \sexprTree $M'$ such that 
$M'$ represents $M$ and
$M'$ solves $\IT(\sigma,\Gamma)$.
\end{proposition}
\begin{proof}
We will prove following statement from which follows 
the rest by proposition \ref{gram-lnf-prop}:
If $M$ is \lterm such that $(\sigma,\Gamma) \garr M$,
then there exists exactly one \sexprTree $M'$ such that 
$M'$ represents $M$ and
$M'$ solves $\IT(\sigma,\Gamma)$.\\


By induction on the length of rewriting chain of $(\sigma,\Gamma) \garr M$.\\

\textbf{If} $\sigma = \alpha$ such that $\alpha \in A$, then:

$(\sigma,\Gamma)$ must rewrite to
$(~f~( \rho_1 , \Gamma )~\dots~( \rho_m , \Gamma )~)$.

And there is only one possibility for $f$ since $M$ is given.

$M = (~f~M_1~\dots~M_m~)$ where $( \rho_i , \Gamma ) \garr M_i$.\\

From induction hypothesis we have that 
there exists exactly one \sexprTree $M_i'$ such that 
$M_i'$ represents $M_i$ and
$M_i'$ solves $\IT(\rho_i,\Gamma)$.\\

In order to construct $M'$ solving $\IT(\sigma,\Gamma)$ 
we have only one possibility since we know the right $f$ to take:

\mbox{
$M' = $ 
\Tree 
[.$\alpha$ 
   [.$f$ $M'_1$ $\dots$ $M'_m$ ]
]}\\


\textbf{Otherwise}, for $\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha,$ $(n > 0)$:\\



$(\sigma,\Gamma)$ must rewrite to
$(~\lambda~x_1~\dots~x_n~.~( \alpha ; \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n )~)$.

$M = (~\lambda~x_1~\dots~x_n~.~N~)$ where 
$( \alpha ; \Gamma , x_1:\tau_1 , \dots , x_n:\tau_n ) \garr N$.\\

From induction hypothesis we have that 
there exists exactly one \sexprTree $N'$ such that 
$N'$ represents $N$ and
$N'$ solves $\IT(\alpha,\Gp)$.\\

In order to construct $M'$ solving $\IT(\sigma,\Gamma)$ in non-atomic $\sigma$
we have only one possibility:

\mbox{$M' = $
\Tree [.$\tau_1\ar\dots\ar\tau_n\ar\alpha$ 
 [.$\lh{x_1~\dots~x_n}$ [.$N'$ ] ] ]    }


\end{proof}

%\begin{todo}
%& skontrolovat ty dukazy pořádně sepsal sem 
%  je dost rychle a mužou bejt divný. 
%\end{todo}


\newpage
\subsection{In defense of inhabitation trees}

It may seem that we are overthinking the problem by introduction
of this new structure, whereas it would be sufficient to
use the term generating grammars instead.\\

I see two reasons why inhabitation tree is useful concept.

\begin{enumerate}
\item The first reason is rather subjective.
      In my opinion, it simplifies the problem to be
      better graspable. My arguments are following: 
      \begin{enumerate}
      \item
      It reduces the unpleasant fact
      that term generating grammars are not grammars in
      the standard sense (but instead \textit{2-level-grammars}). 
      \item
      It brings the problem to 
      its more natural domain, 
      from (textual) grammars 
      to (syntactical) trees.
      \item
      We may think about the whole structure of generating 
      at once since the whole tree, although usually infinite, is
      fairly simple structure of alternating layers of \textit{and} 
      and \textit{or} nodes. 
      \item
      It enables us to ask such questions
      as: \textit{What would be usage of inhabitation trees with 
      number labeled edges coming from \textit{or}-nodes?.}
      \end{enumerate}   
\item We can think about the inhabitation tree as about 
      some kind of search space. But another view is also 
      possible: We can think about it as about collection
      of typed lambda terms that are generable by that inhabitation
      tree. 
      
      This is even more visible when we introduce
      concept of an \textit{incomplete inhabitation tree};
      such a tree is created by omission of some subtrees
      with and-node roots (i.e., those accessible by \textit{or} edge).
      \textit{Finite incomplete inhabitation tree} corresponds
      to finite collection of typed \lterms. In very natural 
      sense we can call it a \textit{typed multiterm} --- a collection
      of typed terms forming one tree where those terms share
      the common parts.
      
      Thus the process of term generation is bidirectional ---
      from (incomplete) inhabitation tree (seen as collection) 
      we may obtain its element by solving it. But we may also 
      take collection of typed trees and compress them
      into one tree by sharing their common parts, which
      happens to be the (incomplete) inhabitation tree 
      that can be solved precisely by those terms which 
      constructed it.   
             
\end{enumerate}

In the next chapter we will use A*-algorithm together with
inhabitation tree ideas to perform systematic generation and other
kinds of generation of \lterms.
%;
%but maybe even more interesting application of inhabitation trees
%involving some above mentioned ideas will be revealed in the 
%chapter about future work, since those ideas are not yet implemented.

%\newpage
%\section{ Inhabitation machine }
%
%\begin{todo}
%  & Když na to zbude čas, tak popsat Inhabitation Machine...
%   && něco jako na závěr ještě ve stručnosti popišme 
%      IM, strukturu ze který je odvozenej IT. 
%  & říct, že je to hezkej konstrukt vzhledem k tomu že je konečnej
%  & ale je popsanej velmi zrychla (v tý knize) a implementační
%     detaily jsou celkem vynechaný
%  & na stromový verzi jsou implementační detaily hezky vidět
%  & a navíc pro polymorfní verzi jsou už i IM nekonečný takže
%     se strácí i ta konečnostní výhoda
%  & dát tam obrázek 
%\end{todo}


		

\chapter{ Design of our system }	

In this chapter we will describe our system trying to 
dive into implementation details as little as possible.
Some of those details will be revealed in the next chapter
devoted to the implementation. 

This chapter will be
presented in the mix between the mathematical terminology 
described in the previous chapter and the algorithmic terminology.\\

We can summarize our approach to design of 
the system for Genetic programming with types
by its main design goals:\\ 

\begin{enum}
 & It should eliminate the \textit{closure} constrain of standard GP
   (see \ref{building-blocks}). 
 & It should be generalization of the standard GP described in \ref{GP},
   by which is meant that for \textit{building blocks} satisfying 
   constrains of standard GP there should be simple setup of control parameters
   of the system which will make the system behave in precisely the same way
   as the standard GP behaves. 
 & It should utilize the theory around typed $\lambda$-calculus.  
\end{enum}

\section{Individuals and Building blocks}

We have two competing variants for representing individuals ---
\textit{\atTree} and \mbox{\textit{\sexprTree}} (see \ref{tree-reps}).

Individuals generated by solving inhabitation tree 
are \sexprTree{}s, which can be easily translated to \atTree{}s. \\


Before we start generating we do one important trick.
Suppose we want individuals of the type 
$\sigma = \tau_1 \ar \dots \ar \tau_n \ar \alpha$
from building blocks context $\Gamma$.

All our individuals will have form $\lam{x_1\,\dots\,x_n}{M}$.
%Later in this chapter we will discuss problems occurring with variables
%when performing subtree swap in order to perform crossover.
But since those variables are same for all individuals, 
we may do following trick.

Instead of generating individuals for
$(\tau_1 \ar \dots \ar \tau_n \ar \alpha,\Gamma)$
we generate individuals for $(\alpha;\Gamma,x_1:\tau_1,\dots,x_n:\tau_n)$
and remember to add the $\lambda$-head $\lh{x_1\,\dots\,x_n}$ to
each individual when evaluating it.\\

This trick has two reasons:
\begin{enumerate}
 \item Later in this chapter we will discuss problems occurring with variables
       when performing subtree swap in order to perform crossover.
       We solve those problems by performing \textit{abstraction elimination},
       which eliminates all occurences of variables and $lambda$-abstractions
       in a \lterm. But those problems are caused by local variables. 
       Since $x_1\,\dots\,x_n$ are common for all \lterms, it is valid
       to exclude them from this elimination. Since abstraction elimination
       prolongs size of \lterms it is beneficial to lessen 
       the effect of abstraction elimination in this way. 
        
 \item It also makes our method compatible with standard GP, where
       variables are treated as ordinary element of the set $T$
       and no $\lambda$-abstraction is used.
       
       This ensures that S-expressions will be generated for
       $\Gamma$ satisfying constrains of standard GP and that
       abstraction elimination will have no effect on such individuals.
\end{enumerate}




%\begin{todo}
% & několik fází, několik reprezentací
% & během generování se reprezentace mění 
%   && nejdří obsahuje lambdy i volný proměnný pak už ne
%      &&& vygeneruje se v \sexprTree
%      &&& ta se převedena na \atTree aby jsme provedli abstraction elimination
%\end{todo}

\section{Generating individuals}

%\begin{todo}
%& Dřív se to menovalo 
%"Our approach to \textit{solving} Inhabitation tree"
%tak tu napsat pěknej uvod o tom, že naši jedinci jsou lambda
%termy a že je  budeme generovat pomocí 
%řešení inhabitačních stromu.
%\end{todo}


Input for term generating algorithm is pair $(\sigma,\Gamma)$ where:\\

\begin{enum}
 & $\sigma$ is desired type of each generated individual.
 & $\Gamma$ is \textit{building blocks} context. 
\end{enum}~
 
Each generated \lterms $M$ must  satisfy \GMS. \\

We generate terms in long normal form.
This is achieved by \textit{solving} inhabitation tree for $(\sigma,\Gamma)$.  
The main design choice behind our approach to solving inhabitation trees
is ability to be variable enough to enable choice between 
\textit{systematic}  
and \textit{ramped half-and-half} (described in \ref{GPgene}) 
methods of generating terms 
by choice of simple search strategy.\\

\Lets explain what is meant by \textit{systematic} 
method of generating terms. 

Systematic way of solving inhabitation tree corresponds to
generating terms in their long normal forms
in order from smallest to largest in number of symbols and \lheads.

We will achieve this by using \textit{A* algorithm}. 

%That means minimizing number of and-nodes in 
%and-or-tree view of point on inhabitation tree.
%But since inhabitation tree is a special kind of 
%and-or-tree with alternating layers of 
%and-nodes (symbols and \lheads) and or-nodes 
%(types) it is also minimizing number of 
%size of whole inhabitation/and-or tree. 


\subsection{A* algorithm}

A* is a general informed search algorithm used for finding least-cost 
path from a given start state to some goal state in a state space.
According to \cite{AIAMA} A* is the most widely known form of best-first search.

By state space is meant oriented weighted graph with states as vertices and edges 
labeled with numbers corresponding to distance or cost. 
Edge from $s_i$ to $s_j$ means that state 
$s_j$ is reachable from state $s_i$ in one step with cost $dist(s_i,s_j)$.
In pseudocode we will refer to set of states reachable from 
state $s$ in one step as to $s.nexts()$.

Typically we use A* algorithm to find path from one specific start state to another 
specific goal state. But sometimes  
we can be interested only in finding a goal state which 
is specified by some $isGoal$ predicate give to the algorithm as
input (and the state space may contain many such goal states). 
This later variant corresponds to our situation, 
so we will continue by describing this variant.

As A* traverses the state space, it follows a path of the lowest 
expected total cost. The expectation is based on \textit{heuristic}
function which for state $s$ predicts distance to the nearest goal state. 
It uses a priority queue of states as its crucial data structure.
In this queue the priority of state $s$ is 
the total expected cost of the path from start state 
going through $s$ and continuing to the nearest goal state. \\

Here follows A* algorithm written in pseudocode:  

\Pseudokod{A*( start , isGoal , heuristic )}{
	$open \la$ empty priority queue \;
	$closed \la$ empty set \;\;
	
	$start$.G = 0 \;
	$start$.F = 0 + $heuristic$( $start$ ) \;\;
	
	$open$.insert( $start$ )\;\;	
	
	\While{$\neg$ $open$.isEmpty() }{
		$state \la$ $open$.popStateWithLowestF()\;\;
		
		\If{ isGoal( $state$ ) }{
			\Return $state$ \;
		}\;
		
		closed.insert( $state$ ) \;\;
		
		\For{$next \in$ $state$.nexts() }{		
		    $newG \la$ $state$.G + dist($state$,$next$)\;\;

			\If{ $( next \not\in open \wedge next \not\in closed ) \vee newG < next.$G}{\;
			
				$next$.G $\la newG$                          \;
				$next$.F $\la newG + heuristic$( $next$ )  \;\;
				
				\If{ $next \not\in open$ }{
					$open$.insert( $next$ )\;
				}
					
			
			}						
		}
	}\;
	
	\Return no-reachable-goal 
}	

Now we will modify this general A* algorithm so
it will be more suited for our needs. We want following 
properties:\\

\begin{enum}
& The state space is a tree.
& $dist(s_i,s_j) = 1$
& We do not want to find one, but $n$ goal states. 
\end{enum}~

For state space which is a tree the condition
$(next \not\in open \wedge next \not\in closed)$ 
is always true, since it is possible
to come to a state only from its parent state. 
This fact results in simplification of algorithm's code and behavior;
we may completely omit the $closed$ set and the checks in the for loop.\\

Those changes result in the following code:
 
\Pseudokod{Our-A*( start , n , isGoal , heuristic )}{	

	$open    \la$ empty priority queue \;
	$results \la$ empty set\;\;	
	
	$start$.G = 0 \;
	$start$.F = $heuristic$( $start$ ) \;\;
	
	$open$.insert( $start$ )\;\;	
	
	\While{$\neg$ $open$.isEmpty() }{
	
		$state \la$ $open$.popStateWithLowestF()\;\;
		
		\If{ isGoal( $state$ ) }{
			
			$results$.insert( $state$ )\;
						
			\If{ $\vert results \vert = n$ }{
				\Return $results$ \;
			}
		}\;
		
		\For{$next \in$ $state$.nexts() }{
			$next$.G $\la $state$.G + 1$ \;
			$next$.F $\la $next$.G + heuristic$( $next$ )  \;
			$open$.insert( $next$ )\;			
		}
	}\;
	
	\Return results
}


The most important part of A* algorithm is the heuristic function.
A heuristic function must be \textit{admissible} in order to make
A* algorithm find the shortest possible path.

A heuristic function $h$ is said to be \textit{admissible} if it satisfies 
following condition for every state $s$:
$$ h(s) \leq h^{*}(s) $$

where $h^{*}(s)$ is the true length of the shortest path from $s$ to 
the nearest goal state.

In other words, a heuristic function is \textit{admissible} if it is
\textit{optimistic}.

\subsection{Our state space}

% - Stavový prostor 
%   Nedokončený term
%   Navíc unfin typ gama
%   my jen podmnozinu urcenou funkci nexts() a vsemi triv koreny unfin typ gama
%   Vzdy vezmeme nejlevejsi unfin a udelame z nej konkrétnější term pomoci 
%   vyberu v or uzlu tim nam vzniknou 
%   nove unfin uzly odpovidajici pozadavkum and uzlu.
%   Protoze vyplnovani jednotlivych vetvi je na sobe nezavisle 
%   tak je jedno v jakem poradi je budem vyplnovat 
%   a tak je nase "z leva doprava" ok.
% - isFinal(state)
%   Term který neobsahuje unfin uzel
% - Heuristika 
%   simple -pocet unfin uzlů. 
%   Lepsi - predpocitat si to chytre s gama' a udelat si mapu v kery kdyz to neni tak dat 1.
% - Vic nez 1 reseni
%   staci pokracovat pri nalezeni reseni.



In order to describe our state space to search in \lets introduce
extended version of our tree representation for typed \lterms from \ref{typed-tree-reps}.
This extension consists in adding a new kind of leaf node $(\sigma,\Gamma)$ 
standing for unfinished tree of a type $\sigma$ with context $\Gamma$ to use
as set of building blocks.\\
%\footnote{\textbf{TODO}: Tady to napsat nějak líp s tim gamma }

Initial start states are all of a form "one node tree $(\sigma,\Gamma)$" since we
are trying to generate a typed \lterm of a type $\sigma$ from building blocks $\Gamma$.\\

A tree with no unfinished leaf is considered as final state.  \\

We will define our state space inductively by specifying algorithm for obtaining 
successors of a state.

A non-final state must have one or more unfinished leafs. 
One of those unfinished leafs is selected. 
It is the first one found by depth-first search (from the root of a state tree)
i.e., the leftmost unfinished leaf (if we consider that tree written as expression).

Let $(\sigma,\Gamma)$ be the selected leaf. Successor state is constructed 
by replacing this leaf by new subtree. We must distinguish two cases of $\sigma$ :
\textit{Atomic type} $\sigma$ and \textit{function type} $\sigma$.\\


\textbf{Atomic type} $\sigma = \alpha $, $\alpha \in A$.

For atomic $\sigma$ there are as many successors as there are members of $\Gamma$ of a form 
$(f,\rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \boldsymbol{\alpha} )$,
where $m \geq 0 $. In other words all the members of $\Gamma$ of a type
$\alpha$ or of a type for function which returns $\alpha$.

For each 
$(f,\rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \boldsymbol{\alpha} ) \in \Gamma$,
where $m > 0$ the new subtree which will replace $(\sigma,\Gamma)$ has following form: \\

\Tree
   [.$\alpha$
	[.f	
 		\text{$(\rho_1,\Gamma)$}
 		\text{$(\rho_2,\Gamma)$}
 		\text{$\dots$} 		
 		\text{$(\rho_m,\Gamma)$}		 				 			
	]   
   ]\\

And for m = 0:

\Tree [.$\alpha$ f ] \\


\textbf{Function type} 
$\sigma = \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$
where $n \geq 1, \alpha \in A$.

For function type $\sigma$ there is exactly one successor.

For every  $i \in \{1,\dots,n\}$ we create new \textit{variable name} $x_i$ which is not yet used anywhere in the whole tree.
%included in context $\Gamma$ as variable or constant name.

The new subtree which will replace $(\sigma,\Gamma)$ has following form: \\

\Tree
   [.\text{$\tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$}
	[.\text{$\lambda x_1 \dots x_n$}	
 		\text{$(\alpha,\Gamma \cup \{ (x_1,\tau_1) , \dots , (x_n,\tau_n) \})$}		 				
	]   
   ]\\[3em]

We should discuss that it is correct to have as successors of a state those
created by expansion of the leftmost unfinished leaf (in contrast with
more broad successor set where any unfinished leaf may be expanded).
What we need to show is that no final state will be omitted by the method 
and that there is no shorter path omitted. 

One can see that it is correct by considering that 
the order of expanding unfinished leafs is irrelevant 
since expansion of each unfinished leaf is 
independent from expansion of any other unfinished leaf
(up to renaming of variables which is irrelevant).

Every unfinished leaf of a state must be replaced by 
tree containing no unfinished leafs in order to be final state. 
But since expansion of an unfinished leaf has no impact 
(besides new variable names) on expansion of other unfinished
leafs we may choose the order of expansions arbitrarily. \\

We prefer this variant because:
 \begin{enum}
 	& It has smaller or equal number successors for each state, 
 	  i.e. smaller or equal branching factor.
 	& It makes the state space a tree, in contrast with
 	  the naive state space which is not a tree. 
 \end{enum}

 
%\begin{todo}
%Mužem na to koukat jako na takovou tu redukci simetrií, 
%najít to v AIAMA a pořádně popsat.
%\end{todo}



\subsection{Our heuristic function}

A heuristic function takes as input a state and returns as output an estimation of number
of steps needed to get to a final state by the shortest possible path.

Our heuristic function is a simple one: The number of unfinished leafs in the state tree.

This heuristic function is admissible because every unfinished
leaf must be at least once expanded to become final. 


\subsection{Further improvements}

We may easily put all deterministic expansion steps into one step 
--- by which we mean the lambda head expansion. This could be
done when expanding atomic type by also expanding its 
non-atomic arguments. This would result in that only atomic types
wold by in unfinished leafs. It is important to accordingly change
the \textit{dist} in A* which is now 1 generally. 

It is also worth mentioning that \textit{dist} 1 is also for
lambda heads with long list of variable names in the current specification --- we do not bother ourselves with this detail.
But when it is not the desirable behavior it is very easy to change
by using mor complicated \textit{dist} than 1.



\subsection{Search strategy}

Now we will describe further modification of the A* algorithm 
which is generalizing it enough to enable use of \textit{ramped half-and-half}
term generating method and other term generating methods.\\


It is done by adding one more input argument for the algorithm.
This additional argument is a search strategy whose purpose is
to determine which successor states of all possible successor states 
will be added to the priority queue, i.e., a search strategy 
is there to filter out some successor states from $state.nexts()$.
This filtering may be non-deterministic. It is based on
the depth of expanded unfinished leaf and on properties of 
the newly added subtree, e.g., whether the newly added subtree
contains one or more unfinished leafs (so it corresponds to non-terminal) 
or not (so it corresponds to terminal). 

By \textit{depth} we really mean $\lfloor depth/2 \rfloor$, because our 
formal state trees contain those type nodes above each symbol node. 
\\


In previous situation where there is no filtering of states involved 
we know that if queue becomes empty, then we have exhaustively searched
all possible states. 

But since filtering is involved this is no longer true.
Therefore, if the queue becomes empty before the required number of
terms is generated, then the generating process is restarted and 
it continues to generate terms.

We also enable strategy to have some internal state which is 
initialized (possibly non-deterministically) each time the 
generating process restarts. \\

Hopefully those changes will be clarified by the following pseudocode:

\Pseudokod{A*-with-strategy( start , n , isGoal , heuristic , strategy )}{	

	$results \la$ empty set\;\;	
	

	\While{ $\vert results \vert < n$ }{ 		
		
		$strategy.init()$\;\;
		
		$open    \la$ empty priority queue \;\;
		
		
		$start$.G = 0 \;
		$start$.F = $heuristic$( $start$ ) \;\;
		
		$open$.insert( $start$ )\;\;	
		
		\While{$\neg$ $open$.isEmpty() }{
		
			$state \la$ $open$.popStateWithLowestF()\;\;
			
			\If{ isGoal( $state$ ) }{
				
				$results$.insert( $state$ )\;
							
				\If{ $\vert results \vert = n$ }{
					\Return $results$ \;
				}
			}\;
			
			$nexts \la strategy.filter$( $state$.nexts() , $state$.depth() ) \;\;			
			
			\For{$next \in nexts$ }{
				$next$.G $\la $state$.G + 1$ \;
				$next$.F $\la $next$.G + heuristic$( $next$ )  \;
				$open$.insert( $next$ )\;			
			}
		}
	}\;
	
	\Return results
}

\Lets show examples of search strategies. 
They are described by describing 
their $filter(nexts,depth)$ 
and $init()$ methods\footnote{It is also needed to 
add forceUniqueness() boolean predicate to the strategy
since Koza's ramped half and half requires uniqueness.
Let us skip this step here in this explanation,since it can be easily implemented but adds lot of unnecessary distraction. May the reader forgive.}
. Again, in order to avoid confusion, 
\lets stress that $depth$ is really $\lfloor depth/2 \rfloor$ since
our formal states contain above each symbol node a type node.





\subsubsection{Systematic strategy}

The previous systematic behavior may be obtained by trivial 
$filter$ function which returns all $nexts$.
Systematic strategy needs no internal state, 
therefore $init$ is doing nothing.

\subsubsection{Ramped half-and-half strategy}

In the $init$ are randomly (uniformly) initialized two variables of the
strategy internal state:

\begin{enum}
 & \textit{isFull} - A boolean value, determining whether \textit{full}
                     or \textit{grow} method will be performed.
 & \textit{d} - A integer value from $\setDots{2}{D_{init}}$, where 
                $D_{init}$ is predefined maximal depth (e.g. 6).                    
\end{enum}~


The method $filter(nexts,depth)$ returns precisely one randomly
(uniformly) selected  element from a subset of $nexts$
(or zero elements if $nexts$ is empty). 
This means that the queue always contains only one (or zero) state.

The subset to select from is determined by $depth, d$ and $isFull$.\\


Those elements of $nexts$ whose newly added subtree contains one ore more 
unfinished leafs are regarded as \textit{non-terminals}, whereas 
those whose newly added subtree contains no unfinished leaf are regarded as 
\textit{terminals}.\\


If $depth = 0$, then the subset to select from is  
set of all \textit{non-terminals} of $nexts$.

If $depth = d$, then the subset to select from is
set of all \textit{terminals} of $nexts$.\\


In other cases of $depth$ it depends on value of $isFull$.

If $isFull = true$, then the subset to select from is 
set of all \textit{non-terminals} of $nexts$.

If $isFull = false$, then the subset to select from is 
whole $nexts$ set.

%\begin{todo}
%& eště se musí nějak prohrotit to, že tam je navíc ta lambda hlava
%kerá ještě o něco kazí tu $depth$  - to už uplně neplatí tim že je v
%uvodu popsaný to že se ta hlava ukousne ale stejně to ještě pořádně zkontrolovat
%& ukázat že je to fakt to samý co kozovský klasický generování
%& zavést tam pojem run pro ten jeden vnější while cyklus 
% (i když to je asi jasný z kontextu)
%\end{todo}



\subsubsection{Geometric strategy}


We can see those two previous strategies as two extremes on the spectrum of 
possible strategies.

\textit{Systematic strategy} filters no successor state thus performing
exhaustive search resulting in discovery of $n$ smallest terms in one run.

On the other hand, \textit{ramped half-and-half strategy} filters 
all but one successor states resulting in degradation of 
priority queue to "fancy variable". 
And each term is generated in its own run.\\

\textit{Geometric strategy} is simple yet fairly effective term generating 
strategy somewhere in the middle of this spectrum.

It is parameterized by parametr $q \in (0,1)$, its default well-performing 
value is $q = 0.75$. \\

For each element of $nexts$ it is probabilistically decided whether
it will be returned or omitted. A probability $p$ of returning is
same for all elements, but depends on $depth$. 
It is computed as follows:

$$ p = q^{depth} $$
   
This formula is motivated by idea that it is important to
explore all possible root symbols, but as the $depth$ 
increases it becomes less "dangerous" to omit 
an exploration branch. 

We can see this by considering that this strategy results in
somehow undisciplined A* search.
With each omission we make the search space smaller. But with
increasing depth these omissions have smaller impact on the search space,
i.e., they cut out lesser portion of the search space.

Another slightly esoteric argument supporting this formula is that "root 
parts" of a program usually stand for more crucial parts
with radical impact on global behavior of a program, 
whereas "leaf parts" of a program usually
stand for less important local parts (e.g. constants).  

It also plays nicely with the idea that "too big trees should be killed".\\



Systematic strategy does not have an internal state, thus $init$ does nothing.


\subsection{Final remarks}
We should mention that since simply typed lambda calculus
is equivalent to implicational fragment of intuitionistic logic
it wold be possible to do term generation by prover for this logic. But logic prover is concerned with finding one proof of a difficult
proposition, whereas types usually tend to be easy propositions
but we need enormous amount of them. So this two fields, although 
equivalent in theory do not have equivalent goals in practice.\\

We have also tried generation directly from grammar rules
corresponding to basic inference rules -- we did it before
encountering the work of \cite{barendregt10}, but since this has two
drawbacks we have not supported the implementation of the former
term generator to the actual state of source code. 

Those drawbacks are that it is not in normal form and that
it is not obvious which type shoud by added in the grammar 
rule corresponding to the \textit{$\ar$-elimination}.

%\begin{todo}
% 
%& sjednotit terminologii nexts successors atd
% 
%& Odiskutovat nějak tohle:
%přirozeně chceme při produkci velkýho množství termu obhospodařovávat nějaký
%velký množství rozdělanejch termu v nějaký struktuře a postuipně je dodělávat.
%Proto přichází jako tahle struktura na mysl nějaká prioritní fronta,
%otázka je kde vzít ty priority a A* je jednoduchá ale přitom optimum hledající 
%odpověď na tuhle otázku, dostatečně volná skrz tu heuristiku.  
%
%& určitě někde zmínit ospravedlnění proč si napsat dokazovač sám
%a né požít nějakej už hotovej - de o to, že dokazovače jsou delaný na to aby
%udělali jeden dukaz typicky složitýho tvrzení, my sme v situaci kdy
%chceme co nejvíc (a co možná co nejvíc rozmanitejch) dukazu jednoduchýho problému. 
%
%& nekde zmínit že s těma nexts nezacházíme takle dementně, že samozřejmě vybíráme 
%jen z těch přidávanejch podstromu ale že tim nechcem komplikovat popis 
% 
%\end{todo}


%\subsection{Other tried approach to generating individuals}
%
%\begin{todo}
% & popsat tu ten puvodní přístup, kde se bralo přímo 
%   zavedení a eliminace pro každej typovej konstrukt 
%   a podle toho se pak udělali příslušný term generating grammar pravidla
%   kerý odpovídaj manipulacímse strommem
% & ale má nevýhody
%   && hlavně to neni v normální formě, takže generujeme spoustu 
%      ruzně dopočítanejch beta-rovnáse-ekvivalentních termu
%   && u pravidla zavedení implikace neni uplně zřejmý
%      co tam použít za typ
%   && takže sice to na to ze začátku běhalo ale nakonec to bylo vyměněno
%      za IM metodu since ta je jednoduší, líp se přemejšlí o ní,
%      konstrukty jako and a or nemusíme zavádět na urovni pravidel
%      ale stačí tam dat vhodný polymorfní konstruktory/destruktory 
% & nějak tu metodu pomenovat a přemenovat podle toho nadpis
%\end{todo}





\newpage
\section{Crossover}

Design goal behind our approach to crossover operation is
to try to generalize standard tree swapping crossover.
This is not because of dislike of more specialized crossovers;
it comes from systematic approach for exploring the domain of
typed functional GP --- thus I would like to start with the most common one.\\

%In the future, after finding sufficiently nice generalization of the standard tree %swapping crossover I plan to move my attention to more 
%specialized crossovers, like the one presented in \cite{yu01}.  \\

The crossover operation in standard GP is performed 
by swapping randomly selected subtrees in each parent 
S-expression (see \ref{GPxover}).

For typed lambda terms two difficulties arise: Types and variables.\\

We will show how to crossover \textit{typed \lterm trees} in both 
\atTree and \sexprTree notation.\\

As in standard GP our crossover will be performed by swapping
two subtrees. But now with constraint that both subtrees have
the same type. This is discussed in grater detail in \ref{typed-swapping}.\\

Variables bring more difficulties then types do.
This problem arises from variables that are free in subterms corresponding 
to swapped subtrees. 

Following example illustrates the problem. \Lets have these two
parent trees with selected nodes in bold.\\



\Tree [.$\lh{x_1}$ [.f [.$\lh{x_2}$ [.\textbf{g} $x_2$ c ] ] $x_1$ ] ]
\Tree [.$\lh{x_1}$ [.h $x_1$ $\mathbf{x_1}$ ] ]

~\\The swap of subtrees results in following trees:\\

\Tree [.$\lh{x_1}$ [.f [.$\lh{x_2}$ $\mathbf{x_1}$ ] $x_1$ ] ]
\Tree [.$\lh{x_1}$ [.h $x_1$ [.\textbf{g} $\mathbf{x_2}$ \textbf{c} ] ] ]
 
~\\The problem is that variable $x_2$ in second tree
is not bound by any $\lambda$-head and since
it is not element of $\Gamma$, the second tree is not well-typed \lterm.  

% - This is only one example of a problem caused by swapping subtrees with variables,
% - Resolve problems with free variables or avoid variables completely. 


%In \ref{repairing-method} is presented more detailed discussion of problems with
%variables and method which repairs such "broken" terms. 

First approach tried at this problem was to repair the broken term,
but since this method is not currently implemented we will describe it
in just a few words. If the moved free variable ends in the scope 
where is defined variable with the same name and type, then it
is OK. If it is not this case, we may rename this variable to some 
other variable "visible" in its new scope if such a variable has the same 
type. If there is no such a suitable variable we may generate new value 
to replace the variable by term generating method. 

As you can see, this is not much elegant solution. 

But we can avoid dealing with this problem by avoiding use of variables.
This can be achieved by process called abstraction elimination.
 

\subsection{Abstraction elimination}
\label{toSKI}
%... Conversion to SKI combinators

\textit{Abstraction elimination} is a process of transforming 
an arbitrary \lterm into \lterm that contains no lambda abstractions
and no bound variables.
The newly produced \lterm may contain function applications, 
free symbols from former \lterm and some new symbols standing for 
combinators $\Scomb$, $\Kcomb$ and $\Icomb$. \\

Those combinators are defined as:
\begin{align*}
\Scomb &= \lam{f\,g\,x}{f\,x\,(g\,x)} \\
\Kcomb &= \lam{x\,y}{x} \\
\Icomb &= \lam{x}{x} 
\end{align*}

\newcommand{\Ae}{\mathop{\mathrm{\AE}}}

\Lets describe transformation $\Ae$ performing this 
process.\footnote{In our implementation we must also deal with types, because 
our individuals are annotated with types. But since this process
is fairly straightforward but cumbersome to describe, we will 
skip explaining it here. Reader interested in seeing it can see 
the source code of our implementation. }
\begin{align*}
\Ae[x]           &= x &\\[0.4em]
\Ae[\,(M\,N)\,]  &= (\Ae[M]\;\;\Ae[N]) &\\[0.4em]
\Ae[\lam{x}{x}]  &= \Icomb &\\
\Ae[\lam{x}{M}]  &= (\Kcomb~\Ae[M]) &\textbf{if } x \not\in \FV(M)\\
\Ae[\lam{x}{\lamb{y}{M}}] &= \Ae[\lam{x}{\Ae[\lam{y}{M}]}]  
&\textbf{if } x \in \FV(M)\\
\Ae[\lam{x}{(M\,N)}] &= (\Scomb~\Ae[\lam{x}{M}]~\Ae[\lam{x}{N}])  
&\textbf{if } x \in \FV(M) \vee x \in \FV(N)\\
\end{align*}

\newcommand{\Bcomb }{\mathbf{B}}   
\newcommand{\BBcomb }{\mathbf{B*}}
\newcommand{\Ccomb }{\mathbf{C}}   
\newcommand{\CCcomb }{\mathbf{C'}}
\newcommand{\SScomb }{\mathbf{S'}}

This is simple version of this process. More optimized version,
in the means of the size of resulting term and its performance
is following one, presented in \cite{jones87}.\\

This version operates with more combinators:\\

\begin{align*}
\Bcomb  &= \lam{f\,g\,x}{f\,(g\,x)} \\
\Ccomb  &= \lam{f\,g\,x}{f\,x\,g} \\
\SScomb &= \lam{c\,f\,g\,x}{c\,(f\,x)\,(g\,x)} \\
\BBcomb &= \lam{c\,f\,g\,x}{c\,(f\,(g\,x))} \\
\CCcomb &= \lam{c\,f\,g\,x}{c\,(f\,x)\,g} \\
\end{align*}

\newpage
And the transformation can be written as follows.

\begin{align*}
\Ae[x]           &= x &\\
\Ae[\,(M\,N)\,]  &= (\Ae[M]\;\;\Ae[N]) &\\
\Ae[\lam{x}{M}]  &= A[ x ; \Ae[M] ] &
\\[1em]
A[x;x]           &= \Icomb &\\
A[x;y]           &= (\Kcomb~y)\\
A[x;\,(M\,N)\,]  &= Opt[~\Scomb~(A[x;M])~(A[x;N])~]
\\[1em]
Opt[~\Scomb~(\Kcomb~M)~(\Kcomb~N)~]   &= \Kcomb~(M~N)\\
Opt[~\Scomb~(\Kcomb~M)~\Icomb~]       &= M\\
Opt[~\Scomb~(\Kcomb~M)~(\Bcomb~N~L)~] &= \BBcomb~M~N~L\\
Opt[~\Scomb~(\Kcomb~M)~N~]            &= \Bcomb~M~N\\
Opt[~\Scomb~(\Bcomb~M~N)~(\Kcomb~L)~] &= \CCcomb~M~N~L\\
Opt[~\Scomb~M~(\Kcomb~N)~]            &= \Ccomb~M~N\\
Opt[~\Scomb~(\Bcomb~M~N)~L~]          &= \SScomb~M~N~L
\end{align*}
\\

As is stated in \cite{jones87},
the biggest disadvantage of this technique is that the translated
term is often much larger than in its lambda form --- the size of
the translated term can be proportional to the
square of the size of the original term. 

But the advantage is also tempting --- no need to deal with variables
and lambda heads.


\subsection{Typed subtree swapping}
\label{typed-swapping}

First thing to do in standard subtree swapping is to select random node
in the first parent. 

We modify this procedure so that we allow
selection only of those nodes with such a type that there exists  
a node in the second parent with the same type.

Standard subtree swapping crossover as a first thing selects 
whether the selected node will be inner node (usually with probability 
$p_{ip} = 90\%$) or leaf node (with probability 10\%).

We are in a more complicated situation, because one of those 
sets may be empty, because of allowing only nodes with possible "partner"
in the second parent. Thus we do this step only if both sets are
nonempty. 

After selecting a node in the first parent we select node in the
second parent such that type of that node must by the same as the type 
of the first node. Again, this may eliminate the "90-10" step of
first deciding whether the selected node will be internal node 
or leaf node.

When both nodes are selected we may swap the trees. \\

If the abstraction elimination was performed, then 
since the trees are of the same type and there are no variables to be 
moved from their scope, the offspring trees are well typed.\\


Both \sexprTree and \atTree are able to be crossed by this 
mechanism. But \atTree has more possibilities then \atTree.
This comes from the fact that every subtree of the \sexprTree
corresponds to a subtree of \atTree, but there are subtrees
of \atTree that do not correspond to a subtree of a \sexprTree.\\

Following example should clarify this.

\Tree[.@	
   [.@ \textbf{f} x ]
   [.y ]  		 			
]
\Tree[.\textbf{f} x y ]~\\

In \atTree, \textbf{f} is leaf thus subtree, 
whereas in \sexprTree it is internal node thus not a subtree.\\ 

Another nice property of \sexprTree{}s with no lambdas 
is that they are the same representation as S-expressions
used by standard GP.\\

Again, similarly as for standard version, 
a maximum permissible depth $D_{created}$ 
for offspring individuals is defined (e.g. $D_{created} = 17$).
If one of the offspring has greater depth than this limit, then 
this offspring is replaced by the first parent in the result of 
the crossover operator. If both offspring exceeds this limit, than 
both are replaced by both parents.  

For \atTree the $D_{created}$ must be larger since
\atTree (without lambdas) is a binary tree. This 
enlargement is approximately proportionate to average number of 
function arguments. We use generous $D_{created} = 17\times3$. 


%\subsection{ Repairing method }
%\label{repairing-method}
%
% 
%
%\begin{todo}
%& !!!
%\end{todo}
%
%
%\subsection{ Comparison of the two methods }
%\label{comarison-ski-repairing}
%
%\begin{todo}
%& !!!
%& říct že nakonec sme přilnuli k zbavit se proměnejch
%\end{todo}






\chapter{ Implementation }	

In this chapter we will briefly discus some parts of the implementation
of our system. Some previous knowledge of Haskell is assumed, but one can
freely skip this chapter since it is not crucial for the 
overall message of this thesis. \\

Our system is implemented in functional language \textit{Haskell}.
Program implementing it is called \textit{Fishtron}.
It runs as a simple server application and its GUI is
accessible through web browser.\\



Evaluation of individuals is performed by translating them
from \lterms into Haskell programs which are evaluated
in the Haskell interpreter \textit{Hint} accessible 
in the code as \texttt{Interpreter} monad.  \\

Each problem definition typically consists of two Haskell 
source code files. First file (usually called \texttt{Problem.hs}) 
contains problem definition involving such things as desired 
type of individual, context of building blocks, 
fitness function implementation and various control parameters 
like term generating method, crossover,
default number of runs, generations, population size etc. 

Those functions or constants of building block context
that are not part of the standard Haskell \textit{Prelude}\footnote{
A standard module imported by default into all Haskell modules.}
need to be defined in the second file (usually called \texttt{Funs.hs}).\\

After start of the program one can access the interface
by pointing a browser to \texttt{http://localhost:3000}. Here is simple 
form where registered problems are listed. Beside problem selection
one can also change some control parameters. After hitting the "start"
button evolution begins. From that point until the end of the running
experiment, the front-end JavaScript program periodically asks
server for new run information. Server makes this information available 
at the end of each generation. This information contains various
data: Haskell and JavaScript source codes of the best individual,
some statistical data about the just evaluated generation,  
standard output of the server program, etc.

These data are immediately available in the form of
continuously updated graphs and if 
the \textit{phenotype} JavaScript file is present 
for the problem, then is also available immediate 
presentation of the best individual from each generation.
JavaScript functions standing for best individuals are 
also immediately accessible through JavaScript console,
e.g. for debugging purposes.\\  
  
Now \lets move our attention to the architecture of the evolution engine.
Various dirty parts of the outside world (such as textual output,
communication with front-end, access to random number generator) are  
packed into \texttt{Eva} state monad defined in \texttt{Eva.hs}.\\
  
Core of the evolution engine (defined in \texttt{GP\_{}Core.hs})
is done so it enables various structures
to be evolved - such as \sexprTree{}s, \atTree{}s or even boolean lists
of fixed length as to support beside GP also Genetic Algorithms.\\[1em]

This is possible due to following type classes. 
 
~\\\texttt{
class Gene term opt where\\
$~\;\;$generateIt :: Int -> opt -> Eva [term]\\[1em]
class Muta term opt where\\
$~\;\;$mutateIt :: opt -> term -> Eva term \\[1em]
class Cros term opt where\\
$~\;\;$crossIt :: opt -> term -> term -> Eva (term,term) \\[1em]
class Evolvable term a gOpt mOpt cOpt where\\
$~\;\;$evolveIt :: RunInfo -> Problem term a gOpt mOpt cOpt  \\
$~~~~~~~~~~~~~~~~~~~~~~$
-> Eva (term,FitVal,Maybe Int)
} ~\\


Put simply, if we define instances for 
\texttt{Gene}, \texttt{Muta} and \texttt{Cros}, then we get
instance of \texttt{Evolvable}. 

\texttt{Gene} correspond to individual generating method for
structure \texttt{term} parametrized by option \texttt{opt}.
Analogically for mutation and crossover.

Instances of those three type classes are located in the file
\texttt{GP\_{}Data.hs}.

We will skip deeper explanation of this topic now.\\

One technical detail that I think is worth mentioning here
is a trick involved in evaluation of Haskell programs 
corresponding to individuals. Straightforward evaluation 
technique would be to simply call evaluation function on every
string corresponding to individual expression. 

But since most time consuming activity
of evaluating using \textit{Hint} is its initialization,
it is much more effective to collect all the individuals  
to be evaluated in the current generation and create huge
string representing list of expressions
and evaluate this string of all individual. \\

Another notable comment about implementation comes
from fact that \textit{Hint} needs to know type of the expression
being evaluated. This particular thing had enormous enlarging effect
on pain produced by need to fight the type system in order to
make it reasonably working piece of software. This may be caused
by the fact that I am still pretty inexperienced Haskell programmer,
since this is my first bigger program done in this language.

Moral of this remark is the following one: 
If you plan to use the \textit{Hint}
in your Haskell program on place where it will be used to eval 
expressions of various types, think through the integration of the
evaluator into the whole system in advance.\\ 


\newpage
\Lets finish this brief chapter about implementation of our system
by listing other not yet mentioned source files with a little information 
about them.   \\[1em]


\begin{easylist}[itemize]
& \texttt{Server.hs}    --- File containing main of the server.
& \texttt{Register.hs}  --- Place for registering the problem to make it appear 
                            in the list of problems in GUI.
& \texttt{InhabTree.hs} --- Core of the term generating method.
& \texttt{TTerm.hs}     --- Code related to typed \lterms 
                            represented as \atTree{}s. 
& \texttt{TTree.hs}     --- Code related to typed \lterms 
                            represented as \sexprTree{}s.
& \texttt{Heval.hs}     --- Evaluator wrapper code.
& \texttt{Dist.hs}      --- Implementation of "probability distribution" data 
                            structure used as container for population.
\end{easylist}

~\\[6em]

\includegraphics[scale=0.4]{skryn.png}

\chapter{Results}
	In this section will be presented usage of the system in order to solve specific problems. Several problems will be described and some of them compared with results
of others. 

\section{Simple Symbolic Regression}

\textit{Simple Symbolic Regression} is a problem described
in \cite{koza92}. Objective of this problem is to 
find a function $f(x)$ that fits a sample
of twenty given points. The target function is 
function $f_{t}(x) = x^4 + x^3 + x^2 + x$.  

Desired type of generated programs $\sigma$ and 
building blocks context $\Gamma$ are following.
\newcommand{\Real}{\mathbb{R}}
\begin{align*}
\sigma = \Real \ar &\Real\\
\Gamma = \{
  (+)  &: \Real \ar \Real \ar \Real    ,\\
  (-)  &: \Real \ar \Real \ar \Real    ,\\
  (*)  &: \Real \ar \Real \ar \Real    ,\\
  rdiv &: \Real \ar \Real \ar \Real    ,\\
  sin  &: \Real \ar \Real              ,\\
  cos  &: \Real \ar \Real              ,\\
  exp  &: \Real \ar \Real              ,\\ 
  rlog &: \Real \ar \Real              \}\\
\end{align*}
where
\begin{align*}
rdiv(p,q) &= \begin{cases} 1 &\mbox{if } q = 0 \\
p/q & \mbox{otherwise } \end{cases}  \\
rlog(x) &= \begin{cases} 0 &\mbox{if } x = 0 \\
log(\vert x\vert) & \mbox{otherwise}. \end{cases}
\end{align*}

Fitness function is computed as follows

$$ fitness(f) =  \sum\limits_{i=1}^{20}{ \vert f(x_i)-y_i }\vert   $$

where $(x_i,y_i)$ are 20 data samples from $[-1,1]$, such that $y_i = f_t(x_i)$.\\

An individual $f$ such that $\vert f(x_i)-y_i \vert < 0.01 $ for all data samples is 
considered as a correct individual.\\

\newpage
\subsection{Experiments}

On this problem we demonstrate that if control parameters are set to be
compatible with standard GP, then the results obtained fit the results
presented in \cite{koza92} relatively well. \\

Those control parameters are following two:

~\begin{enum}
 & \textit{Ramped half-and-half strategy} is used as search strategy.
 & Option of preserving the best individual into the next population is set \textbf{off}.
\end{enum}~

The experiment consisted of 50 independent runs of GP algorithm.
Each run had maximally 50 generations (50 + 1 for generation 0)
and 500 individuals as population size.\\

We analyze the ability of the system to produce a correct solution.
We are interested in percentage of runs that led to discovery of
correct solution. And we also use the popular measurement  
method within GP field --- the \textit{performance curves}
described in \cite{koza92}.

First performance curve is the cumulative \textit{probability of success}
$P(M,i)$, where $i$ is generation number ($i = 0$ for initial population) 
and $M$ is the size of the population. 
$P(M,i)$ is probability that a correct solution will
be find in one run of GP with maximal number of generations $i$ and 
population size $M$. $P(M,i)$ is computed as percentage of 
runs yielding a correct solution in generation $i$ or earlier.

From values of $P(M,i)$ are computed values of the second performance curve
$I(M,i,z)$
--- the total number of individuals that must be processed to yield a correct
solution with probability $z$ by multiple runs of GP with maximal 
number of generations $i$ and population size $M$ (usually $z = 0.99$).

As is described in \cite{koza92},
The number $R(M,i,z)$ of independent runs required to yield a correct solution 
is computed as
 
$$ R(M,i,z) = \lceil\frac{\log(1-z)}{\log(1-P(M,i))}\rceil ,$$

thus $I(M,i,z)$ is computed as

$$I(M,i,z) = (i+1) \cdot M \cdot R(M,i,z) .$$\\

It scored 17/50 (34\%) success rate. 
Minimal $I(M,i,z)$ was in generation 23 
with 192,000 individuals to be processed.
The experiment took 46 minutes.\\

%Performance curves are following.
%\includegraphics[scale=0.65]{reports/SSR/1/probabs.png}
%\includegraphics[scale=0.65]{reports/SSR/1/indivs.png}\\

\Lets compare it with results from \cite{koza92} (based on 113 runs):

Success rate 35\%; minimal $I(M,i,z)$ in generation 24 
with 162,500 individuals to be processed.
 This seems like a pretty good match.
\\



Second experiment was performed to compare 
\textit{ramped half-and-half strategy} with our
\textit{geometric strategy} (with default parameter $q=0.75$). 
Thus all control parameters stay same except for the 
search strategy for generating algorithm.

It scored 21/50 (42\%) success rate. 
Minimal $I(M,i,z)$ was in generation 29 
with 150,000 individuals to be processed.
The experiment took 26 minutes.\\

So our geometric strategy is slightly more successful then 
ramped half-and-half strategy. But more interesting is that
it made the whole experiment almost twice as fast; 
reason of this will be discussed in more detail in the next problem. 

\includegraphics[scale=0.45]{reports/SSR/p.png}

\includegraphics[scale=0.45]{reports/SSR/i.png}

%\includegraphics[scale=0.65]{reports/SSR/2/probabs.png}
%\includegraphics[scale=0.65]{reports/SSR/2/indivs.png}\\



\newpage
\section{Artificial Ant}

\textit{Artificial Ant} is another problem described
in \cite{koza92}. Objective of this problem is to 
find a control program for an artificial ant so
that it can find all food located on "Santa Fe" trail.

The Santa Fe trail lies on toroidal square grid.
The ant is in the upper left corner, facing right.
It has following layout.

\includegraphics[scale=1]{santafe.png}



The ant is able to move forward, turn left, and sense if a food 
piece is ahead of him.

\newpage

Desired type of generated programs $\sigma$ and 
building blocks context $\Gamma$ are following.
\begin{align*}
\sigma = An&tAction\\
\Gamma = \{~~
  l    &: AntAction                              ,\\
  r    &: AntAction                              ,\\
  m    &: AntAction                              ,\\
  ifa  &: AntAction \ar AntAction \ar AntAction  ,\\
  p2   &: AntAction \ar AntAction \ar AntAction  ,\\
  p3   &: AntAction \ar AntAction \ar AntAction \ar AntAction  \}
\end{align*}

Action $l$ turns the ant left.

Action $r$ turns the ant right.

Action $m$ moves the ant forward.

Action $ifa x y$ (if-food-ahead) performs action $x$ if a food piece is ahead of the ant,
otherwise it performs action $y$.

Action $p2 x y$ first performs action $x$ and after it action $y$.

Action $p3 x y z$ first performs action $x$, after that action $y$ and finally $z$.\\

Actions $l, r$ and $m$ each take one time step to execute.

Ants action is performed over and over again until it reaches predefined
maximal number of steps. In \cite{koza92} this limit is set to be 400 time steps.

Fitness value is equal to number of eaten food pieces.

An individual such that eats all 89 pieces of food is 
considered as a correct solution.\\

There is also mentioned following ant program which is described as correct solution.

\texttt{~\\
(ifa m (p3 l (p2 (ifa m r) (p2 r (p2 l r)))(p2 (ifa m l) m)))\\
} 

But here arises a problem: In our implementation this program needs 545
time steps; if it is given only 400 time steps, then it eats only 79 pieces
of food. It is uncertain whether it is caused by my mistake in implementation
or by inaccuracy in \cite{koza92}. Since everything else seems working just 
fine, I have decided to set the limit to 600 in my experiments.


\subsection{Experiments}


Due to those uncertainties, we will not perform comparing experiments 
as we did in the problem of simple symbolic regression.\\

In the following experiments, the option of preserving the best individual 
into the next generation is active (since we are not trying to compare 
with results in \cite{koza92}).





As usually, first experiment consisted of 50 independent runs of GP algorithm.
Each run had maximally 50 generations and 500 individuals as population size.
\textit{Ramped half-and-half strategy} is used as search strategy for the first 
experiment.\\


It scored 7/50 (14\%) success rate. 
Minimal $I(M,i,z)$ was in generation 28 
with 449,500 individuals to be processed.
The experiment took 265 minutes.

%Performance curves are following.
%\includegraphics[scale=0.65]{reports/Ant/2/probabs.png}
%\includegraphics[scale=0.65]{reports/Ant/2/indivs.png}\\

Similarly as in previous problem, second experiment was performed to compare 
\textit{ramped half-and-half strategy} with our
\textit{geometric strategy} (with default parameter $q=0.75$). 
Thus all control parameters stay same except for the 
search strategy for generating algorithm.

It scored 19/50 (38\%) success rate. 
Minimal $I(M,i,z)$ was in generation 10 
with 115,500 individuals to be processed.
The experiment took 107 minutes.\\

\includegraphics[scale=0.45]{reports/Ant/p.png}

\includegraphics[scale=0.45]{reports/Ant/i.png}

\newpage

%\includegraphics[scale=0.65]{reports/Ant/1/probabs.png}
%\includegraphics[scale=0.65]{reports/Ant/1/indivs.png}\\

This is big improvement in all three watched factors:

\begin{enumerate}
 \item 38\% vs 14\% success rate.
 \item 115,500 vs 449,500 individuals needed to be processed.
 \item 107 vs 265 minutes.  
\end{enumerate}

Third factor, the time difference might give us clue about what is going on.

Those two experiments differs only in used term generating method.
During one run the vast majority of the time is spend by evolution,
the generation process only slightly prolongs the time spend in generation 0.
The run time of the evolution is influenced by two important factors:
Number of generations and time that crossover takes.

The number of generations of more successful method is surly smaller,
but it seems improbable that it would have such a big impact 
(this is idea is supported by results of experiments with
even-parity problem, where times are almost the same but success 
rates differs significantly). 

Time of crossover is proportionate to number of nodes that 
an individual has. Thus it seems that observed slowdown is
due to terms being too big. \\

In order to test this hypothesis, we run additional experiments
to get statistics about dynamics of individuals' sizes. 

Following graphs are base upon 10 run experiments.\\ 

\includegraphics[scale=0.45]{reports/Ant/ts.png}

%Ramped half-and-half strategy:

%\includegraphics[scale=0.65]{reports/Ant/term-sizes-ANT-10behu-ramped.png}

%Our geometric strategy:

%\includegraphics[scale=0.65]{reports/Ant/term-sizes-ANT-10behu-geom.png}

Those graphs show enormous differences. 
The graph of Ramped half-and-half strategy evidences
occurence of well known problem of GP called \textit{bloat}.
Bloat is defined as \textit{program growth without (significant) return in
terms of fitness} \cite{fg}.

As is further stated in \cite{fg}, bloat has significant practical effects.

~\begin{enum}
 & Large programs are computationally expensive to evolve and run.
 & Large programs can be hard to interpret.
 & Large programs may exhibit poor generalisation. 
\end{enum}~
 
For these reasons bloat has been a subject of intense study in GP. 

On the contrary, our strategy seems to
be successfully fighting with bloat  in this particular problem.

It is also noticeable from the correct solutions that have been found.

Here follow all 7 correct solutions found using ramped half-and-half strategy.\\

\begin{easylist}[enumerate]
& \texttt{p2 (ifa (p2 l r) (ifa (ifa (p2 m m) (p2 m l)) (p2 (ifa l r) (p2 m r)))) (ifa (i
fa (p3 m (ifa m m) l) (p2 r r)) (ifa (ifa (p3 (p2 m l) (ifa (p3 (p2 m (ifa l m))
 (p3 (ifa r (p2 (ifa (p2 m r) (p3 l r m)) (p2 m r))) m l) (p2 m l)) (ifa (p3 r l
 l) (p3 m m r))) (ifa (p3 l l r) (p2 (ifa l r) (p2 m r)))) (p2 l r)) (ifa (p3 m
l l) (p2 r r))))}
& \texttt{p3 (ifa (p3 (p3 m m r) (p3 (ifa (p3 (p3 m m r) (p3 r l l) (ifa m l)) (ifa (p3 l
 r (ifa (ifa m m) (p3 r l (p2 (ifa m l) (ifa (p3 (p2 l (p3 (ifa m m) (p2 l m) (p2 l r)))
  (p2 (ifa (ifa m r) (p3 r m l)) (p3 (ifa l l) (p3 l r l) (p2 l l))) (p2
(ifa (ifa m l) (p2 l r)) (p2 (p2 l m) (p3 l r m)))) (ifa (ifa (p3 (p3 r m m) (p3
 (p2 l m) (p2 l m) (ifa m m)) (ifa l m)) (ifa (p3 l r m) (ifa (ifa r r) (p2 m l)
))) l)))))) (ifa l l))) m (p2 (p2 (ifa l l) (ifa m r)) r)) (ifa m l)) (ifa (p3 l
 r (ifa (ifa m m) (p3 r l (p2 (ifa m l) (ifa (p3 (p2 l (p3 (ifa m m) (p2 l m) (p2 l r)))
  (p2 (ifa (ifa m r) (p3 r m l)) (p3 (ifa l l) (p3 l r l) (p2 l l))) (p2
(ifa r (p2 l r)) (p2 (p2 l m) (p3 l r m)))) (ifa (ifa (p3 (p3 r m m) (p3 (p2 l m
) (p2 l m) (p2 l r)) (ifa l m)) (ifa (p3 l r m) (ifa (ifa r r) (p2 m l)))) l))))
)) (ifa l l))) m (p2 (p2 (ifa l l) (ifa m r)) r)}
& \texttt{p2 (ifa (p3 m m m) (p2 r m)) (p3 (ifa l r) (ifa (ifa l (p3 (p3 r r l) (ifa l m)
 (ifa m (ifa (p2 (p3 (p3 (p3 (ifa m l) (p2 r r) (ifa m m)) (p3 (p3 r r r) (ifa r
 r) (p2 m l)) (p3 (p2 m m) (p3 r m r) (ifa m r))) (p2 (p2 (p3 r l m) (p2 m m)) (
p3 (ifa r l) (p2 m l) (p2 r m))) (p3 (ifa (ifa r l) (p3 m m r)) (p3 (ifa r m) (p3 l l r)
 (p2 r r)) l)) (p2 (p2 (p2 (p2 r r) (p2 r l)) (p3 (ifa r r) (p2 l l) (p2
 m l))) (p3 m r l))) (p2 (ifa (ifa (p3 (ifa m r) (p2 (p3 (p3 r r l) (ifa l m) (p3
  m l r)) (p2 m r)) (p3 l l l)) (ifa (p3 l l m) (ifa l r))) (ifa (ifa (ifa m r)
(ifa l l)) (p2 (ifa l m) (p2 l m)))) (p3 (ifa (p2 (p2 r m) (p3 m l m)) (ifa (p2
r l) (p3 r m r))) (p2 (p2 (p2 r r) (ifa l r)) (p3 (p3 m m m) (ifa m r) (ifa l m)
)) (p2 m l))))))) r) (ifa l r))}
& \texttt{p2 (p3 (p2 m l) (ifa r l) (ifa m l)) (ifa (p2 m m) (ifa r l))}
& \texttt{p2 (p2 m r) (ifa m (p2 (p2 r r) (ifa (ifa (ifa m r) (p3 m m l)) r)))}
& \texttt{ifa (p2 (p2 m m) (p2 m l)) (p3 (p2 r r) (ifa m l) (p2 m l))}
& \texttt{p2 (ifa (p2 (ifa r m) (p2 (p2 (p2 l m) (p3 m m r)) (ifa l l))) (p3 (p2 l l) 
(ifa m l) m)) l}
\end{easylist}	


\newpage	
Compare it with 18 solutions obtained by using geometric strategy 
(I accidentally lost one solution).\\

\begin{easylist}[enumerate]
& \texttt{ifa m (p3 r (ifa (ifa m (ifa m (p3 r (ifa m r) m))) (ifa m (p2 (p2 m (ifa m (ifa
m (ifa m (p3 r (ifa m r) m))))) (ifa m r)))) m)}
& \texttt{p3 (ifa m l) (p3 m l (ifa (p2 m (ifa l (p2 (p2 (p2 m (ifa m m)) (ifa (ifa m l)
r)) (ifa m l)))) r)) r}
& \texttt{ifa l (ifa (p3 (p2 (p2 r (ifa r m)) (p3 l (ifa m (ifa m l)) r)) l m) (p2 r (ifa
 m (p2 l (p2 (p2 l (ifa m r)) m)))))}
& \texttt{p3 m (p3 r (ifa m l) l) (ifa (p2 (p3 m m r) (ifa m l)) r)}
& \texttt{p3 (ifa m (ifa (ifa m m) (ifa m r))) m (p3 r (ifa (p3 (ifa m r) m (p3 r (ifa m
l) m)) l) l)}
& \texttt{p3 (ifa m l) (p2 m l) (ifa (p3 (ifa m (ifa m l)) m r) (p2 (ifa m (ifa r l)) l))}
& \texttt{p3 (p3 r (ifa (p2 (ifa m (ifa (ifa m r) m)) m) l) m) l (ifa m r)}
& \texttt{ifa (p3 m (ifa (ifa r m) r) l) (p3 (p3 l (ifa m r) r) (ifa m (ifa m l)) m)}
& \texttt{p3 m (p3 l (ifa (p3 m (ifa m m) m) r) r) (ifa m (ifa m l))}
& \texttt{p2 (ifa (ifa m (p3 l (p2 (p2 (ifa (ifa m l) l) m) (ifa m r)) (p2 (ifa (ifa (ifa
 (ifa (p3 r l m) r) l) l) (p3 l (ifa m r) (ifa m r))) m))) r) (p2 (p3 r (ifa (if
a (p2 (ifa m (ifa (ifa (ifa (p3 m r (ifa l l)) l) l) l)) m) (p3 l (p2 (ifa m (p2
 l m)) (ifa m r)) m)) l) m) l)}
& \texttt{ifa (ifa m (ifa m r)) (ifa (ifa (ifa m l) r) (p3 (p3 l (ifa m r) (p3 r l r)) (
ifa m l) m))}
& \texttt{ifa (p2 m r) (ifa (p2 (ifa (p2 l m) r) m) (p3 l m (ifa r (p3 l (ifa m r) r))))}
& \texttt{p3 (p3 l (ifa m r) r) (ifa (p3 m m (p2 r (ifa (p3 r (ifa m l) (ifa m l)) l))) l
) m}
& \texttt{p3 r (ifa m (p3 l l (ifa (ifa m m) r))) m,89.0,Just 4)
(ifa (p3 (ifa m l) r (ifa m l)) (ifa m (p3 (p3 l (ifa m r) m) r (ifa m l)))}
& \texttt{ifa m (p3 (ifa l (p3 r (ifa (ifa l l) (ifa (ifa r r) r)) (ifa r r))) (ifa (ifa
m r) r) m)}
& \texttt{p3 (ifa (ifa (ifa m l) l) l) (p3 m (ifa (ifa (ifa (p2 m l) l) (ifa r r)) l) (
ifa m (ifa m r))) r}
& \texttt{ifa (ifa (ifa m r) r) (p3 r (ifa (ifa m (p3 (ifa m (ifa m r)) (ifa (ifa r l) r)
 (ifa m l))) (ifa l (p3 (ifa m l) (ifa m l) (ifa m (ifa m r))))) m)}
& \texttt{p3 (p3 m r (ifa (p2 m m) l)) l (ifa m r)}
\end{easylist}~

\newpage

\Lets also examine average term sizes of the Simple Simbolic Regression problem,
again based on 10 runs.\\

\includegraphics[scale=0.5]{reports/SSR/ts.png}


In this case the difference is not that radical, but still we can see
that runs of SSR using ramped half-and-half tend to have terms approximately twice
as big as runs using geometric strategy.

\newpage	
\section{Even-parity problem}
The even-parity function is a boolean function taking as inputs $N$
boolean values and returning \textit{True} if an even number of inputs 
are \textit{True}. For odd number it returns \textit{False}.

The reason why examine this problem is that it has been used by many researchers
as a benchmark for GP \cite{yu01}.

We compare our results with that obtained by Yu in \cite{yu01},
where is presented approach to evolve recursive and modular programs
by use of higher-order functions and $\lambda$-abstractions.
Since the results are highly sensitive to set of \textit{building blocks}, 
we use very similar set of \textit{building blocks} in order to make 
the results comparable. 

\begin{align*}
\sigma = [Boo&l] \ar Bool\\
\Gamma = \{
  and   &: Bool \ar Bool \ar Bool                              ,\\
  or    &: Bool \ar Bool \ar Bool                              ,\\
  nand  &: Bool \ar Bool \ar Bool                              ,\\
  nor   &: Bool \ar Bool \ar Bool                              ,\\
  foldr &: (Bool \ar Bool \ar Bool) \ar Bool \ar [Bool] \ar Bool ,\\
  head' &: [Bool] \ar Bool                                   ,\\
  tail' &: [Bool] \ar [Bool]                              \}\\
\end{align*}

The type $[Bool]$ stands for \textit{list of $Bool$s} and for purpose of
this problem is considered atomic.

Unlike in \cite{yu01}, we use specific instance of polymorphic 
function \texttt{foldr}. 
And modifications of functions \texttt{head} 
(returning the first element of the list) 
and \texttt{tail} (returning the list without the first element) are used; 
making them total by returning default value \textit{False}
and \texttt{[]}, respectively.

The \texttt{fold} function is higher order function widely used in \textit{Haskell}
programming language, recursively defined as follows.

\texttt{~\\
foldr :: (a -> b -> b) -> b -> [a] -> b \\
foldr f z []     = z \\
foldr f z (x:xs) = f x (foldr f z xs)  
}\\

\Lets demonstrate \texttt{foldr}'s behavior on the following simple example.

\texttt{~\\
foldr xor False [True,False,True] \\
= xor True (foldr False [False,True]) \\
= xor True ( xor False ( foldr False [True] ) )\\
= xor True ( xor False ( xor True (foldr False [] ) )\\
= xor True ( xor False ( xor True False) )\\
= False
}\\

The reasons why is this difficult problem 
for GP to solve are following \cite{yu01}:

\begin{itemize}
 \item Since a single change of one input generates a different output,
       the output is highly sensitive to all inputs.
 \item The building blocks set does not contain functions \textit{xor}
       or \textit{eq}, which are keys to solving the problem.
       But those two functions may be discover by GP.  
\end{itemize}

We use the same fitness function as in \cite{yu01}. 
The fitness function examines the individual by giving
it all possible boolean lists of length 2 and 3.
Thus resulting in $2^2 + 2^3 = 12$ test cases.
For each correct response it receives $1/12$, 
therefore correct solution receives fitness value $1$.\\

It is interesting that such a few fitness cases are
sufficient to find general solution (it was so for every
solution I tried). It comes from the $\Gamma$, the solution tends 
always be \texttt{foldr} with \textit{xor} defined by the lambda
abstraction. 

%\begin{todo}
% & zmínit tu, že to pri týyle building block stačí na general solution...
%\end{todo}

\subsection{Experiments}

Series of five experiments have been performed to test properties of our system.
Each experiment consisted of 50 independent runs of GP algorithm.
Each run had maximally 50 generations
and 500 individuals as population size.\\
 
In those experiments we examined influences of the following factors:

\begin{itemize}
 \item Option of preserving the best individual into the next population.
 \item Option of performing $\eta$-normalization of generated individuals 
      (conversion from \lnf to \benf, see \ref{benefits}).
 \item Representation of individuals: \sexprTree vs \atTree.
 \item Option of performing optimized abstraction elimination
       (see \ref{toSKI}) instead of the simple one.
\end{itemize} 
 



The setup of the first experiment was following:\\ 

\begin{enum}
 & \textit{Geometric} search strategy for generating individuals with default $q=0.75$.
   This option is unchanged for all five experiments.
 & No best individual preservation.
 & $\eta$-normalization was not performed.
 & Individuals represented as \sexprTree{}s. 
 & Simple abstraction elimination.
\end{enum}~

Thus the simplest one setup; all extra features are off. 
In each following experiment one extra feature will be turned on,
in hope that it will enhance the performance.

The correct individual was yield 
in 17 out of 50 runs, i.e., 34\% success rate.

%The performance curve for $P(M,i)$ is following.
%\includegraphics[scale=0.65]{reports/ep/report1/probabs.png}

Interestingly, in 2 runs out of 50, a correct solution was present in the initial population.

%The performance curve for $I(M,i,z)$ is following.
%\includegraphics[scale=0.65]{reports/ep/report1/indivs.png}

The smallest value in this graph is used to indicate the minimum effort
(i.e. number of individuals to be processed) for GP to yield a correct solution.

Thus for this experiment this analysis suggest
to perform only term generating and no evolution at all, 
because the smallest value 56500 is in generation 0.

This experiment took 59 minutes (on average desktop computer).\\

Second experiment was different from the first one in that it 
preserved the best individual to the next generation.

It scored 22/50, i.e., 40\% success rate. A correct solution was
created once in the initial generation and again it was
sufficient to make the value in generation 0
be the smallest value in the $I(M,i,z)$ curve; now 114000
 individuals to be processed.
This experiment also took 59 minutes.\\


Third experiment was different from the second one in that 
$\eta$-normalization of generated individuals was performed.

It scored 27/50, i.e., 54\% success rate. 
Two runs contained a correct solution in the initial population.
But now the $I(M,i,z)$ was best for generation 1 with 56000
 individuals to be processed.
This experiment took 53 minutes.\\


Fourth experiment was different from the third one in that
\atTree representation was used instead of \sexprTree.

It scored 34/50, i.e., 68\% success rate. Again two runs
contained a correct solution in the initial population.
The $I(M,i,z)$ was best for generation 0 with 56500 
individuals to be processed.
This experiment took 64 minutes.

As we will see, this setup was the most successful one.
%,thus we again show the performance curves.

%\includegraphics[scale=0.65]{reports/ep/report4/probabs.png}
%\includegraphics[scale=0.65]{reports/ep/report4/indivs.png}

Fifth, last experiment was different from the fourth one in that
optimized abstraction elimination was used instead of the simple one.

It scored 33/50, i.e., 66\% success rate. One run
contained a correct solution in the initial population.
The $I(M,i,z)$ was best for generation 6 with 59500 
individuals to be processed.
This experiment took only 30 minutes. \\



\includegraphics[scale=0.5]{reports/ep/p.png}

\includegraphics[scale=0.5]{reports/ep/i.png}



\subsection{Comparison with other results}

Our results are less successful then those 
presented by Yu \cite{yu01};
they scored 40/50, i.e., 80\% success rate.  
Her $I(M,i,z)$ was best for generation 4 with 17500 
individuals to be processed.
(We scored 34/50 (68\%); min $I(M,i,z) = 56,500$.)

Yu also uses crossover as the only genetic operator, thus we
can narrow the discussion to discussion of generating method and 
crossover method. 


Design of our crossover comes from that we wanted it to be
very simple generalization of standard tree-swapping crossover.
Whereas the crossover used by Yu is more structure oriented
--- as is stated in \cite{yu01} the crossover \textit{can only operate 
between nodes in two main programs or between nodes in the same
kind of $\lambda$ abstraction (i.e., $\lambda$ abstractions that 
represent the same function arguments to the same higher order function).} And also variable terminals of Yu's lambda functions
are constrained to be only the variables of that lambda function, not from some outer scope. This may be advantage for this particular problem because xor does not need any outer variable
(recall that most common solution is combination of xor and foldr in some similar way as foldr exaple above).
From an optimist's point of view 34/50 versus 40/50 
is not such a huge difference when we take to account that
our general tree-swapping crossover is competing with specialized one.   
%This seems to be great advantage for optimizing the 

Since our method has better success rate for generation 0, it would 
be interesting to try combination of our generating 
method with crossover used by Yu.


Another optimistic observation is that our result at least outperforms 
other results mentioned in \cite{yu01} with which Yu's method is compared.


\textit{Generic genetic programming} scored 17/60 (28\%); 
min $I(M,i,z) = 220,000$.

\textit{GP with ADFs} scored 10/29 (34\%); 
min $I(M,i,z) = 1,440,000$.

%
%
%
%\begin{todo}
%& napsat tam jak vypadaj nějaký nalezeneý řešení
%\end{todo} 

\newpage
\section{Flies and Apples}

\textit{Flies and Apples} is problem consisting in breeding 
control programs for a fly agent in a simple world.\\ 

Whereas previous problem is a kind of problem where we know precisely
what problem we are solving and where we are more interested in 
performance statistics than in the solutions since we know the correct solution in 
advance, problem we are going to present now is of more playful nature.
In this problem we are not interested in numerical results, instead 
we are hoping for interesting \textit{behavior} to emerge.

We start with defining a simple simulation and goal for agents.
Then we prepare first testing simulation world. 
After that we write some agent programs by hand. After that we 
identify some useful recurring 
themes occurring in those programs and pack them into functions
which will become building blocks. During this process are also identified
useful sensory data to be prepared for the agent program.
After that we run the evolution and
iteratively continue in this process  --- we may add more testing worlds,
add more functions or change them, etc.
      
Since we do not need to satisfy the \textit{closure} constraint, 
it is much easier to perform such a "human-computer jam" than it
would be in standard GP. And it is further simplified by use of
functional language (such as \textit{Haskell}) where almost
every construct can be expressed as function (or value).

Another advantage of typed GP over standard GP is that 
it is more capable of handling huge set of building blocks,
since the search space can be significantly smaller due to 
fact that types must match.
\\



Fly world is a square grid. 
Each cell is ether empty or contains precisely one object.
There are three kinds of objects:

~\begin{enum}
 & Wall
 & Apple
 & Fly
\end{enum}~

Apples and flies have \textit{energy}.

Moreover, each fly has its control program and inner state, 
so called \textit{registers}.   

There is a queue of all flies in the fly world determining which 
fly is next to make a move.
After the fly in the front of the queue performs its move, 
it is put to the back. In other words, flies are cyclically
taking turns similarly like in common card games.\\

A control program of a fly has as input a collection of various sensory information
and returns as output agents move and agents registers for the next turn.

Fly can perform two kinds of moves:

~\begin{enum}
 & \textit{Trave.} Moves the fly to one of four adjacent cells determined by 
   direction specified in the move.  
   Result of the travel move depends on the content of the target cell.    
   && If it is empty, then the fly simply moves to that cell.
   && If it contains wall, then no movement is performed.
   && If it contains an apple, then the fly moves to that cell, the apple 
      is eaten and its energy is added to energy of the fly.  
   && If it contains another fly, then the fly moves to that cell, the fly with bigger 
      energy stays alive and the weaker one is eaten
      and its energy is added to energy of the stronger fly.  
 & \textit{Split.} Puts a daughter fly to one of four adjacent cells.
    Together with the direction, amount of energy and new registers 
    for the fly are specified by this move. The mother gives this energy to 
    its child. In order to be successful, the target cell must be empty
    and the mother fly must have energy greater then 1. 
\end{enum}~

Agent's fitness is computed as sum of fitness cases; each 
fitness case consists of one fly world and fitness in a
fly world is computed as sum of agent's energy and of energy of all his descendants
(daughters, granddaughter, ...) after predefined number of rounds. 

\Lets look in more detail on input for fly. It is collection of 
various useful information. Its components are:

~\begin{enum}
 & Current value of fly's Registers. 
 & Energy of the fly.
 & Direction of the last successful travel move.
 & Boolean value indicating whether the last move was successful.
 & Direction of the nearest apple.
 & Distance to the nearest apple.
 & Energy of the nearest apple.
 & Direction of the nearest fly.
 & Distance to the nearest fly.
 & Energy of the nearest fly.
 & Direction of the "center of gravity" of all apples.
 & Distance to the "center of gravity" of all apples.
\end{enum} ~  

Set of building blocks is following.

\begin{align*}
\sigma = Input &\ar Output\\
\Gamma = \{
dUp              &: Direction                                       ,\\              
dDown            &: Direction                                       ,\\              
dLeft            &: Direction                                       ,\\              
dRight           &: Direction                                       ,\\            
output           &: Move \ar Registers \ar Output                   ,\\            
\end{align*}
\begin{align*}
travel           &: Direction \ar Move                              ,\\          
split            &: Direction \ar Int \ar Registers \ar Move        ,\\       
easySplit        &: Input \ar Move                                  ,\\     
myEnergy         &: Input \ar Int                                   ,\\     
myLastTravel     &: Input \ar Direction                             ,\\                
myWasSuccess     &: Input \ar Bool                                  ,\\      
nAppleDir        &: Input \ar Direction                             ,\\                    
nAppleDist       &: Input \ar Distance                              ,\\                   
nAppleEnergy     &: Input \ar Int                                   ,\\              
nFlyDir          &: Input \ar Direction                             ,\\                      
nFlyDist         &: Input \ar Distance                              ,\\                    
nFlyEnergy       &: Input \ar Int                                   ,\\              
cAppleDir        &: Input \ar Direction                             ,\\          
cAppleDist       &: Input \ar Distance                              ,\\        
myRegs           &: Input \ar Registers                             ,\\         
xGet             &: Input \ar Int                                   ,\\     
yGet             &: Input \ar Int                                   ,\\     
zGet             &: Input \ar Int                                   ,\\     
dGet             &: Input \ar Direction                             ,\\          
xSet             &: Int \ar Registers \ar Registers                 ,\\          
ySet             &: Int \ar Registers \ar Registers                 ,\\          
zSet             &: Int \ar Registers \ar Registers                 ,\\          
dSet             &: Direction \ar Registers \ar Registers           ,\\                
xInc             &: Registers \ar Registers                         ,\\              
yInc             &: Registers \ar Registers                         ,\\              
zInc             &: Registers \ar Registers                         ,\\              
rotCW            &: Direction \ar Direction                         ,\\                                                             
(==)             &: Int \ar Int \ar Bool                            ,\\                                      
(<=)             &: Int \ar Int \ar Bool                            ,\\               
if'              &: Bool \ar Output \ar Output \ar Output           ,\\                                  
if'              &: Bool \ar Move \ar Move \ar Move                 ,\\                                  
if'              &: Bool \ar Direction \ar Direction \ar Direction  ,\\              
if'              &: Bool \ar Int \ar Int \ar Int                    ,\\     
if'              &: Bool \ar Distance \ar Distance \ar Distance     ,\\        
if'              &: Bool \ar Registers \ar Registers \ar Registers  ,\\                           
0                &: Int                                             ,\\                                     
1                &: Int                                             ,\\                                    
2                &: Int                                             \}\\
\end{align*}

\newpage
We will not discuss it in great detail, only briefly: First four 
elements are four direction values. Next four functions are
constructors for $Output$ and $Move$ type. Next twelve functions
are for accessing components of $Input$. Next eleven functions are 
related to $Registers$. The $rotCW$ performs clock-wise rotation of a direction.
Next two functions are comparison functions. Next six are various \textit{if}s. And last 
three are integer constants. 

This gives 44 elements of building block context. It is quiet huge 
context, especially if it was a standard GP \TuF.\\

So the set of atomic types is following =\\
$\{Move, Bool, Registers, Input,
Direction, Int, Output, Distance\}$.\\


We currently use three worlds.
First one looks like this:\\

~\\[2em]

\includegraphics[scale=0.8]{fly/1.png}


Other two worlds are reaction on typical behavior 
of a trivial and boring fly which was successful
enough to prevent further evolution.
This boring strategy was just following the path 
of the nearest apple. 

\newpage

Following two worlds are therefore "Devil's gift" for this fly.\\

\includegraphics[scale=0.4]{fly/2.png}
\includegraphics[scale=0.4]{fly/3.png}\\

In the second world the boring fly enters the corridor
and misses the mega apple.
The third world escalates previous idea into absurdity.
The boring fly eats nothing since it dumbly tries to eat the
apple behind the wall.

By introduction of those two new worlds new kinds of behavior emerged,
but not interesting enough to be worth explaining here. One can,
however, check those behaviors out in the gallery of interesting fly 
behaviors, included on the enclosed CD.\\

In the initial experiments there was problem with that flies did not
want to split. This was due to that they were given default energy 1.
Since this problem is not tied by some unchangeable definition,
we changed this default value to 100 and aded a siplified version of split constructor. That had aftermath in new kinds of behavior.

~\\
\includegraphics[scale=0.33]{fly/4.png}

\newpage

Application of this kind of approach is for example 
computer games design where non-linearity and adaptability
to players actions are highly appreciated. 
Also games are not tight by some fixed specification
so this approach might be very appropriate in this industry.  

~\\[1em]

\includegraphics[scale=0.35]{fly/5.png}





		
\chapter{Previous related work}

\section{Evolution of Constrained Syntactic Structures }
\textit{Evolution of Constrained Syntactic Structures}
is mechanism proposed by Koza in \cite{koza92} for relaxing 
the \textit{closure} requirement.

Problem specific \textit{rules of construction}
are used to constrain which function node
can have which child node.

Those rules are used in generating method similar to the standard one.

For purposes of crossover types of symbols are listed - 
e.g. in the problem of \textit{symbolic multiple regression} 
(and various other problems)
those types are 

~\begin{easylist}[itemize]
& the root, and
& non root points.
\end{easylist}~

Crossover is performed by first selecting any node in the first
parent and after that by selecting node of the same type as the first node.\\

System based on types do this task more indirectly and in a
simple systematic manner.

\section{Strongly Typed Genetic Programming}
One of first such type based systems was that described by Montana
in \cite{montana95}. This system is called 
Strongly Typed Genetic Programming (STGP).
Beside simple types he also uses kind of polymorphism 
called \textit{generic functions} based on table lookup.

Used generating method is generalization of the standard one
dealing with type restrictions and . Moreover, checking preventing
selection of node which makes it impossible to complete the subtree 
is performed.

The crossover works in similar manner as that Koza used
in his Constrained Syntactic Structures. Such a crossover
is less cautious then that used in our system
since it may fail to find a node with desired type in the second parent.
If the crossover fails, then the crossover returns either parents or nothing.

We were briefly measuring fail rates for such a crossover and
it was somewhere around 15\% for even-parity problem. Thus we prefer
to perform the more cautious version described in \ref{typed-swapping}.  \\

This system does not involve use of lambda abstractions or
higher-order functions.

\section{Higher-Order Functions and Lambda Abstractions} 
In \cite{yu01} Yu presents a GP system utilizing
polymorphic higher-order functions 
\footnote{Higher-order function is a function taking another function as 
input parameter.} and lambda abstractions.

Important point of interest in this work is use of
\texttt{foldr} function as a tool for \textit{implicit recursion},
i.e. recursion without explicit recursive calls. 

Approach described in this work is more specialized then our approach
in the following ways.

\begin{itemize}
\item The terminal set for constructing lambda abstraction subtrees 
      is limited to use only constants and variables of that particular
      lambda abstraction, i.e., outer variables are not allowed to be used
      as terminals in this work.
\item  Whereas we see lambda head of lambda function as 
another kind of node (as parent node of the body of the function), 
in this work whole lambda function is considered more as single
point with internal structure. Higher-order functions
and its lambda abstractions are said to form \textit{two-layer-hierarchy}
called in this work \textit{structure abstraction}.
\item The crossover used in this work \textit{can only operate 
between nodes in the two main programs or 
between nodes in two lambda abstractions that represent 
the same function arguments to the same higher order function. }
\end{itemize}

The difference between approach presented in this work and our approach 
may be stated in the following way. Approach presented in \cite{yu01}
tries to adopt some useful constructs from domain
of functional programming and use them for a benefit in problem solving by GP.
Whereas our approach is to perform genetic programming directly in the
simply typed lambda calculus.
  

% v even parity scoruje o něco lépe než my.


\section{Functional Genetic Programming with Combinators}
In \cite{kes} by Briggs and O’Neill is presented technique 
utilizing GP with combinators in similar way as we do.

The difference between approach presented in this work
and our approach is that in this work terms are generated
straight from \textit{library} (their term used for context) 
of combinators, whereas we first generate $\lambda$-terms which are
subsequently translated into combinator terms.

Thus we can in our system see the combinator representation
as option which can be turned off, whereas their system has this
feature "hard-wired". On the other hand, their advantage is
that direct generation of terms generates 
shorter combinator terms than indirect generation.   

They are using more general type system then us
--- the Hindley–Milner type system. So the system
is polymorphic.

They also discuss the properties of exhaustive enumeration
of terms and compare it with GP search.  
Since our successful term generating \textit{geometric} strategy
is based on undisciplined exhaustive enumeration, this
is another interesting thing that connects our two systems.   

Another really interesting concept is their \textit{Generalized
genetic operator} based on term generation. Suppose we have
some terms we want to operate on 
(for one term it corresponds to mutation, for two (parent) 
terms it is crossover, etc.). In order to do that we construct 
\textit{library} of all subterms of all terms we want to operate
on and from this library we generate new individuals by supplying
it with desired type.

It would be nice to use some ideas inspired from this work in
the future improvements of our system. 


%\section{Genetic Programming with Polymorphic Types and Higher-Order Functions}
%by Franck Binard and Amy Felty
%
%\begin{todo}
% & dopsat nebo vyhodit
%\end{todo}


%\chapter{Ideas yet not implemented}
\chapter{Future work}

\section{Polymorphic variant}

At this point I have almost complete implementation
of polymorphic generalization of the system.

This involves extending the type system so that we
also have type variables.

Lets very briefly sketch the key phases.

Then instead of simple equality checking of the
current type with return type of the function, we 
try to fined \textit{most general unification}
of those two type terms. If it succeeds, than 
we may use this function symbol (or constant).
Important part is to deal in the right way wit obtained 
substitution. 

%\begin{todo}
% & poznamky o čem se mimojiné zmínit:
%   && $(\$) : (\alpha \ar \beta) \ar \alpha \ar \beta$ 
%      jako alternativa k ADF ! Myslim že hodně zajimavý
%\end{todo}

\section{Mutation}

Since we have term generating method, it is easy to 
define subtree changing mutation. 

Also node changing mutations are easy.

It would be also nice to perform some kind of informed
(or clever) mutation  - that is mutation that tries more possibilities 
and selects the best one (i.e. performs a local search).

In the lambda calculus context, it would be interesting to try 
some reduction based mutations --- simplifying the term
by bringing it to normal form form which it was dragged 
away by crossovers and other mutations.

Also mutations of number-like constants would be useful. 

%\begin{todo}
%& říct že na tu sme se moc nesoustředili ze dvou duvodu
%  ktery to ospravedlňujou 
%	&& koza taky
%	&& ta s vygereovanim podstromu je v zásadě použití
%	   algoritmu na generovaní jedince a tak nám přišlo
%	   lepší pořádně se soustředit na generování jedince a
%	   začít do toho mutaci tahat až bude generování pořádně
%	   prozkoumáno.
%& To ale neznamená že jí nemáme radi, naopak se těšíme
%  že její vhodný zavedení celou metodu ještě zlepší.
%  
%& Nápad na to jak to dělat:
%  && nutno ošetřit - co všechno do gammy alemyslim že v phodě když je uplne 
%     stejná jako ta generujicí, pač ve vy SKI-ovanym termu stejne nejsou zadny 
%     proměný 
%  && mít nějakej předem danej počet pohybu na frontě, abych mohl efektivně
%     kontrolovat že to přidání mutací moc nespomalý
%  && Po tom počtu pohybu končim generování
%  && mam nějakej počet vygenerovanejch termu
%  && pro chytrou mutaci mužu chtít třeba 10 termu
%     &&& pokud se mi jich vygenerovalo míň, tak beru míň
%     &&& pokud se mi jich vygenerovalo víc, tak vemu 10 náhodně
%     &&& těch 10 vyhodnotim (jakože dosazenejch na jejich místo)
%     &&& nejlepší si vezmu
%     &&& dobrý udělat to zase najednou pro všechny v tý generaci mutovaný
%         pánčto je nákladný spouštět Hint
%& Další mutace - beta eta redukce (?? asi moc naroční viz dale) 
%  && zmenšující mutace
%  && udělaná že převedem SKIBC do lambda termu a to redukujem? to asi ne pač
%     pak převod to spíš prodlouží, spíš asi mít redukční pravidla pro SKIBC
%     a těma redukovat, ale je otázka jak moc tam budou vznikat SKIBC-redexi
%     - ty mohou vznikat jen při @-tree reprezentaci
%  && to otevírá otázku toho, že by elementy gammy mohla mít k sobě redukční pravidla
%& Další mutace - jednoduše náhrada podstromu/uzlu za nějakej z gammy
%  && nezvětšující mutace
%& Další mutace - mutace číselnejch-like konstant
%& To mě přivádí k myšlence sice pro generování ale myslim podstatný:
%  jakmile jsou v problému číselný-like konstanty tak mužu udělat pro 
%  jeden tvar hodně jedincu - což šetří jak čas na generování, tak to prozkomává 
%  víc danej tvar

%\end{todo}





\section{Big Context}

\textit{Big Context} is an unfinished problem 
involving breeding of clasical functional functions
as \texttt{head,tail,map,elem,filter}. We first take
small sufficient $\Gamma$ for each function to be evolved.
After that we  fuse those contexts into one big context and check whether the functions are still evolvable. Or we may try to 
evolve several functions at once by evolving tuples of them.

We have postponed this problem since it is much more suitable
for polymorphic system.

\section{Parallelisation}
Since Haskell is very good at Parallelisation 
and GP is easily parallelised, it comes naturally to 
connect the two.



%Tento problém je napůl rozpracovaný, hlavně protože by se na něj o dost více
%hodilo použít polymorfní typy, které jsou také ne uplně dodělané a véto thesis
%se o nich zminujeme jen velmi okrajově. Přestose myslím jedná o zajímavou myšlanku
%a proto ho popíšeme.


\section{Future ideas with Inhabitation trees}

I have lot of ideas involving inhabitation trees and 
informed term generation. Number labeled 
inhabitation tree may be used as more informed
version of geometric strategy. 
Variation of ant colony optimization
could be used as vehicle for this informed
term generation. This ACO would differ
from classical one in that
it would use trees instead of paths.
Ant may select one edge (or-node)
or clone himself (and-node). The global criterion 
determining the pheromone trail intensity 
are based upon fitness and local heuristic
would be some term shortening factor.  \footnote{
Unfortunately I have run out of time for writing this
thesis so I must let this favorite theme of mine mostly unexplained.}


%\section{Roadmap}
%This is idea based on
%\section{Tree ants}


%\section{[Propojení těch dvou barandrechtskejch odvozovacích pravidel do jedinýho]}
%\section{[Chytřejší heuristika pro A* která si to předpočítá na $\overline{\Gamma}$ ]}
%\section{[šlechtění těch strategií na generování termů]}
%\section{[šlechtění fitness funkcí a nápad s "turnajem olympioniků"]}
%\section{[šlechtění search strategií pro prohledávání]}
%\section{[šlechtění alternative family trees]}
%
%\section{vypisování jedinců tak že nahradíme kombinátory \lterm{}ama a redukujem }



%\chapter{Conclusion}
\chapter*{Conclusion}

The goal of this thesis was to design and implement a system
performing GP over some typed functional programming language.
As this typed functional programming language we have chosen the 
\textit{simply typed lambda calculus}.
We have also obligated ourselves to do it in a such way that 
generalizes the standard GP, rather than
crates completely new system.

We may say that we have chosen the simplest possible
option in both areas --- the simply typed calculus and the standard GP.
We see this decision as fortunate one --- since it enables us add more
complex features after sufficient exploration of those simplest cases.
   
We have started the journey toward this goal by study 
of subject that intersects both those areas:
Method which for desired type and context generates lambda terms.  

The way to obtain this method started with inference rules 
(the defining concepts of the typed lambda calculus),
it proceeded through term generating grammars and finished 
with inhabitation trees.

A* algorithm in combination with inhabitation trees
has been utilized to drive systematic enumeration
of typed \lterms in their \lnf. This enumeration was further 
parameterized by simple search strategy in order to enable
such different approaches as \textit{systematic} generation 
and \textit{ramped half-and-half} generation to be definable
in the means of simple search strategy.\\

With this apparatus in hands we introduced novel approach
to term generation by defining the \textit{geometric} search 
strategy. This strategy is further parameterized by parameter
$q$,  but since we wanted to avoid suspicion that success
of this generating method depends on fine-tuning of this
parameter, we used its default value $q = 0.75$ in every experiment.

After being generated all terms undergo process of \textit{abstraction
elimination}, which enables our simple tree swapping crossover operation
which is only genetic operator that we have used in all experiments.

We examined it in three different experiments, which all 
supported the idea that it has very desirable qualities.

First two experiments, \textit{Simple Symbolic Regression} and 
\textit{Artificial Ant}, were performed in order to compare
the \textit{geometric} strategy with the standard GP term 
generating method \textit{ramped half-and-half}. 

In both experiments were observed improvements in the following aspects.

\begin{enumerate}
 \item Success rate was improved.
 \item Minimal number of individuals needed to be processed in order to yield 
       correct solution with probability 99\% was lowered.
 \item Run time was significantly reduced. 
 \item Average size of a term was decreased.
\end{enumerate}

In the case of Artificial Ant problem, all improvements were significant.

These results make me believe that \textit{geometric} strategy might
be welcomed reinforcements in the fight against the bogey of 
the GP community --- the \textit{bloat}.
Or at least, it seems to work for those two experiments in a such way.\\


In the third experiment, the \textit{even-parity} problem, geometric strategy 
showed ability to yield correct solution in the initial generation
8 times out of 200 runs (throughout 5 experiments), which is interesting
result since it is uninformed search. 

Five consecutive experiments with even-parity problem supported 
hypothesis that $\eta$-normalization of generated terms enhances performance.
We have also seen that use \atTree{}s instead of more traditional \sexprTree{}s
as tree representation of individuals is able to enhanced performance.

Use of optimized abstraction elimination instead of its basic variant 
showed significant improvements in time consumption with little or 
non effects on performance rates.

Implemented system was designed with importance of interactivity in mind,
resulting in server/client architecture for core/GUI components.

%\newpage
%\begin{todo}
% & my tu řikáme že to závadíme kuli typum, ale my děláme v víc! -dáváme tam
%   i abstrakce atd - ale mužeme říct, že tim že se to pak převede zase na 
%   to bez abstrakcí tak setrváváme dal v krasnym bez-lambda abstrakčí ale
%   s tou výhodou že naše metoda gerneruje všechny struktury pro ten danej typ ...
%   nebo tak něco - asi to dát spíš do design of our system nebo někam
%   ale někde to asi zmínit
%\end{todo}
%
%
%Další témata co zmínit v úvodu nebo v závěru :\\
%
%
%
%\begin{easylist}[itemize]
%
%
% 
%& zmínit že existuje i jinej pohled na to proč tam zavádět typy:
%  && mužem koukat na to tak že konstrejnt je naše TuF tim, že 
%     v ní nemužou bejt libovolný funkce
%  && ale tuhle constrain mužeme omezit tim že konstrjnujeme 
%     právě použití tech bilding bloku, kde constrains jsou daný těma typama
%     - jakoby výměna global constrain za localní konstrejny na tech bilding  
%       blockách
%  && druhá možná ješte silnější motivace je že pomocí typu pak mohou vznikat 
%     smysluplnější programy, tím že dostavají vstupy které přesně čekají
%     a dávají výstupy které mají nějaké očekávané vlastnosti    
%
%& Možná se zmínit o tý představě dvou pólu
%  induktivním a deduktivním -
%  klasickym GP a Dokazovačem, čim složitější
%  typovej sytem tim víc se jedná o deduktivní   
%  systém a blíží se to dokazovači.
%   && Ve světle toho, je pak hezky vidět, že
%      je chytrý exhaustivně systematicky prohledávat od nejmenšího
%      po největší- jak budeme zesložiťovat typovej systém, tak
%      čim dál víc bude už stačit najít vubec nějaký řešení a bude čim dál těžší
%      jich generovat kvanta
%
%
%& že imlementace je delana těma vtipnejamtajpklasama umožnujicima libovolný
%  evoluční struktury na evolvovani
%
%& říct že vnekterejch aspektech je videt, že to je work in progress
%  ale že si myaslime že i tak to ukazuje pěkný výsledky 
%  a že doufame že ty nedodělky budou další prací odstraněny
%
%
%& Říct jak jde text:
%  && Začnem popsanim klasickýho GP podle Kozy
%  && Vyslovíme definice okolo lambda termu a typu
%  && Vyslovíme definici inhabitačního stromu
%     kterej je stromovou verzí IM od Barendrechta.
%  && Matematicky popíšem jak na to dem my.
%  && Popíšem rozšíření zakladního systému na 
%     polymorfní.
%  && Popíšem to z programátorskýho hlediska
%  && Popíšem příklady na kterejch to demonstrujem
%  && Srovnáme to s nějakejma přístupama co už 
%     existujou 
%
%& Nějak naťuknout už tady v úvodu jak se to má při
%  srovnání s tim jak na to dou jiný (Yu atd)
%  
%& Výkřiky co zmínit:
%  && Na co zaměřený:
%     &&& Proof of a concept
%     &&& Spíš jednoduchost
%         &&&& Ve většině aspektú to sleduje
%              Kozu - tzn i v málo řídících
%              parametru
%     &&& Duraz na generování termu (!)              
%
%  && Na co nezaměřený (ale snad taky ok v tom):
%     &&& Performance   
%     &&& Mature knihovna co by šla použít robustně
%         v cizim kódu - spíš to počítá že to 
%         bude dál rozvíjeno a tohle je mezizastávka
%
% & TALK ABOUT GP is part of EA etc. and maybe define the GP by 
%   defining EA and then specifying the differences or something 
%   like that...   
%
%& bylo by asi mimojiný dobrý 
%ospravedlnit/okomentovat tak fancy (=javascript frontend) 
%implementaci při 
%dost nedodělanejch jinejch věcech (heuristika by 
%mohla být lepší, mutace by mohli bejt, atd)
%ale věřil sem, že je to proces iterativní
%a že je po celou dobu dobrý mít na čem to testovat.
%Javascript byl vybranej pač HTML je nejpromáklejší
%UI jaký vubec je na zemi teď (state of dzí art)
%a tak to snad nebyla stráta času.
%navíc se naskitla díky tomu další pěkná příležitost
%k ukázání toho, že to neni omezený na haskell
%ale že se kombinatorový konstrukty daj 
%v zásadě přeložit do libovolnýho jazyka celkem bez problému
%nebo minimalně do JS. Ale todle možná nepsat tolik do závěru
%spíš nějaky podrobnosti přehodit do implementačních detailu
%a tady to jen tak líznout.
%   
%\end{easylist}
%
%
%\textbf{Jen když zbude čas:}
%
%\begin{todo}
%
%& Complexity discussion 
%&& !!!
%&& zmínit že mi moc neřešíme formální/asymptotický složitosti
%  &&& de nám hlavně o realný časy
%&& vyjít z komplexnosti A* asi
%  &&& složitost výrazně ovlivněná kvalitou heuristický funkce
%
%
% & Brief history of the project\\
%  zmínit třeba tyhle fáze:
%  && že sem si delal evaluator sám co sem pak s performenc duvodu opustil
%  && že sem hledal intuicionistickej dokazovač ale že nejsem nenarazil na vhodnej
%     pač nejsou delany na to plivat hodně dukazu  
%  && že sem nejdriv pouzival ten vypsychlej dokazovač co tam měl nějaký ty 
%     spešl rozhodovací stromy
%  && pak sem to přepsal tak aby to nebylo jen na GP šitý ale obecně na jakoukoliv 
%     evoluci
%  && ...
%
%& pro \ref{benefits}
%&& možná eště zmínit něco jako : \\
%Jedna drobná nevýhoda je, že pokud chceme generovat termy od nejmenšího do největšího
%a pokud lnf generujeme od nejmenšího po největší tak pak generovaní \benf
%timhle trikem nemusí bejt od nejmenšího po nějvětší. V naší aplikaci (tzn GP)
%ale nijak nebazírtujeme na týhle vlastnosti a nakonec budeme spíš stejně používat
%nějaký nedeterminizmy aby to fungovalo líp, takže to vubec nevadí, že budou trochu přeházený. 
%
% & pro \ref{barlike}
% && možná zmínit že podobně jako mužeme transformovat inference rules
%   na grammar rules, mužeme i obráceně grammar rules
%   převést na inference rules - to bylo ostatně uděláno v tom dukazu ale
%   je to podle mě hezký říct ty pravidla explicitně.
% & pro \ref{lnf}
% && potřebuju eště nějaký \textbf{propositions} z \cite{barendregt10} tu uvíst ?
% && ta definice je tam daná pro $\tur{}{M}{\sigma}$,
%   ukazat ze ta naše je ok, že tam má navíc to $\Gamma$.
%   Stačí udělat tríček že beru funkci co má lambda hlavu odpovídající 
%   tomu kontextu.
%   
%   & k \ref{Term-generating-grammar}
%    &&  buď dokázat ty převody na grammar rules explicitně
%     nebo říct že se to dokáže podobnym stylem jako v dukazu 
%     v sekci \ref{barlike}
%    && možná ocitovat toho van Wijngaarden [1981] s těma 2-level grammar
%     jak je to v Barendregtovi \cite{barendregt10} 
% & možná konec (tree reprez of typed terms) subsekce trochu přeformulovat, je to takový   
%   kostrbatý...
% & o odvoz pravidlech říct
%   && možná zmínit že se jedná o \textit{gentzen style proof}
%   && říct že jsou to modus ponens a jeden směr věty o dedukci  
% & nějak přepsat \ref{GP-prog-trees} aby to bylo ve stejnym stylu jako v \ref{tree-reps}
% & napsat transformace mezi \atTree a \sexprTree
%%& ujistit se že je v části o tree reprezentacích 
%%  je napsaný co to znamená že podstrom má nějakej typ,
%%  že podstromy odpovídaj podvýrazum atd
%% & EXAMPLES of tree representations of \lterms  
%% & nějak to sjednotit s tim uplně na začátku v popisu GP kde se popisuje
%%   přímo \sexprTree a říct proč se tomu tak řiká 
%%   (na tom místě co je v textu dřív) 
%% & zkontrolovat že reference tree-reps se odkazuje na dobrý místo,
%%   mohlo by se to rozbít tim že tree reprezentations - typovany/netypovany 
%%   jsou od sebe oddelený
%\end{todo}





\addcontentsline{toc}{chapter}{Conclusion}	
	

\begin{thebibliography}{9}

% TODO bacha neni tam jednotnej formát!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


\bibitem{koza92}
  John R. Koza,
  \emph{Genetic Programming: On the Programming of Computers by Means of Natural Selection}.
  MIT Press, Cambridge, MA,
  1992. 

\bibitem{koza05}
  Koza, J.R., Keane, M., Streeter, M., Mydlowec, W.,Yu, J., Lanza, G. 
  \emph{Genetic Programming IV: Routine Human-Competitive Machine Intelligence.} 
  Springer, 2005. ISBN 978-0-387-26417-2 

\bibitem{fg}
 Riccardo Poli, William B. Langdon, Nicholas F. McPhee
 \emph{A Field Guide to Genetic Programming}.
 Lulu Enterprises, UK Ltd, 2008.

\bibitem{yu01}
  T. Yu. 
  \emph{Hierachical processing for evolving recursive and modular 
        programs using higher order functions and lambda abstractions}. 
  Genetic Programming and Evolvable Machines,
  2(4):345–380, December 2001. ISSN 1389-2576.


\bibitem{montana95}
D. J. Montana. 
\emph{Strongly typed genetic programming.} 
Evolutionary Computation, 3(2): 199–230, 1995.
%URL \url{ http://vishnu.bbn.com/papers/stgp.pdf }. nefacha

\bibitem{haynes96}
T. D. Haynes, D. A. Schoenefeld, and R. L. Wainwright. 
\emph{Type inheritance in strongly typed genetic programming.} 
In P. J. Angeline and K. E. Kinnear, Jr., editors, Advances
in Genetic Programming 2, chapter 18, pages 359–376.
MIT Press, Cambridge, MA, USA, 1996. ISBN 0-262-01158-1.\\ 
URL 
\url{http://www.mcs.utulsa.edu/~rogerw/papers/Haynes-hier.pdf}.


\bibitem{olsson94}
J. R. Olsson. 
\emph{Inductive functional programming using incremental program 
transformation and Execution of logic programs by 
iterative-deepening A* SLD-tree search.} 
Dr scient thesis, University of Oslo, Norway, 1994.

\bibitem{kes}
Forrest Briggs, Melissa O’Neill.
\emph{Functional Genetic Programming and Exhaustive
Program Search with Combinator Expressions.}
International Journal of Knowledge-based and Intelligent Engineering Systems,
Volume 12 Issue 1, Pages 47-68, January 2008. 


\bibitem{barendregt84}
H. P. Barendregt,
\emph{The Lambda Calculus: its Syntax and Semantics}, 
revised ed., North-Holland, 1984.

\bibitem{barendregt92}
H. Barendregt , S. Abramsky , D. M. Gabbay , T. S. E. Maibaum.
\emph{Lambda Calculi with Types.} 
Handbook of Logic in Computer Science, 1992. 

\bibitem{barendregt10}

  Henk Barendregt, Wil Dekkers, Richard Statman,
  \emph{Lambda Calculus With Types}.
  Cambridge University Press,
  2010. \\
  URL \url{http://www.cs.ru.nl/~henk/book.pdf}.

\bibitem{jones87}
Simon Peyton Jones. 
\emph{The Implementation of Functional Programming Languages}. 
Prentice Hall, 1987.


\bibitem{AIAMA}
	Stuart J. Russell, Peter Norvig,
	\emph{Artificial Intelligence: A Modern Approach}.
	Pearson Education,
	2003. 

\end{thebibliography}


\appendix
\chapter{Enclosed CD}

The enclosed CD contains implementation source codes,
installation instructions and some other material, 
all described in a greater detail in the \texttt{readme.txt}
file.
	
	
\end{document}





%-------------------------------------------------------------------------------
%% STARY pasaže co vnich je možna eště něco co by se dalo použit
%-------------------------------------------------------------------------------





%\section{Term generating grammar}
%
%Inference rules are good for deriving statements of the form \GMS, but our
%goal is slightly different; we would like to generate many \lterms M for a given type 
%$\sigma$ and context $\Gamma$.
%
%Our approach will be to take each inference rule and transform it to a rule of term generating
%grammar. With this term generating grammar it will be much easier to reason about generating 
%\lterms.
%	
%It won't be a grammar in classical sense because we will be operating with infinite sets of
%nonterminal symbols and rules. \footnote{TODO : mention terminal symbols - situation around 
%variables and their construction with ' symbol.}
%
%Let $Non = Type \times Context $ be our {\it nonterminal} set. 
%So for every $i \in Non$ is $i = (\sigma_i , \Gamma_i )$.
%
%\Lets consider each relevant inference rule and its corresponding grammar rule.
%
%First inference rule is {\it implication elimination} also known as 
%{\it modus ponens}: 
%\[
%	\frac{\Gamma \vdash M : \sigma \rightarrow \tau \qquad
%		  \Gamma \vdash N : \sigma }
%	     {\Gamma \vdash (M N) : \tau }
%\]
%\\
%For every $\sigma, \tau \in \mathbb{T}$ and for every {\it context} 
%$\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right)$ there is a grammar rule of a form\footnote{ 
%Terminal symbols for parenthesis and normally {\it space} now \textvisiblespace \quad (for {\it function application} operator) are visually highlighted. }: 
%\[	
%	( \tau , \Gamma )  \gar
%	\bigg( ( \sigma \rightarrow \tau , \Gamma ) 
%	  \mbox{ \Vtextvisiblespace[1em] } ( \sigma , \Gamma ) \bigg)
%\]
%\\
%
%Second inference rule is {\it implication introduction}: 
%\[
%	\frac{\Gamma \cup \{ ( x,\sigma ) \} \vdash M : \tau }
%	     {\Gamma \vdash (\lambda x . M) : \sigma \rightarrow \tau }
%\]
%\\
%$\forall \sigma, \tau \in \mathbb{T}$ 
%$\forall${\it context} $\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right) $ 
%$\forall x \in V $ such that there is no $(x,\rho) \in \Gamma$ 
%there is a grammar rule:
%\[ 
%	( \sigma \rightarrow \tau , \Gamma )  \gar
%	\bigg( \mbox{ {\Large $\lambda$ x . }}( \tau , \Gamma \cup \{ (x,\sigma) \} ) \quad \bigg)
%\]
%\\	
%
%Third inference rule is {\it axiom}: 
%\[
%		\frac{( x , \sigma )  \in \Gamma}
%		     {\Gamma \vdash x : \sigma}
%\]
%\\
%$\forall \sigma \in \mathbb{T}$ 
%$\forall${\it context} $\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right) $ 
%$\forall x \in V \cup C $ such that $(x,\sigma) \in \Gamma$ 
%there is a grammar rule:
%\[ 
%	( \sigma , \Gamma )  \gar \mbox{ {\Large x}}
%\]
%\\
%
%We will demonstrate \lterm generation on example. 
%Again on $(\lambda f . (\lambda x . (f x) ))$. 
%We would like to generate \lterm of a type 
%$(\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau)$
%with $\Gamma = \{\}$.
%\begin{align*}
%	& ((\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau),\{\}) \\ 
%	\gar & \Big( \mbox{ {\Large $\lambda$f.}}
%	  ( \sigma \rightarrow \tau , \{ (f,\sigma \rightarrow \tau) \} ) 
%	~ \Big)
%	\\
%	\gar & 
%	\Big( \mbox{ {\Large $\lambda$f. }}
%		\Big( \mbox{ {\Large $\lambda$x. }}
%	  	 	( \tau , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) 
%		~ \Big)  	 
%	~ \Big)
%	\\
%	\gar & 
%	\Big( \mbox{ {\Large $\lambda$f. }}
%		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
%	  	 	\Big( 
%	  	 	  ( \sigma \rightarrow \tau , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) 
%			  \mbox{ \Vtextvisiblespace[1em] } 
%			  ( \sigma , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} )  \Big) 
%		~ \Big)  	 
%	 ~ \Big)
%	\\
%	\gar & 
%	\Big( \mbox{ {\Large $\lambda$f. }}
%		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
%	  	 	\Big( 
%	  	 	  \mbox{ {\Large f}} 
%			  \mbox{ \Vtextvisiblespace[1em] } 
%			  ( \sigma , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) \Big) 
%		~ \Big)  	 
%	~ \Big)		
%	\\
%	\gar & 
%	\Big( \mbox{ {\Large $\lambda$f. }}
%		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
%	  	 	\Big( 
%	  	 	  \mbox{ {\Large f}} 
%			  \mbox{ \Vtextvisiblespace[1em] } 
%			  \mbox{{\Large x}} \Big) 
%		~ \Big)  	 
%	~ \Big)
%\end{align*}
%







%\section{-OLD-Inference and grammar rules producing terms in Long normal form}
%
%As is said in \cite{barendregt10} there are two
%inference/grammar rules which together generate
%simply typed lambda terms in their long normal forms.
%
%
%Inference rule 1: 
%\[
%	\frac{\Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} \vdash M : \alpha }
%	     {\Gamma \vdash (\lambda x_1 \dots x_n . M) : 
%	     \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha }
%\]
%\\
%Proof of correctness:
%\[
%	\dfrac{
%		\dfrac
%		 {\Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} \vdash M : \alpha}
%		 {\dfrac
%		   {\Gamma \cup \{ (x_1,\tau_1),\dots,(x_{n-1},\tau_{n-1})\} \cup 
%		                \{(x_n,\tau_n) \} \vdash M : \alpha}
%		   {\dfrac{\Gamma \cup \{ (x_1,\tau_1),\dots,(x_{n-1},\tau_{n-1})\}  
%		                \vdash (\lambda x_n . M) : \tau_n \rightarrow \alpha}
%				  { \vdots }		   
%		   }
%		 }		 
%	 }
%	     {\Gamma \vdash (\lambda x_1 \dots x_n . M) : 
%	     \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha }
%\]
%\\
%... there is a grammar rule:
%\[ 
%	( \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha , \Gamma )  \gar
%	\bigg( \mbox{ {\Large 
%	$\lambda x_1 \dots x_n .$ 
%	}}( \alpha , \Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} ) \quad \bigg)
%\]
%\\
%
%Inference rule 2: 
%\[
%	\frac{ (f , \rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha ) \in \Gamma \qquad
%	       \Gamma \vdash M_1 : \rho_1 \quad
%	       \dotsm \quad
%	       \Gamma \vdash M_m : \rho_m        
%	      }
%	     {\Gamma \vdash (f M_1 \dots M_m) : \alpha}
%\]
%\\
%Proof of correctness (\textbf{TODO REPAIR} Conceptually it is ok but there is sazba-bug somewhere): 
%\[
%   \dfrac
%     {\dfrac
%      {\dfrac
%       {\dfrac         
%         {\dfrac  
%          {\dfrac
%           {\boxed{(f , \rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha ) \in \Gamma}}
%           {\Gamma \vdash f : \rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha}
%           \quad
%           \boxed{\Gamma \vdash M_1 : \rho_1} }
%          {\Gamma \vdash (f M_1) : \rho_2 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha }
%          }{\vdots} 
%         \quad 
%         \ddots }
%       {\Gamma \vdash (f M_1 \dots M_{m-2}) : \rho_{m-1} \rightarrow \rho_m \rightarrow \alpha}
%       \quad
%       \boxed{\Gamma \vdash M_{m-1} : \rho_{m-1}}  }
%      {\Gamma \vdash (f M_1 \dots M_{m-1}) : \rho_m \rightarrow \alpha}       
%      \quad 
%      \boxed{\Gamma \vdash M_m : \rho_m} }
%	 {\Gamma \vdash (f M_1 \dots M_m) : \alpha}
%\]
%\\
%... there is a grammar rule:
%\[ 
%	( \alpha , \Gamma )  \gar
%	\bigg( \mbox{ {\Large f }}
%	  \mbox{ \Vtextvisiblespace[1em] } 
%	  ( \rho_1 , \Gamma )
%	  \mbox{ \Vtextvisiblespace[1em] } 
%	  \dots
%	  \mbox{ \Vtextvisiblespace[1em] } 
%	  ( \rho_m , \Gamma )
%	  \quad \bigg)
%\]


%\begin{todo}
%& SHOW correctness of those inference rules by composing them of 
%	  $E^{\rightarrow}$, $I^{\rightarrow}$ and \textit{axiom}.
%& SHOW more examples of inference rules transformed into grammar rules.
%& DESCRIBE general algorithm for this transformation.
%& TALK ABOUT $\tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$ 
%& TALK ABOUT $\beta \eta^{-1}$-normal form which is generated by this method.
%& Napsat tu (nebo asi někam jinam
%	ale hlavně někam), že de generovat přímo i termy v beta normalní 
%	formě, ale že se mi to zdálo o dost pracnější díky tomu, že
%	musíme čekovat jestli neni něco toho typu i v ne atomickejch
%	vrcholech/typech...
%\end{todo}



%\section{---OLD---Tree representations of \lterms}


%From the definition of \lterm (\ref{deflam}) we can straightforwardly derive 
%the classical tree representation for \lterms. Term M is translated into tree $T[M]$ by following rules:
%
%\begin{itemize}
%	\item $x \in V \cup C$ translates into \textit{leaf} $x$.
%	\item $(P$ $Q)$ translates into tree\\
%		\Tree
%			[.@	
%		 		\text{$T[P]$}
%		 		\text{$T[Q]$}		 			
%			] 
%	\item $\lambda x . P$ translates into tree\\
%	 	\Tree
%			[.\text{$\lambda x$}	
%		 	 	\text{$T[P]$}	
%			] 
%\end{itemize}
%
%We can enhance this representation by compressing consecutive lambda abstractions into one
%tree node like this: 
%
%\begin{itemize}
%	\item $\lambda x_1 \dots x_n . P$ translates into tree\\
%	 	\Tree
%			[.\text{$\lambda x_1 \dots x_n$ }	
%		 	 	\text{$T[P]$}	
%			] 
%\end{itemize}
%
%As this representation comes directly from definition it is evident 
%that it covers all possible \lterms.
%
%We will refer to this representation as to \textit{\atTree}.\\
% 
%
%For representing expressions as trees it is however more common to use a little different
%representation. It will also be the representation suitable for showing 
%that \textit{solving} Inhabitation tree generates wanted \lterm.
%
%\begin{itemize}
%    \item $x \in V \cup C$ translates into \textit{leaf} $x$.
%	\item $(f M_1 M_2 \dots M_n)$ where $f \in V \cup C, n \geq 1$ translates into tree\\
%		\Tree
%			[.f	
%		 		\text{$T[M_1]$}
%		 		\text{$T[M_2]$}
%		 		\text{$\dots$}
%		 		\text{$T[M_n]$}		 				 			
%			] 
%	\item $\lambda x_1 \dots x_n . M$ translates into tree\\
%	 	\Tree
%			[.\text{$\lambda x_1 \dots x_n$ }	
%		 	 	\text{$T[M]$}	
%			] 
%\end{itemize}
%
%Notice that this representation does not cover all \lterms, 
%e.g. $(\lambda x.x) y$ is not expressible in it. But it does not bother us. 
%
%We will refer to this representation as to \textit{\sexprTree}.\\ 
%
%\Lets now consider representation for \textit{typed \lterms}.
%Straightforward approach would be to add to each node a type entry which 
%would be the type of the \lterm corresponding to subtree having this
%node as the root node. 
%
%Approach more suitable for our purpose is to add a special type node above each node.
%More specifically:
%
%\Lets consider tree $t$ corresponding to a \lterm of a type
%$\sigma$ with root $r$ and subtrees $s_1 , \dots , s_n$. 
%Then corresponding tree $TT[t]$ for typed \lterm is 
%obtained from the tree $t$ as follows:  
%
%\begin{equation*}
%\mbox{ 
%TT[
%\Tree
%	[.r 	
%	  	  \text{$s_1$}
%		  \text{$s_2$}
%		  \text{$\dots$}
%		  \text{$s_n$}
%	] 
%}]=
%\mbox{
%\Tree
%	[.\text{$\sigma$ }
%	    [.r 	
%	  	  \text{$TT[s_1]$}
%		  \text{$TT[s_2]$}
%		  \text{$\dots$}
%		  \text{$TT[s_n]$}
%		]	  	
%	] 
%}
%\end{equation*}






