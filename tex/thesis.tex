\documentclass[12pt,a4paper]{report}


% Nejistý fráze který tam všude používám
% ... of a form ...
% Let us ...

%takle asi spíš psát ty anglický věty !!!!!!!!!!!!!!!!!!!!!! 
% takle nak to píše barendrecht :
%$\FV(M)$ the set of free variables of term M is defined inductively as follows.

% !!! místo we can atd psát one can ...
%      - ale ted to vypada že to neni tak žhavý pravidlo

% ujednotit i.e. , ty čaky kolem toho atd

% myslim že používam málo čárek, přečíst to a vygooglit kde mi příde že by mohla bejt

% ujednotit kurzívu a normální písmo v pseudocodu

% zčekovat: za any musí bejt plural, nejde any term ale musí bejt any terms atd
%           pro singular je a term.




\setlength\textwidth{145mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{15mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}


\usepackage[utf8]{inputenc}
\usepackage{qtree}

\usepackage{color}

\usepackage[ampersand]{easylist}

\usepackage{amssymb}

\usepackage[vlined]{algorithm2e}
%% \usepackage{algpseudocode}
\usepackage{framed}

\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}


\newcommand{\Lets}{Let us\xspace}
\newcommand{\lets}{let us\xspace}
\newcommand{\lterm}{$\lambda$-term\xspace}
\newcommand{\lterms}{$\lambda$-terms\xspace}
\newcommand{\lhead}{$\lambda$-head\xspace}
\newcommand{\lheads}{$\lambda$-heads\xspace}

\newcommand{\la}{\leftarrow\xspace}
\newcommand{\Lp}  {\Lambda^{\prime}\xspace}



\newcommand{\turst}[3]{$#1 \vdash #2 : #3$\xspace}
\newcommand{\GMS}{\turst{\Gamma}{M}{\sigma}}


\newcommand{\EA}{EA\xspace} % TODO nahradit správnym "evoluťionar algorithms"

\newcommand{\setDots}[2]{ 
	\lbrace #1 , \dots , #2 \rbrace
}


\newcommand{\Pseudokod}[2]{
	\begin{framed}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\SetKwProg{Fn}{function}{}{}
		\Fn{#1}{#2}
	\end{algorithm}
	\end{framed}
}

\newcommand{\pseudo}[1]{
	\begin{framed}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		#1
	\end{algorithm}
	\end{framed}
}

\newenvironment{enum}
{\begin{easylist}[itemize]}
{\end{easylist}}

\newenvironment{todo}
{ ~\\[0.5em]
  {\color{red}\textbf{TODO}}
  \begin{easylist}[itemize]}
{ \end{easylist}
  ~}



%% Balíček hyperref, kterým jdou vyrábět klikací odkazy v PDF,
%% ale hlavně ho používáme k uložení metadat do PDF (včetně obsahu).
%% POZOR, nezapomeňte vyplnit jméno práce a autora.
%\usepackage[ps2pdf,unicode]{hyperref}   % Musí být za všemi ostatními balíčky
%\hypersetup{pdftitle=Typed Functional Genetic Programming}
%\hypersetup{pdfauthor=Tomáš Křen}

% Tato makra přesvědčují mírně ošklivým trikem LaTeX, aby hlavičky kapitol
% sázel příčetněji a nevynechával nad nimi spoustu místa. Směle ignorujte.
\makeatletter
\def\@makechapterhead#1{
  {\parindent \z@ \raggedright \normalfont
   \Huge\bfseries \thechapter. #1
   \par\nobreak
   \vskip 20\p@
}}
\def\@makeschapterhead#1{
  {\parindent \z@ \raggedright \normalfont
   \Huge\bfseries #1
   \par\nobreak
   \vskip 20\p@
}}
\makeatother


\newcommand\Vtextvisiblespace[1][.3em]{%
  \mbox{\kern.06em\vrule height.3ex}%
  \vbox{\hrule width#1}%
  \hbox{\vrule height.3ex}}

\title{Typed Functional Genetic Programming}
\author{Tomáš Křen}
\date{Prague 2013}

\begin{document}

% Trochu volnější nastavení dělení slov, než je default.
\lefthyphenmin=2
\righthyphenmin=2

%%% Titulní strana práce

\pagestyle{empty}
\begin{center}

\large

Charles University in Prague 

\medskip

Faculty of Mathematics and Physics

\vfill

{\bf\Large MASTER THESIS}

\vfill

%%% \centerline{\mbox{\includegraphics[width=60mm]{../img/logo.eps}}}

%\begin{figure}[!ht]
%  \centering
%  \includegraphics{logo.eps}
%  \caption{Default}\label{fig:default}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stačí todle odkomentovat a dát nahoře LaTeX (F2) , pak DVI->PDF (F9)  a pak View PDF
%\includegraphics[scale=0.5]{logo.eps}
\includegraphics[scale=0.15]{logomff.png}

\vfill
\vspace{5mm}

{\LARGE Tomáš Křen}

\vspace{15mm}

% Název práce přesně podle zadání
{\LARGE\bfseries Typed Functional Genetic Programming}

\vfill

% Název katedry nebo ústavu, kde byla práce oficiálně zadána
% (dle Organizační struktury MFF UK)
%%%%Name of the department or institute
%Department of Theoretical Computer Science and Mathematical Logic\\
%{\small Department of Theoretical Computer Science and Mathematical Logic} \\
{\fontsize{0.46cm}{1em}\selectfont 
Department of Theoretical Computer Science and Mathematical Logic}

\vfill

\begin{tabular}{rl}

Supervisor of the master thesis: & RNDr. Petr Pudlák, Ph.D. \\
\noalign{\vspace{2mm}}
Study programme: & Theoretical Computer Science \\ %Teoretická informatika \\
\noalign{\vspace{2mm}}
Specialization: & 
%Neprocedurální programování a umělá inteligence \\
{\fontsize{0.3cm}{1em}\selectfont 
%Neprocedurální programování a umělá inteligence} \\
Non-Procedural Programming and Artificial Intelligence} \\
\end{tabular}

\vfill

% Zde doplňte rok
Prague 2013

\end{center}

\newpage

%%% Následuje vevázaný list -- kopie podepsaného "Zadání diplomové práce".
%%% Toto zadání NENÍ součástí elektronické verze práce, nescanovat.

%%% Na tomto místě mohou být napsána případná poděkování (vedoucímu práce,
%%% konzultantovi, tomu, kdo zapůjčil software, literaturu apod.)

%% on tam měl %% \openright

\noindent
Dedication.

\newpage

%%% Strana s čestným prohlášením k diplomové práci

\vglue 0pt plus 1fill

\noindent
I declare that I carried out this master thesis independently, and only with the cited
sources, literature and other professional sources.

\medskip\noindent
I understand that my work relates to the rights and obligations under the Act No.
121/2000 Coll., the Copyright Act, as amended, in particular the fact that the Charles
University in Prague has the right to conclude a license agreement on the use of this
work as a school work pursuant to Section 60 paragraph 1 of the Copyright Act.

\vspace{10mm}

\hbox{\hbox to 0.5\hsize{%
In ........ date ............
\hss}\hbox to 0.5\hsize{%
signature of the author
\hss}}

\vspace{20mm}
\newpage


%%% Povinná informační strana diplomové práce

\vbox to 0.5\vsize{
\setlength\parindent{0mm}
\setlength\parskip{5mm}

Název práce:
Název práce
% přesně dle zadání

Autor:
Jméno a příjmení autora

Katedra:  % Případně Ústav:
Název katedry či ústavu, kde byla práce oficiálně zadána
% dle Organizační struktury MFF UK

Vedoucí diplomové práce:
Jméno a příjmení s tituly, pracoviště
% dle Organizační struktury MFF UK, případně plný název pracoviště mimo MFF UK

Abstrakt:
% abstrakt v rozsahu 80-200 slov; nejedná se však o opis zadání diplomové práce

Klíčová slova:
% 3 až 5 klíčových slov

\vss}\nobreak\vbox to 0.49\vsize{
\setlength\parindent{0mm}
\setlength\parskip{5mm}

Title:
% přesný překlad názvu práce v angličtině

Author:
Jméno a příjmení autora

Department:
Název katedry či ústavu, kde byla práce oficiálně zadána
% dle Organizační struktury MFF UK v angličtině

Supervisor:
Jméno a příjmení s tituly, pracoviště
% dle Organizační struktury MFF UK, případně plný název pracoviště
% mimo MFF UK v angličtině

Abstract:
% abstrakt v rozsahu 80-200 slov v angličtině; nejedná se však o překlad
% zadání diplomové práce

Keywords:
% 3 až 5 klíčových slov v angličtině

\vss}

\newpage


\tableofcontents	
	
%\chapter{Introduction}
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Témata co zmínit v úvodu :\\

\begin{easylist}[itemize]
& Začít asi tim že evoluce je cool a že
  tohle bude o tom jak do GP dát typy.
& Říct jak jde text:
  && Začnem popsanim klasickýho GP podle Kozy
  && Vyslovíme definice okolo lambda termu a typu
  && Vyslovíme definici inhabitačního stromu
     kterej je stromovou verzí IM od Barendrechta.
  && Matematicky popíšem jak na to dem my.
  && Popíšem rozšíření zakladního systému na 
     polymorfní.
  && Popíšem to z programátorskýho hlediska
  && Popíšem příklady na kterejch to demonstrujem
  && Srovnáme to s nějakejma přístupama co už 
     existujou 
& Možná se zmínit o tý představě dvou pólu
  induktivním a deduktivním -
  klasickym GP a Dokazovačem, čim složitější
  typovej sytem tim víc se jedná o deduktivní   
  systém a blíží se to dokazovači.
   && Ve světle toho, je pak hezky vidět, že
      je chytrý exhaustivně systematicky prohledávat od nejmenšího
      po největší- jak budeme zesložiťovat typovej systém, tak
      čim dál víc bude už stačit najít vubec nějaký řešení a bude čim dál těžší
      jich generovat kvanta
& Nějak naťuknout už tady v úvodu jak se to má při
  srovnání s tim jak na to dou jiný (Yu atd)
& Výkřiky co zmínit:
  && Na co zaměřený:
     &&& Proof of a concept
     &&& Pořádně to vymáchat v typový teorii
     &&& Spíš než bejt další z řady nástroju
         se snaží zobecnovat ty už existující
     &&& Spíš jednoduchost
         &&&& Ve většině aspektú to sleduje
              Kozu - tzn i v málo řídících
              parametru
     &&& Duraz na generování termu (!)              
     &&& Po implementační stránce
         &&&& Sranda s JS překladem
         &&&& Servrová architektura
         &&&& oboje spíš srandy
  && Na co nezaměřený (ale snad taky ok v tom):
     &&& Performance   
     &&& Mature knihovna co by šla použít robustně
         v cizim kódu - spíš to počítá že to 
         bude dál rozvíjeno a tohle je mezizastávka
\end{easylist}

	


\chapter{Genetic Programming}
\label{GP}

\textit{Genetic programming} (GP) is a technique inspired by biological evolution
that for a given problem tries to find computer programs able to solve that problem. 
GP was developed by John Koza \cite{koza92} in 1992.

A problem to be solved is given to GP in a form of \textit{fitness function}. 
Fitness function is a function which takes computer program as its input and 
returns numerical value called \textit{fitness} as output. 
The bigger fitness of a computer program is, the better solution of a problem.

GP maintains a collection of computer programs called \textit{population}. 
A member of population is called \textit{individual}. 
By running GP algorithm evolution of those individuals is performed.

Individuals are computer program \textit{expressions} kept as \textit{syntactic trees}. 
Basically those trees are rooted trees with a function symbol in each internal node 
and with constant symbol or variable symbol in each leaf node. 
Number of child nodes for each internal node corresponds to the number of arguments of a function whose symbol is in that node.

Another crucial input besides fitness function is a collection of \textit{building blocks}.
It is collection of symbols (accompanied with an information about number of arguments).
Those symbols are used to construct trees representing individuals.  
\\\\
\Lets describe GP algorithm briefly:

At the beginning initial population is generated from building blocks randomly.

A step of GP algorithm is stochastic transformation of the current population into 	
the next population.

This step consists of two sub steps\footnote{TODO : Technically it is done in a little  
bit different fashion which is equivalent.}:
\begin{itemize} 
	\item Selection of \textit{parents} for individuals of the next population based on the fitness.
	      The bigger fitness of an individual of the current population is, 
	      the better chance of success being selected as parent it has.  
	\item Application of genetic operators (such as \textit{crossover}, 
	      \textit{reproduction} and \textit{mutation}) 
		  on parent individuals producing new individuals of the next population.  
\end{itemize}	  
This transformation is repeatedly applied for a predefined number of steps (which is called 
number of \textit{generations}) or until some predefined criterion is met.	
\\\\
\Lets now look on GP at more detail. 


\section{Program trees}

In GP programs are represented as expressions. \Lets define expression inductively:

\begin{todo}
 & Asi to přejmenvat na S-expression.  
\end{todo}

\begin{itemize}
	\item Constant\footnote{By constants we also mean procedures with zero  	
		  arguments.} or variable symbol $s$ is expression.
	\item Let there be a function with symbol $f$ which has $n$ arguments. 
	      And let there be expressions $e_{1}, ..., e_{n}$. 
	      Then ( $f$ $e_{1}$ ... $e_{n}$ ) is expression
	      \footnote{This notation comes from Lisp programming language, 
	      classical notation would be $f(e_{1}, ... ,e_{n})$. }.   
\end{itemize}

There is straightforward tree representation corresponding to these two cases:

\begin{itemize}
	\item One node tree $s$.
    \item \Tree
			[.$f$	
		 		\text{$t_{1}$}
		 		\text{...}
		 		\text{$t_{n}$} ]\\\\
		 Where $t_{1}, ..., t_{n}$ are trees corresponding to expressions $e_{1}, ..., e_{n}$.	   
\end{itemize}

Computer program with inputs $x_{1}, ..., x_{n}$ is realized as expression in which 
may occur variables $x_{1}, ..., x_{n}$.

\section{Building blocks}
\label{building-blocks}

Set of building blocks consists of two sets.

\begin{itemize}
	\item Terminal set $T$ : Set of symbols used as leaf nodes of 	               
	      program trees standing for constants and variables.
	\item Function set $F$ : Set of symbols used as internal nodes 
	      of program trees standing for functions.
\end{itemize}

\newcommand{\TuF}{$T \cup F$\xspace}

Therefore $building$ $blocks = $ \TuF.\\

Beside symbols in \TuF there must also be 
an implementation for each symbol which is not variable. 
And for every function symbol from $F$ there must be specified 
number of arguments.\\

There is one important constrain on function implementations for functions from F:
There should be type $A$ that for every function symbol $f \in F$ the corresponding function implementation standing behind this symbol should be of a type 
$A \times ... \times A \rightarrow A$ and should be total (defined on all
combinations of inputs). And analogically for $t \in T$ being of a type A.  

\begin{todo}
& koza tomu řiká closure, přečíst si tu pasáž (str 81) a 
upravit podle toho tady tu terminologii
\end{todo}

Satisfying this constrain ensures that every program tree build 
from \TuF will be total.

We will refer to this constrain as \textit{over-one-type} constrain
in the further text.\\

We can say that the motivation behind this work is to construct system where we
eliminate this constraint. 

\section{Generating individuals}
\label{GPgene}

We will describe three tree generating method described by Koza 
in \cite{koza92} called \textit{ramped half-and-half}. \\

An individual tree is constructed by random (uniform) selection of symbol from 
subset of \TuF for the root node and after that by generation of its 
subtrees recursively. 
Number of subtrees of a node corresponds 
to the number of arguments for the selected symbol. 

There are two generating sub-methods called \textit{full} and 
\textit{grow} each restricting differently the subset of \TuF. 
This restriction depends on depth of the
node for which the symbol is being selected. 

In order to perform \textit{ramped half-and-half} method
one of those two sub-methods is randomly (uniformly) 
selected and the selected one is performed.
This selection is done for each generated tree, so there
is 50\% chance to be generated by \textit{full} 
and 50\% chance to be generated by \textit{grow}.\\


A maximum depth $D_{initial}$ is defined (e.g. 6).

For each tree is selected its 
depth $d$ from $\setDots{2}{D_{initial}}$ randomly (uniformly).\\


In the root (\textit{depth} = 0) both \textit{full} and \textit{grow}
select the symbol from the set $F$.

In nodes with \textit{depth} $\leq d$ the \textit{full}
method selects from the set $F$, whereas the \textit{grow} method 
selects from whole \TuF.

In nodes with \textit{depth} $= d$ both methods select from the
set $T$.

\section{Selection}

In \EA there is plenty of options for selection mechanisms 
for us to choose from. Again we will describe mechanism Koza
used in his first book on GP. It is the \textit{Roulette selection}.
It uses fitness value of each individual in straightforward way to determine probability of selecting this individual.\\

Let there be $popSize$ individuals in the population.\\
And let $f_{i}$ be fitness value for individual i 
where $i \in \setDots{1}{popSize}$. 

Then $p_{i}$ probability of selection of individual $i$ is computed
as follows:

$$ p_{i} = \dfrac{ f_{i}  }{ \sum\limits_{j=1}^{popSize}{f_{j} }  } $$

\section{Crossover}
\label{GPxover}

For Koza the most important genetic operator in GP is 
crossover. It is operator inspired by sexual reproduction
occurring in the nature. Generally speaking crossover takes
two (parent) individuals and combines theirs genomes to produce 
two possibly new (child) individuals.   

In GP the most common crossover is \textit{Subtree swapping crossover}.
This crossover randomly selects one node in each parent tree.
Two new child individuals are constructed by swapping subtrees 
which have roots in those selected nodes.\\

Example should clarify this process. Here are two parent trees with 
selected nodes in bold:

\Tree [.$ifneq$ $1$
		 	   [.\textbf{iflt} $0$ $x$ [.$-$ $0$ $x$ ] $1$ ]
		 	   [.$+$ \text{$x$} \text{$2$} ]
		 	   $1$ ]
\Tree [.$\%$ \text{$x$}
         	 [.\textbf{ifeq} \text{$1$} \text{$x$} \text{$x$} \text{$0$} ] ]\\

And here are two child trees with swapped subtrees:

\Tree [.$ifneq$ $1$
		 	   [.\textbf{ifeq} \textbf{1} \textbf{x} \textbf{x} 
		 	     \textbf{0} ]
		 	   [.$+$ \text{$x$} \text{$2$} ]
		 	   $1$ ]
\Tree [.$\%$ \text{$x$}
         	 [.\textbf{iflt} \textbf{0} \textbf{x} 
         	   [.\textbf{-} \textbf{0} \textbf{x} ] \textbf{1} ] ]\\


\begin{todo}
 & zmínit maximální hloubku vzniklýho dítěte (17)
\end{todo}


\section{Reproduction}

Reproduction is simple mechanism providing preservation of solutions
from the current population to the next one. It simply copies 
one individual to the next population.

\section{Mutation}

Mutation is genetic operator modifying one individual.
There are many options for mutation mechanisms 
for us to choose from. Here will be described 
\textit{Subtree generating mutation} 
witch uses mechanism for generating individuals.

This mutation randomly selects one node in the individual tree.
New mutant individual is constructed by replacement of 
subtree with root in the selected node by new generated
tree. This new tree is generated by mechanism for generating 
individuals.


\section{Construction of a next population}

Let $pop_{t}$ be the current population which we want to 
transform into the next population $pop_{t+1}$. 
We start by initializing $pop_{t+1}$ by empty population.

Then we iteratively fill $pop_{t+1}$ by individuals 
returned by genetic operators.

In each iteration one genetic operator is randomly selected.
Then required amount of individuals for the operation is selected
by individual selection mechanism described above (one or two
individuals for our genetic operators).
And those selected individuals are used as input for selected
genetic operator which produces some new individuals.
Those new individuals are inserted into $pop_{t+1}$.

This process continues until $pop_{t+1}$ is filled with
$popSize$ individuals.  

Selection of genetic operation in each operation is 
controlled by probabilities for each genetic operator.\\

Koza in \cite{koza92} uses as default probabilities those values:

\begin{itemize}
	\item \textit{Crossover}    : $90\%$
	\item \textit{Reproduction} : $10\%$
	\item \textit{Mutation}     :  $0\%$
\end{itemize}



\newpage
\section{GP algorithm in pseudocode}

Bellow is described GP algorithm in pseudocode.

\Pseudokod{GP( fitness, \TuF, popSize, numGens, probabs )}{
  $gen \leftarrow$ 0 \;\;
  
  $pop \leftarrow generateInitialPopuletion( $ \TuF$ ) $ \;
  ($popWithF,terminate,best) \leftarrow$ 
  evaluate($fitness$, \TuF, $pop$)\;\; 	
	
  \While{ $gen < numGens$ $\wedge$ $\neg terminate$  }{  	
	$newPop \leftarrow$ empty population \;
	$newPop$.insert( $best$ )\;\;
	
	$i \leftarrow 1$ \;
	\While{ $i < popSize$ }{	
		$op \leftarrow probabilisticallySelectOperation(probabs)$ \;\;
				
		\Switch{op}{
		
			\Case{Crossover}{
				$parent1 \leftarrow$ selection( $popWithF$ ) \;
				$parent2 \leftarrow$ selection( $popWithF$ ) \;\;
				
				($child1$,$child2$) = crossover( $parent1$ , $parent2$ )\;\;
				
				$newPop$.insert( $child1$ ) \;
				$newPop$.insert( $child2$ ) \;
				$i \leftarrow i + 2$ \;
			}
			\Case{Reproduction}{
				$indiv \leftarrow$ selection( $popWithF$ ) \;
				$newPop$.insert( $indiv$ ) \;
				$i \leftarrow i + 1$ \;
			}
			\Case{Mutation}{
				$indiv \leftarrow$ selection( $popWithF$ ) \;
				$mutant \leftarrow$ mutate( $indiv$ , \TuF ) \;
				$newPop$.insert( $mutant$ ) \;
				$i \leftarrow i + 1$ \;	
			}		
		}
	}\;
	
	$pop \leftarrow newPop$  \;
	($popWithF,terminate,best) \leftarrow$ 
	evaluate($fitness$, $TuF$, $pop$)\;
	$gen \leftarrow gen + 1$ \; 
 }\;
 
 \Return pop \;
}

\begin{samepage}
\Lets clarify the input arguments:
\begin{itemize}
	\item \textit{fitness} - Fitness function.
	\item \textit{\TuF} - Building blocks accompanied with an  	
	      information about implementations and argument numbers.
	\item \textit{popSize} - Population size.
	\item \textit{numGens} - Number of generations.
	\item \textit{probabs} - Genetic operators probabilities.
\end{itemize} 
\end{samepage}

%In order to clarify the code let us describe in greater detail inputs and

Behaviors of contained procedures
generateInitialPopuletion(),
evaluate(),
probabilisticallySelectOperation(),
selection(),
crossover() 
and mutation() are hopefully clear from verbal description.


\begin{todo}
 & mention best preservation and same popSize checking.
 & TALK ABOUT GP is part of EA etc. and maybe define the GP by 
   defining EA and then specifying the differences or something 
   like that...    
 & History, citations, etc ....
 & ten algoritmus v kozovi dělá "Designate Result" já tam vracim 
   poslední populaci
\end{todo}




\chapter{Mathematical background}
	
\begin{todo}
 & Shrnout tu nějak ve skratce tuhle kapitolu.. 
\end{todo}
		
\section{Lambda term}
\label{deflam}

\newcommand{\then}{\Rightarrow\xspace}

\newcommand{\lamb}[2]{( \lambda \, #1 \, . \, #2 )}
\newcommand{\lam}[2]{\lambda \, #1 \, . \, #2}

\newcommand{\ST}{\mathop{\mathrm{ST}}}
\newcommand{\FV}{\mathop{\mathrm{FV}}}

\newcommand{\Scomb }{\mathbf{S}}
\newcommand{\Kcomb }{\mathbf{K}}
\newcommand{\Icomb }{\mathbf{I}}


Let $V$ be infinite countable set of {\it 
variable names}.  \\* 
Let $C$ be set of {\it constant names}.	 \\*		
Then $\Lambda$ is set of {\it \lterms} defined inductively as follows.	
\begin{align*}
x   \in V \cup C  &\then x     \in \Lambda \\
M,N \in \Lambda   &\then (M~N) \in \Lambda 
\textit{~~~~~~(Function application)} \\
x   \in V , M \in \Lambda &\then \lamb{x}{M} \in \Lambda
\textit{~~~~~($\lambda$-abstraction)} 
\end{align*}

Function application and $\lambda$-abstraction are concepts
well known from common programming languages. 
For example in JavaScript 
$(M~N)$ translates to expression \texttt{$M$($N$)} and
$\lamb{x}{M}$ translates to expression \texttt{function($x$)\{return $M$;\}}.
In other words, the function application 
corresponds to the act of supplying a function 
with an argument and
the $\lambda$-abstraction is equivalent to 
\textit{anonymous function}. \\


It is usual to use $V = \Sigma^+$ set of all non-empty finite strings of symbols 
from $\Sigma$ where $\Sigma$ is some alphabet set, e.g.  
$
\Sigma =
\setDots{a}{z}$.

And $C$ is set supplied by "the user" to enrich 
the language with constant names standing
for some predefined behavior.

A constant may stand just for another \lterm
or it may stand for some predefined constant 
such as 0,1,2,... and primitive operations on
them, e.g. addition. 
But the specific implementations 
of these constants will not be ours big concern 
while reasoning about methods for generating 
\lterms .\\

We use upper case symbols such as 
$M,N,M_1,M_2,...,N_1,N_2,...$
to denote arbitrary lambda terms in contrast with
those in lower case such as
$a,b,c,...,x,y,z,$
$f_1,f_2,...,g_1,g_2,...$
denoting variable names, elements of $V$.

Sometimes we will use lower case symbols
to denote arbitrary variable names (such as
$x$ in $\lamb{x}{M}$),
and at other times, we will use them
as specific variable names in specific terms 
(such as $x$ in $\lamb{x}{x}$).
The distinction between the two should be
clear from the context.

We use bold upper case symbols such as 
$\Scomb, \Kcomb, \Icomb$ as abbreviation
for specific terms, e.i. 
$\Icomb = \lamb{x}{x}$.  
\\
	
Examples of \lterms:

\begin{easylist}[itemize]
& $x$
& $\lamb{x}{x}$
& $\lamb{x}{f}$
& $(\lamb{x}{(x~x)}~\lamb{x}{(x~x)})$
& $\lamb{x}{\lamb{y}{x}}$
& $\lamb{f}{\lamb{g}{\lamb{x}{((f~x)~(g~x))}}}$
\end{easylist}~

We use following notation
abbreviations for better readability:

\begin{easylist}[itemize]
& $M_1~M_2~M_3~\dots~M_n$ for 
  $(\dots((M_1~M_2)~M_3)~\dots~M_n)$ 
& $\lam{x_1 x_2 \dots x_n }{M}$ for
  $\lamb{x_1}{\lamb{x_2}{\dots\lamb{x_n}{M}\dots}}$
\end{easylist}~
  
For example we can write
$\lamb{f}{\lamb{g}{\lamb{x}{((f~x)\,(g~x))}}}$
as $\lam{f\,g\,x}{f~x\,(g~x)}$.

\subsection{ Subterms, free variables and substitution }

In order to demonstrate what is a subterm of a term
\lets define inductively $\ST(M)$ set of all subterms of a term $M$.
\begin{align*}
\ST(x)          &= \{x\} \\
\ST((P~Q))      &= \{(P~Q)\} \cup \ST(P) \cup \ST(Q) \\
\ST(\lam{x}{P}) &= \{\lam{x}{P}\} \cup \ST(P) 
\end{align*}

%takle asi spíš psát ty anglický věty !!!!!!!!!!!!!!!!!!!!!! 
% takle nak to píše barendrecht :
 
$\FV(M)$ the set of free variables of a term M is defined inductively as follows.
\begin{align*}
\FV(x)          &= \{\}                        &\textbf{if } x \in C  \\
\FV(x)          &= \{x\}                       &\textbf{if } x \in V  \\
\FV((P~Q))      &= \FV(P) \cup \FV(Q)          &          \\
\FV(\lam{x}{P}) &= \FV(P) \smallsetminus \{x\} &
\end{align*}

A variable in \lterm $M$ is called \textit{bound} if it is not free.

\lterm M is called \textit{combinator} if $\FV(M)=\emptyset$.\\


Notable examples of combinators are combinators
$\Scomb$, $\Kcomb$ and $\Icomb$.
\begin{align*}
\Scomb &= \lam{f\,g\,x}{f\,x\,(g\,x)} \\
\Kcomb &= \lam{x\,y}{x} \\
\Icomb &= \lam{x}{x} 
\end{align*}

As will be shown in \ref{toSKI}
these combinators will help us in performing crossover of \lterms.\\

$M[x:=N]$ the substitution of a term $N$ for the free occurrences of 
a variable $x$ in a term $M$ is defined inductively as follows.
\begin{align*}
x[x:=N]           &= N \\
y[x:=N]           &= y \text{ ~~~~where } x \not= y  \\
(P~Q)[x:=N]       &= ( P[x:=N]  ~ Q[x:=N] ) \\
\lamb{x}{P}[x:=N] &= \lam{x}{P}\\
\lamb{y}{P}[x:=N] &= \lam{y}{( P[x:=N] )} \text{ ~~~~where } x \not= y
\end{align*}




\begin{todo}
& možná zmínit že y nesmí být v N volně v připadě\\
  $\lamb{y}{P}[x:=N] = \lam{y}{( P[x:=N] )}$ , 
  ale řekl bych že zbytečnej detail
\end{todo}


\subsection{$\beta$-reduction}

In order to perform computation there must be some
mechanism for term evaluation. In $\lambda$-calculus there
is $\beta$-reduction for this reason.\\

\newcommand{\bRedex}{$\beta$-redex\xspace}
\newcommand{\bRedexes}{$\beta$-redexes\xspace}
\newcommand{\bArrow}{\rightarrow_\beta\xspace}
\newcommand{\eArrow}{\rightarrow_\eta\xspace}
\newcommand{\eeArrow}{\rightarrow_{\eta^{-1}}\xspace}

A term of a form $\lamb{x}{M}N$ is called \textit{\bRedex}.
A \bRedex can be $\beta$-reduced to term $M[x:=N]$. 
This fact is written as \textit{relation} $\bArrow$ 
of those two terms:
\begin{equation} \label{eq:bRed}
\lamb{x}{M}N \bArrow M[x:=N]
\end{equation}
It is also possible to reduce \textit{subterm \bRedexes} 
which can be formally stated as:
\begin{align*}
P \bArrow Q &\then (R~P)      \bArrow (R~Q) \\
P \bArrow Q &\then (P~R)      \bArrow (Q~R) \\
P \bArrow Q &\then \lam{x}{P} \bArrow \lam{x}{Q}  
\end{align*}

In other words, $\beta$-reduction is the process 
of insertion of arguments supplied to a function into 
its body. 

\subsection{$\eta$-reduction}

Similarly as for $\beta$-reduction we can define $\eta$-reduction 
except that instead of \ref{eq:bRed} we use:  

$$\lamb{x}{(M~x)} \eArrow M \text{ ~~~~where } x \not\in FV(M) $$

\subsection{$\eta^{-1}$-reduction}

$\eta^{-1}$-reduction (also called $\eta$-expansion) is 
the reduction converse to $\eta$-reduction.
Again it may be obtained by replacing \ref{eq:bRed}, now with:  

$$M \eeArrow \lamb{x}{(M~x)} \text{ ~~~~where } x \not\in FV(M) $$



%\subsection{Reductions for $c \in C$}
%\textbf{TODO možná vynechat...}

\subsection{$\beta$-normal form}

A \lterm is a \textit{$\beta$-normal form} if it does not have a $\beta$-redex as
subterm.
\\\\
A normal form may be thought of as a result of a term evaluation. 



\begin{todo}
	& říct co to vubec je ty intutivně (v par větách jen) 
	  a nějak to napojit na to jak to používám já
	  pak v tom GP.
	& Citovat nějakýho bárendrechta jako zdroj podrobnějších informací
\end{todo}



\section{Type}
\label{deftype}

\newcommand{\ar}{\rightarrow\xspace}
\newcommand{\T}{\mathbb{T}\xspace}

Let $A$ be set of {\it atomic type names}. \\*
Then $\mathbb{T}$ is set of {\it types} inductively defined as follows:

\begin{align*}
\alpha      \in A  &\then   \alpha \in \T \\
\sigma,\tau \in \T &\then ( \sigma \ar  \tau ) \in \T 
\end{align*} 

Type $\sigma \ar \tau$ is type for functions taking as input
something of a type $\sigma$ and returning 
as output something of a type $\tau$. \\

We use following notation
abbreviation for better readability:\\

$\tau_1 \ar \tau_2 \ar \dots \ar \tau_n$ means 
$\tau_1 \ar (\tau_2 \ar (\dots \ar (\tau_{n-1} \ar \tau_n)\dots))$.\\

In other words, the operator $\ar$ is right-associative.\\

It is easy to see that every $\sigma \in \T$ may be expressed as 
$\tau_1 \ar \dots \ar \tau_n \ar \alpha$ 
where $\alpha \in A$ and $n \geq 0$.\footnote{ 
We can prove it by induction on size of $\sigma$. 
For $\sigma = \alpha \in \T$ is $n = 0$ and for $\sigma = \tau_0 \ar \rho$
we have from induction hypothesis that $\rho = \tau_1 \ar \dots \ar \tau_n \ar \alpha$,
therefore
$\sigma = \tau_0 \ar \tau_1 \ar \dots \ar \tau_n \ar \alpha$.}

Technically speaking, we have types only for functions with one argument, but
this property suggests how we can "simulate" function which takes $n$ arguments.
  

% We will utilize this property ... asi netřeba řikat


\begin{todo}
   & určitě promluvit nějak v úvodu k tomu že se jedná o 
	 Simply typed lambda kalkulus, zmínit že klasicky se těm alfa z A 
	 řiká proměný, ale že je to matoucí vzhledem k polymorfizmu
   & říct že se vždycky zastavěj (jak se to správně jmenuje?) ale
     asi to říct na jinym místě než tady, tady se teprv definujou typy, 
     říct to někde kde už se ví jak sou napojený na termy    
   & možná se hlobějc zmínit o curryingu  
\end{todo}


	

	
\section{Statement of a form $M : \sigma$}

	Let $\Lambda$ be set of {\it \lterms}. \\*
	Let $\mathbb{T}$ be set of {\it types}.       \\*
	A {\it statement} $M : \sigma$ is a pair $(M,\sigma) \in \Lambda \times \mathbb{T}$. \\*
	$M : \sigma$ is vocalized as {\it "$M$ has type $\sigma$"}.\footnote{ 
	$M : \sigma$ can be also imagined as $M \in \sigma$ } \\*
	The type $\sigma$ is the {\it predicate} and the term $M$ is the
	{\it subject} of the statement.  
	
\section{Context}

	Let $\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right)$. 
	($\Gamma$ is a set of {\it statements} of a form $M : \sigma$.)	\\*
	Then $\Gamma$ is {\it context} if it obeys following 
	conditions\footnote{
	The $\pi_1$ corresponds to the projection of the first component of the Cartesian product.
	}:
	\begin{align*}
		 \forall (x,\sigma) \in \Gamma &: x \in V \cup C \\
		 \forall s_1,s_2 \in \Gamma &: s_1 \neq s_2 \Rightarrow \pi_1(s_1) \neq \pi_1(s_2)
    \end{align*}
    
	In other words context is a set of statements with distinct variables or constants as subjects.
	
\begin{todo}
 & TALK ABOUT Context represents library/building blocks.
\end{todo}

	
	
\section{Statement of a form \GMS}

	By writing \GMS we say 
	{\it statement $M : \sigma$ is derivable from context $\Gamma$ }.

	We construct valid statements of form \GMS by using inference rules.
	
		
\section{Inference rule}		
	
	Basically speaking, inference rules are used for deriving statements of a form 
	\GMS from yet derived statements of such a form.
	Those inference rules are written in the following form:
	
	\begin{equation*}
		\frac{\Gamma_1 \vdash M_1 : \sigma_1 \qquad
			  \Gamma_2 \vdash M_2 : \sigma_2 \quad
			  \dotsm \quad
		      \Gamma_n \vdash M_n : \sigma_n}
		     {\Gamma_{n+1} \vdash M_{n+1} : \sigma_{n+1}}
	\end{equation*}	
	
	Suppose we have yet derived statements 
	$\Gamma_1 \vdash M_1 : \sigma_1 ,
	 \Gamma_2 \vdash M_2 : \sigma_2 ,
	 \dots ,
	 \Gamma_n \vdash M_n : \sigma_n$. 
	It allows as to use the inference rule to derive statement
	\mbox{ $\Gamma_{n+1} \vdash M_{n+1} : \sigma_{n+1}$ }.
	 
	For deriving statements including types of a form 
	$(\sigma \rightarrow \tau)$ are essential those two 
	inference rules:
	
	\begin{equation*}
		\frac{\Gamma \vdash M : \sigma \rightarrow \tau \qquad
			  \Gamma \vdash N : \sigma }
		     {\Gamma \vdash (M N) : \tau }
	\end{equation*}	
	
	\begin{equation*}
		\frac{\Gamma \cup \{ ( x,\sigma ) \} \vdash M : \tau }
		     {\Gamma \vdash (\lambda x . M) : \sigma \rightarrow \tau }
	\end{equation*}		 
	 
	This kind of inference rules allows us to derive new statements from yet derived statements, but 
	what if we do not have any statement yet? 
	For this purpose we have other kinds of inference rules such as {\it axiom} inference rule:   
	
	\begin{equation*}
		\frac{( x , \sigma )  \in \Gamma}
		     {\Gamma \vdash x : \sigma}
	\end{equation*}	
	
	\Lets consider an example statement of a form \GMS :
	
	\[
		\{\} \vdash (\lambda f . (\lambda x . (f x) )) : 
		(\sigma \rightarrow \tau) \rightarrow ( \sigma \rightarrow \tau ) 
	\]
		
	This statement is derived as follows: 
	
	\begin{equation*}
	\dfrac{
		\dfrac{ (f,\sigma \rightarrow \tau) \in \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \}  }
		     { \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \} \vdash f : \sigma \rightarrow \tau }
		\qquad
		\dfrac{ (x,\sigma) \in \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \}  }
		     { \{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \} \vdash x : \sigma }
		 }
		 {
			\dfrac{		 	
		 		\{ (f,\sigma \rightarrow \tau) , (x,\sigma)  \} \vdash (f x) : \tau
		 	}{
				\dfrac{\{ (f,\sigma \rightarrow \tau) \} \vdash (\lambda x . (f x) ) : 
				\sigma \rightarrow \tau}
				{ \{ \} \vdash (\lambda f . (\lambda x . (f x) ) ) 
				  : (\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau) }
		 	}
		 }
	\end{equation*}		



	
\section{Term generating grammar}

Inference rules are good for deriving statements of a form \GMS, but our
goal is slightly different; we would like to generate many \lterms M for a given type 
$\sigma$ and context $\Gamma$.

Our approach will be to take each inference rule and transform it to a rule of term generating
grammar. With this term generating grammar it will be much easier to reason about generating 
\lterms.
	
It won't be a grammar in classical sense because we will be operating with infinite sets of
nonterminal symbols and rules. \footnote{TODO : mention terminal symbols - situation around 
variables and their construction with ' symbol.}

Let $Non = Type \times Context $ be our {\it nonterminal} set. 
So for every $i \in Non$ is $i = (\sigma_i , \Gamma_i )$.

\Lets consider each relevant inference rule and its corresponding grammar rule.

First inference rule is {\it implication elimination} also known as 
{\it modus ponens}: 
\[
	\frac{\Gamma \vdash M : \sigma \rightarrow \tau \qquad
		  \Gamma \vdash N : \sigma }
	     {\Gamma \vdash (M N) : \tau }
\]
\\
For every $\sigma, \tau \in \mathbb{T}$ and for every {\it context} 
$\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right)$ there is a grammar rule of a form\footnote{ 
Terminal symbols for parenthesis and normally {\it space} now \textvisiblespace \quad (for {\it function application} operator) are visually highlighted. }: 
\[	
	( \tau , \Gamma )  \longmapsto
	\bigg( ( \sigma \rightarrow \tau , \Gamma ) 
	  \mbox{ \Vtextvisiblespace[1em] } ( \sigma , \Gamma ) \bigg)
\]
\\

Second inference rule is {\it implication introduction}: 
\[
	\frac{\Gamma \cup \{ ( x,\sigma ) \} \vdash M : \tau }
	     {\Gamma \vdash (\lambda x . M) : \sigma \rightarrow \tau }
\]
\\
$\forall \sigma, \tau \in \mathbb{T}$ 
$\forall${\it context} $\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right) $ 
$\forall x \in V $ such that there is no $(x,\rho) \in \Gamma$ 
there is a grammar rule:
\[ 
	( \sigma \rightarrow \tau , \Gamma )  \longmapsto
	\bigg( \mbox{ {\Large $\lambda$ x . }}( \tau , \Gamma \cup \{ (x,\sigma) \} ) \quad \bigg)
\]
\\	

Third inference rule is {\it axiom}: 
\[
		\frac{( x , \sigma )  \in \Gamma}
		     {\Gamma \vdash x : \sigma}
\]
\\
$\forall \sigma \in \mathbb{T}$ 
$\forall${\it context} $\Gamma \in \mathfrak P \left({\Lambda \times  \mathbb{T}}\right) $ 
$\forall x \in V \cup C $ such that $(x,\sigma) \in \Gamma$ 
there is a grammar rule:
\[ 
	( \sigma , \Gamma )  \longmapsto \mbox{ {\Large x}}
\]
\\

We will demonstrate \lterm generation on example. 
Again on $(\lambda f . (\lambda x . (f x) ))$. 
We would like to generate \lterm of a type 
$(\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau)$
with $\Gamma = \{\}$.
\begin{align*}
	& ((\sigma \rightarrow \tau) \rightarrow (\sigma \rightarrow \tau),\{\}) \\ 
	\longmapsto & \Big( \mbox{ {\Large $\lambda$f.}}
	  ( \sigma \rightarrow \tau , \{ (f,\sigma \rightarrow \tau) \} ) 
	~ \Big)
	\\
	\longmapsto & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}
	  	 	( \tau , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) 
		~ \Big)  	 
	~ \Big)
	\\
	\longmapsto & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
	  	 	\Big( 
	  	 	  ( \sigma \rightarrow \tau , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) 
			  \mbox{ \Vtextvisiblespace[1em] } 
			  ( \sigma , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} )  \Big) 
		~ \Big)  	 
	 ~ \Big)
	\\
	\longmapsto & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
	  	 	\Big( 
	  	 	  \mbox{ {\Large f}} 
			  \mbox{ \Vtextvisiblespace[1em] } 
			  ( \sigma , \{ (f,\sigma \rightarrow \tau) , (x,\sigma) \} ) \Big) 
		~ \Big)  	 
	~ \Big)		
	\\
	\longmapsto & 
	\Big( \mbox{ {\Large $\lambda$f. }}
		\Big( \mbox{ {\Large $\lambda$x. }}	  	 	
	  	 	\Big( 
	  	 	  \mbox{ {\Large f}} 
			  \mbox{ \Vtextvisiblespace[1em] } 
			  \mbox{{\Large x}} \Big) 
		~ \Big)  	 
	~ \Big)
\end{align*}


\section{Long normal form}
\label{lnf}

Let \GMS where $\sigma = \tau_0 \ar \tau_1 \ar \dots \ar \tau_n \ar \alpha$.\\
Then $M$ is in \textit{long normal form} if following 
conditions are satisfied:

\begin{itemize}
 \item $M$ is term of a form $\lam{x_1 \dots x_n}{f~M_1~\dots~M_n}$,
 \item each $M_i$ is in \textit{long normal form}.
\end{itemize}

As is shown in \cite{barendregt10} long normal form has those 
nice properties:

\begin{easylist}[itemize]
& Every $M$ in $\beta$-normal form has a long normal form 
$N$ such that $N \twoheadrightarrow_{\eta} M$.
& \textbf{TODO jen zkopčeno přepsat!!} .. if M has a beta-nf, which according to Theorem 2B.4 is
always the case, then it also has a unique lnf and this will be its unique $\beta\eta^{-1}$-nf.
& \textbf{TODO : potřebuju eštěnějaký z tyhle knihy?}
\end{easylist}


\begin{todo}
 & popsat tu jak ji předelat na beta nf
	pokud by se nám chtělo... 
	V barendrech84 (ta nová kniha s žlutejma deskama) je ukázano že je 
	eta CR (čurč roserovská) což snad stačí společně s 
	tim tvrzeničkem z book.pdf že vezmu lnf
	a dělám eta redukce dokud to de. až to nejde tak bych to měl mít v 
	beta-nf
 & přepsat to nějak hezčejc, ten seznam heskejch vlastností
\end{todo}





%"Barendregt-like"
\section{Inference and grammar rules producing 
terms in Long normal form}
\label{barlike}

As is said in \cite{barendregt10} there are two
inference/grammar rules which together generate
simply typed lambda terms in their long normal forms.


Inference rule 1: 
\[
	\frac{\Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} \vdash M : \alpha }
	     {\Gamma \vdash (\lambda x_1 \dots x_n . M) : 
	     \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha }
\]
\\
Proof of correctness:
\[
	\dfrac{
		\dfrac
		 {\Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} \vdash M : \alpha}
		 {\dfrac
		   {\Gamma \cup \{ (x_1,\tau_1),\dots,(x_{n-1},\tau_{n-1})\} \cup 
		                \{(x_n,\tau_n) \} \vdash M : \alpha}
		   {\dfrac{\Gamma \cup \{ (x_1,\tau_1),\dots,(x_{n-1},\tau_{n-1})\}  
		                \vdash (\lambda x_n . M) : \tau_n \rightarrow \alpha}
				  { \vdots }		   
		   }
		 }		 
	 }
	     {\Gamma \vdash (\lambda x_1 \dots x_n . M) : 
	     \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha }
\]
\\
... there is a grammar rule:
\[ 
	( \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha , \Gamma )  \longmapsto
	\bigg( \mbox{ {\Large 
	$\lambda x_1 \dots x_n .$ 
	}}( \alpha , \Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} ) \quad \bigg)
\]
\\

Inference rule 2: 
\[
	\frac{ (f , \rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha ) \in \Gamma \qquad
	       \Gamma \vdash M_1 : \rho_1 \quad
	       \dotsm \quad
	       \Gamma \vdash M_m : \rho_m        
	      }
	     {\Gamma \vdash (f M_1 \dots M_m) : \alpha}
\]
\\
Proof of correctness (\textbf{TODO REPAIR} Conceptually it is ok but there is sazba-bug somewhere): 
\[
   \dfrac
     {\dfrac
      {\dfrac
       {\dfrac         
         {\dfrac  
          {\dfrac
           {\boxed{(f , \rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha ) \in \Gamma}}
           {\Gamma \vdash f : \rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha}
           \quad
           \boxed{\Gamma \vdash M_1 : \rho_1} }
          {\Gamma \vdash (f M_1) : \rho_2 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha }
          }{\vdots} 
         \quad 
         \ddots }
       {\Gamma \vdash (f M_1 \dots M_{m-2}) : \rho_{m-1} \rightarrow \rho_m \rightarrow \alpha}
       \quad
       \boxed{\Gamma \vdash M_{m-1} : \rho_{m-1}}  }
      {\Gamma \vdash (f M_1 \dots M_{m-1}) : \rho_m \rightarrow \alpha}       
      \quad 
      \boxed{\Gamma \vdash M_m : \rho_m} }
	 {\Gamma \vdash (f M_1 \dots M_m) : \alpha}
\]
\\
... there is a grammar rule:
\[ 
	( \alpha , \Gamma )  \longmapsto
	\bigg( \mbox{ {\Large f }}
	  \mbox{ \Vtextvisiblespace[1em] } 
	  ( \rho_1 , \Gamma )
	  \mbox{ \Vtextvisiblespace[1em] } 
	  \dots
	  \mbox{ \Vtextvisiblespace[1em] } 
	  ( \rho_m , \Gamma )
	  \quad \bigg)
\]


\begin{todo}
& SHOW correctness of those inference rules by composing them of 
	  $E^{\rightarrow}$, $I^{\rightarrow}$ and \textit{axiom}.
& SHOW more examples of inference rules transformed into grammar rules.
& DESCRIBE general algorithm for this transformation.
& TALK ABOUT $\tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$ 
& TALK ABOUT $\beta \eta^{-1}$-normal form which is generated by this method.
& Napsat tu (nebo asi někam jinam
	ale hlavně někam), že de generovat přímo i termy v beta normalní 
	formě, ale že se mi to zdálo o dost pracnější díky tomu, že
	musíme čekovat jestli neni něco toho typu i v ne atomickejch
	vrcholech/typech...
\end{todo}





\newpage
\section{Inhabitation tree}

Now we will introduce \textit{Inhabitation tree}, structure slightly different from
\textit{Inhabitation machine}, which was introduced in \cite{barendregt10} by Barendregt.
We can think about Inhabitation tree as about unfolded Inhabitation machine.
The motivation for using Inhabitation trees is belief that it will help us 
reason about generation of \lterms 
of a given type $\sigma$ and with a given context $\Gamma$.  

\subsection{Definition of Inhabitation tree}

\textit{Inhabitation tree} is a \textit{rooted tree}, possibly infinite. 
It has two types of nodes:

\begin{samepage}
\begin{itemize}
  \item Type nodes   
  			- containing type $\sigma \in \mathbb{T}$ 
  			- aka "OR-node" %, Nonterminal-node. 
  \item Symbol nodes 
  			%- containing $ x^{*} \in \mathfrak P \left(V\right) -blobost pač ordered $ or $c \in C$
  			- containing "$\lambda$-head" (nonempty finite sequence of variable names) or constant
  			  name. 
  			- aka "AND-node" %, Terminal-node. - ale nemusí, proto jsem to zakomentoval
\end{itemize}
\end{samepage}

We construct Inhabitation tree for given type $\sigma$ and context $\Gamma$.\\
We will define Inhabitation tree by describing its construction for a given $(\sigma,\Gamma)$.
Notice that it will closely follow the rules from \ref{barlike}:

\begin{itemize}
\item The root of Inhabitation tree for $(\sigma,\Gamma)$ is 
      \textit{type node} with $\sigma$ as type.
\item All \textit{type nodes} have as child nodes only \textit{symbol nodes}. 
\item And all \textit{symbol nodes} have as child nodes only \textit{type nodes}. 
\end{itemize}

\begin{samepage}
Now we will resolve the child nodes of the root node.
There are two cases of $\sigma$ (recall \ref{deftype}): 
\begin{description}
	\item[Atomic type] 
		$\sigma = \alpha $ where $\alpha \in A$. 
	\item[Function type] 
		$\sigma = \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$
		where $n \geq 1, \alpha \in A$.
\end{description}
\end{samepage}


First case \textbf{Atomic type} --- i.e., $\sigma = \alpha$ where $\alpha \in A$:\\
For every $(f,\rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \alpha) \in \Gamma$
where $\alpha \in A$ there is a child \textit{symbol node} of the root containing constant name $f$.
This symbol node containing $f$ has $m$ child subtrees corresponding to Inhabitation trees for 
$(\rho_1,\Gamma),\dots,(\rho_m,\Gamma)$.   

~

Compare this case with corresponding grammar rule:
\[ 
	( \alpha , \Gamma )  \longmapsto
	\bigg( \mbox{ {\Large f }}
	  \mbox{ \Vtextvisiblespace[1em] } 
	  ( \rho_1 , \Gamma )
	  \mbox{ \Vtextvisiblespace[1em] } 
	  \dots
	  \mbox{ \Vtextvisiblespace[1em] } 
	  ( \rho_m , \Gamma )
	  \quad \bigg)
\]

Second case \textbf{Function type} --- i.e., 
$\sigma = \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$
where $n~\geq~1, \alpha~\in~A$:\\
For every  $i \in \{1,\dots,n\}$ we create new \textit{variable name} $x_i$ which is not yet included in context $\Gamma$ as variable or constant name.
 
There is one and only one child \textit{symbol node} of the root containing "$\lambda$-head" 
$\lambda x_1 \dots x_n$ which stands for sequence of variable names $(x_1,\dots,x_n)$.
This symbol node containing $\lambda x_1 \dots x_n$
has one and only one child subtree corresponding to Inhabitation trees for 
$(\alpha,\Gamma \cup \{ (x_1,\tau_1) , \dots , (x_n,\tau_n) \})$.   

~

Compare this case with corresponding grammar rule:
\[ 
	( \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha , \Gamma )  \longmapsto
	\bigg( \mbox{ {\Large 
	$\lambda x_1 \dots x_n .$ 
	}}( \alpha , \Gamma \cup \{ (x_1,\tau_1),\dots,(x_n,\tau_n) \} ) \quad \bigg)
\]
~\\
\Lets consider following $(\sigma,\Gamma)$ as a simple example:

\begin{align*}
\sigma =& ~ \mathbb{B} \rightarrow  \mathbb{B} \rightarrow  \mathbb{B} \\ 
\Gamma =& \{ ~ true : \mathbb{B}  \\
        &  , ~ nand :  \mathbb{B} \rightarrow \mathbb{B} \rightarrow \mathbb{B} ~ \}
\end{align*}

This particular $(\sigma,\Gamma)$ results in the following tree:\\

\Tree
[.\text{ $\mathbb{B} \rightarrow \mathbb{B} \rightarrow \mathbb{B}$ }  
	[.\textbf{$\lambda$x_1 x_2 } 
		[.\text{ $\mathbb{B}$ } 
			\textbf{true}  
			[.\textbf{nand} 
				\qroof{ ~~ $\dotsb$ ~~ }.\text{ $\mathbb{B}$ }
				\qroof{ ~~ $\dotsb$ ~~ }.\text{ $\mathbb{B}$ } 
			]
			\textbf{x_1}
			\textbf{x_2}
		]
	]
]

~

Second example features our well known example:

\begin{align*}
\sigma =& ~ (\sigma \rightarrow \tau) \rightarrow \sigma \rightarrow \tau \\ 
\Gamma =& \{ \}
\end{align*}

Which results in following tree:

\Tree
[.\text{ $(\sigma \rightarrow \tau) \rightarrow \sigma \rightarrow \tau $ } 
	[.\textbf{$\lambda$ f x }	
		[.\text{ $\tau$ }		
			[.\textbf{f} 
				[.\text{ $\delta$ }
					\textbf{x}					
				]
			]
		]
	]
] 


\subsection{And-or tree and searching in Inhabitation tree}
\Lets consider following definition of 
\textit{And-or tree}\footnote{
\textbf{TODO}: Mention that on WIKI there is more general definition, but 
for our purposes is this one sufficient.}:

\textit{And-or tree} is a rooted tree where each node is labeled as either \textit{and-node} 
or\footnote{xor} \textit{or-node}.

By \textit{solving} And-or tree $T$ we mean finding finite tree $T'$
subgraph of $T$ such that it follows these conditions: 
\begin{itemize}
	\item The root of $T'$ is the root of $T$.
	\item Each \textit{and-node} in $T'$ has all the child nodes as in $T$.
	\item Each \textit{or-node}  in $T'$ has precisely one child 
	      node.\footnote{\textbf{TODO}: MENTION why precisely one 
	      and not at least one ..or CHANGE the def. }   
\end{itemize}
~
\Lets now consider following labeling of Inhabitation tree: 

\begin{itemize}
  \item \textbf{Type nodes}   are labeled as \textbf{or-nodes}.   
  \item \textbf{Symbol nodes} are labeled as \textbf{and-nodes}.
\end{itemize}

This labeling has following justification: 

Selection of exactly one
child node in \textit{type node} corresponds to selection of exactly one
grammar rule in order to rewrite nonterminal symbol.  

Selection of all the child nodes in \textit{symbol node} corresponds to 
rewriting all the nonterminal symbols in string that is being generated.  

The motivation for defining \textit{solving} of a And-or tree the way we did is that
a found tree $T'$ corresponds to generated \lterm. 
In order to understand this correspondence let's now talk about various tree representations
of \lterms.

\section{Tree representations of \lterms}
\label{tree-reps}

\newcommand{\sexprTree}{sexpr-tree\xspace}
\newcommand{\atTree}{@-tree\xspace}



From the definition of \lterm (\ref{deflam}) we can straightforwardly derive 
the classical tree representation for \lterms. Term M is translated into tree $T[M]$ by following rules:

\begin{itemize}
	\item $x \in V \cup C$ translates into \textit{leaf} $x$.
	\item $(P$ $Q)$ translates into tree\\
		\Tree
			[.@	
		 		\text{$T[P]$}
		 		\text{$T[Q]$}		 			
			] 
	\item $\lambda x . P$ translates into tree\\
	 	\Tree
			[.\text{$\lambda x$}	
		 	 	\text{$T[P]$}	
			] 
\end{itemize}

We can enhance this representation by compressing consecutive lambda abstractions into one
tree node like this: 

\begin{itemize}
	\item $\lambda x_1 \dots x_n . P$ translates into tree\\
	 	\Tree
			[.\text{$\lambda x_1 \dots x_n$ }	
		 	 	\text{$T[P]$}	
			] 
\end{itemize}

As this representation comes directly from definition it is evident 
that it covers all possible \lterms.

We will refer to this representation as to \textit{\atTree}.\\
 

For representing expressions as trees it is however more common to use a little different
representation. It will also be the representation suitable for showing 
that \textit{solving} Inhabitation tree generates wanted \lterm.

\begin{itemize}
    \item $x \in V \cup C$ translates into \textit{leaf} $x$.
	\item $(f M_1 M_2 \dots M_n)$ where $f \in V \cup C, n \geq 1$ translates into tree\\
		\Tree
			[.f	
		 		\text{$T[M_1]$}
		 		\text{$T[M_2]$}
		 		\text{$\dots$}
		 		\text{$T[M_n]$}		 				 			
			] 
	\item $\lambda x_1 \dots x_n . M$ translates into tree\\
	 	\Tree
			[.\text{$\lambda x_1 \dots x_n$ }	
		 	 	\text{$T[M]$}	
			] 
\end{itemize}

Notice that this representation does not cover all \lterms, 
e.g. $(\lambda x.x) y$ is not expressible in it. But it does not bother us. 

We will refer to this representation as to \textit{\sexprTree}.\\ 

\Lets now consider representation for \textit{typed \lterms}.
Straightforward approach would be to add to each node a type entry which 
would be the type of the \lterm corresponding to subtree having this
node as the root node. 

Approach more suitable for our purpose is to add a special type node above each node.
More specifically:

\Lets consider tree $t$ corresponding to a \lterm of a type
$\sigma$ with root $r$ and subtrees $s_1 , \dots , s_n$. 
Then corresponding tree $TT[t]$ for typed \lterm is 
obtained from the tree $t$ as follows:  

\begin{equation*}
\mbox{ 
TT[
\Tree
	[.r 	
	  	  \text{$s_1$}
		  \text{$s_2$}
		  \text{$\dots$}
		  \text{$s_n$}
	] 
}]=
\mbox{
\Tree
	[.\text{$\sigma$ }
	    [.r 	
	  	  \text{$TT[s_1]$}
		  \text{$TT[s_2]$}
		  \text{$\dots$}
		  \text{$TT[s_n]$}
		]	  	
	] 
}
\end{equation*}

\begin{todo}

& ujistit se že je v části o tree reprezentacích 
  je napsaný co to znamená že podstrom má nějakej typ,
  že podstromy odpovídaj podvýrazum atd

 & říct že to je přesně ta notace co používá koza
 & EXAMPLES of tree representations of \lterms  
 & napsat transformace mezi \atTree a \sexprTree
 & nějak to sjednotit s tim uplně na začátku v popisu GP kde se popisuje
   přímo \sexprTree a říct proč se tomu tak řiká 
   (na tom místě co je v textu dřív)
 
\end{todo}


\section{ Generating terms in Long normal form by solving Inhabitation tree }
	  	
Now we can finally put the pieces together. 
Every solution to an Inhabitation tree has this just described tree form of a typed \lterm. 

 
\begin{todo}
& !!!
& TALK (more?) ABOUT "Barendregt-like" subsection \ref{barlike}\\
		Things about $\beta\eta^{-1}$ normal form, etc.   
\end{todo}


\section{ Inhabitation machine }

\begin{todo}
  & DESCRIBE Inhabitation Machine...
   && něco jako na závěrještě ve stručnosti popišme 
      IM, strukturu ze který je odvozenej IT. 
  & říct, že je to hezkej konstrukt vzhledem k tomu že je konečnej
  & ale je popsanej velmi zrychla (v tý knize) a implementační
     detaily jsou celkem vynechaný
  & na stromový verzi jsou implementační detaily hezky vidět
  & a navíc pro polymorfní verzi jsou už i IM nekonečný takže
     se strácí i ta konečnostní výhoda
  & dát tam obrázek 
\end{todo}


		

\chapter{ Design of our system }	

In this chapter we will describe our system trying to 
dive into implementation details as little as possible.
Some of those details will be revealed in the next chapter
devoted to the implementation. 

This chapter will be
presented in the mix between the mathematical terminology 
described in the previous chapter and the algorithmic terminology.\\

We can summarize our approach to design of 
the system for Genetic programming with types
by its main design goals:\\ 

\begin{enum}
 & It should eliminate the \textit{over-one-type} constrain of classical GP
   (see \ref{building-blocks}). 
 & It should be generalization of the classical GP described in \ref{GP},
   by which is meant that for \textit{building blocks} satisfying 
   constrains of classical GP there should be simple setup of control parameters
   of the system which will make the system behave in precisely the same way
   as the classical GP behaves. 
 & It should utilize the theory around typed $\lambda$-calculus.  
\end{enum}

\section{Individuals and Building blocks}

\begin{todo}
 & několik fází, několik reprezentací
 & během generování se reprezentace mění 
   && nejdří obsahuje lambdy i volný proměnný pak už ne
      &&& vygeneruje se v \sexprTree
      &&& ta se převedena na \atTree aby jsme provedli abstraction elimination
 & dvě možnosti jak uchovávat jedince po vygenerování
   && \sexprTree
      &&& currently implemented
   && \atTree
      &&& currently not implemented, ale neni problém dodělat
      &&& DOĎELAT! :)
   && ale de mezi nima volně převádět
      &&& díky tomu že nemaj lambda abstrakce, jinak by museli bejt ve 
          WHNF/HNF nebo něco takovýho
   && dusledky pro křížení
      &&& \atTree umožňuje víc křížení než \sexprTree
          &&&& každý co de v \sexprTree umí i \atTree ale obráceně to neplatí
      &&& \atTree je zase ekvivalentní tomu kozovskýmu  
 & říct že bilding blocks je prostě kontext
\end{todo}

\section{Generating individuals}

%\begin{todo}
%& Dřív se to menovalo 
%"Our approach to \textit{solving} Inhabitation tree"
%tak tu napsat pěknej uvod o tom, že naši jedinci jsou lambda
%termy a že je  budeme generovat pomocí 
%řešení inhabitačních stromu.
%\end{todo}


Input for term generating algorithm is pair $(\sigma,\Gamma)$ where:\\

\begin{enum}
 & $\sigma$ is desired type of each generated individual.
 & $\Gamma$ is \textit{building blocks} context. 
\end{enum}~
 
Each generated \lterms $M$ should  satisfy \GMS. \\

We generate terms in long normal form.
This is achieved by \textit{solving} inhabitation tree for $(\sigma,\Gamma)$.  
The main design choice behind our approach to solving inhabitation trees
is ability to be variable enough to enable choice between 
\textit{systematic}  
and \textit{ramped half-and-half} (described in \ref{GPgene}) 
methods of generating terms 
by choice of simple search strategy.\\

\Lets explain what is meant by \textit{systematic} 
method of generating terms. 

Systematic way of solving inhabitation tree corresponds to
generating terms in their long normal forms
in order from smallest to largest in number of symbols and \lheads.

We will achieve this by using \textit{A* algorithm}. 

%That means minimizing number of and-nodes in 
%and-or-tree view of point on inhabitation tree.
%But since inhabitation tree is a special kind of 
%and-or-tree with alternating layers of 
%and-nodes (symbols and \lheads) and or-nodes 
%(types) it is also minimizing number of 
%size of whole inhabitation/and-or tree. 


\subsection{A* algorithm}

A* is a general informed search algorithm used for finding least-cost 
path from a given start state to some goal state in a state space.
According to \cite{AIAMA} A* is the most widely known form of best-first search.

By state space is meant oriented weighted graph with states as vertices and edges 
labeled with numbers corresponding to distance or cost. 
Edge from $s_i$ to $s_j$ means that state 
$s_j$ is reachable from state $s_i$ in one step with cost $dist(s_i,s_j)$.
In pseudocode we will refer to set of states reachable from 
state $s$ in one step as to $s.nexts()$.

Typically we use A* algorithm to find path from one specific start state to another 
specific goal state. But sometimes  
we can be interested only in finding a goal state which 
is specified by some $isGoal$ predicate give to the algorithm as
input (and the state space may contain many such goal states). 
This later variant corresponds to our situation, 
so we will continue by describing this variant.

As A* traverses the state space, it follows a path of the lowest 
expected total cost. The expectation is based on \textit{heuristic}
function which for state $s$ predicts distance to the nearest goal state. 
It uses a priority queue of states as its crucial data structure.
In this queue the priority of state $s$ is 
the total expected cost of the path from start state 
going through $s$ and continuing to the nearest goal state. \\

Here follows A* algorithm written in pseudocode:  

\Pseudokod{A*( start , isGoal , heuristic )}{
	$open \la$ empty priority queue \;
	$closed \la$ empty set \;\;
	
	$start$.G = 0 \;
	$start$.F = 0 + $heuristic$( $start$ ) \;\;
	
	$open$.insert( $start$ )\;\;	
	
	\While{$\neg$ $open$.isEmpty() }{
		$state \la$ $open$.popStateWithLowestF()\;\;
		
		\If{ isGoal( $state$ ) }{
			\Return $state$ \;
		}\;
		
		closed.insert( $state$ ) \;\;
		
		\For{$next \in$ $state$.nexts() }{		
		    $newG \la$ $state$.G + dist($state$,$next$)\;\;

			\If{ $( next \not\in open \wedge next \not\in closed ) \vee newG < next.$G}{\;
			
				$next$.G $\la newG$                          \;
				$next$.F $\la newG + heuristic$( $next$ )  \;\;
				
				\If{ $next \not\in open$ }{
					$open$.insert( $next$ )\;
				}
					
			
			}						
		}
	}\;
	
	\Return no-reachable-goal 
}	

Now we will modify this general A* algorithm so
it will be more suited for our needs. We want following 
properties:\\

\begin{enum}
& The state space is a tree.
& $dist(s_i,s_j) = 1$
& We do not want to find one, but $n$ goal states. 
\end{enum}~

For state space which is a tree the condition
$(next \not\in open \wedge next \not\in closed)$ 
is always true, since it is possible
to come to a state only from its parent state. 
This fact results in simplification of algorithm's code and behavior;
we may completely omit the $closed$ set and the checks in the for loop.\\

Those changes result in the following code:
 
\Pseudokod{Our-A*( start , n , isGoal , heuristic )}{	

	$open    \la$ empty priority queue \;
	$results \la$ empty set\;\;	
	
	$start$.G = 0 \;
	$start$.F = $heuristic$( $start$ ) \;\;
	
	$open$.insert( $start$ )\;\;	
	
	\While{$\neg$ $open$.isEmpty() }{
	
		$state \la$ $open$.popStateWithLowestF()\;\;
		
		\If{ isGoal( $state$ ) }{
			
			$results$.insert( $state$ )\;
						
			\If{ $\vert results \vert = n$ }{
				\Return $results$ \;
			}
		}\;
		
		\For{$next \in$ $state$.nexts() }{
			$next$.G $\la $state$.G + 1$ \;
			$next$.F $\la $next$.G + heuristic$( $next$ )  \;
			$open$.insert( $next$ )\;			
		}
	}\;
	
	\Return results
}


The most important part of A* algorithm is the heuristic function.
A heuristic function must be \textit{admissible} in order to make
A* algorithm find the shortest possible path.

A heuristic function $h$ is said to be \textit{admissible} if it satisfies 
following condition for every state $s$:
$$ h(s) \leq h^{*}(s) $$

where $h^{*}(s)$ is the true length of the shortest path from $s$ to 
the nearest goal state.

In other words, a heuristic function is \textit{admissible} if it is
\textit{optimistic}.

\subsection{Our state space}

% - Stavový prostor 
%   Nedokončený term
%   Navíc unfin typ gama
%   my jen podmnozinu urcenou funkci nexts() a vsemi triv koreny unfin typ gama
%   Vzdy vezmeme nejlevejsi unfin a udelame z nej konkrétnější term pomoci 
%   vyberu v or uzlu tim nam vzniknou 
%   nove unfin uzly odpovidajici pozadavkum and uzlu.
%   Protoze vyplnovani jednotlivych vetvi je na sobe nezavisle 
%   tak je jedno v jakem poradi je budem vyplnovat 
%   a tak je nase "z leva doprava" ok.
% - isFinal(state)
%   Term který neobsahuje unfin uzel
% - Heuristika 
%   simple -pocet unfin uzlů. 
%   Lepsi - predpocitat si to chytre s gama' a udelat si mapu v kery kdyz to neni tak dat 1.
% - Vic nez 1 reseni
%   staci pokracovat pri nalezeni reseni.



In order to describe our state space to search in \lets introduce
extended version of our tree representation for typed \lterms from \ref{tree-reps}.
This extension consists in adding a new kind of leaf node $(\sigma,\Gamma)$ 
standing for unfinished tree of a type $\sigma$ with context $\Gamma$ to use
as set of building blocks
\footnote{\textbf{TODO}: Tady to napsat nějak líp s tim gamma }.\\

Initial start states are all of a form "one node tree $(\sigma,\Gamma)$" since we
are trying to generate a typed \lterm of a type $\sigma$ from building blocks $\Gamma$.\\

A tree with no unfinished leaf is considered as final state.  \\

We will define our state space inductively by specifying algorithm for obtaining 
successors of a state.

A non-final state must have one or more unfinished leafs. 
One of those unfinished leafs is selected. 
It is the first one found by depth-first search (from the root of a state tree)
i.e., the leftmost unfinished leaf (if we consider that tree written as expression).

Let $(\sigma,\Gamma)$ be the selected leaf. Successor state is constructed 
by replacing this leaf by new subtree. We must distinguish two cases of $\sigma$ :
\textit{Atomic type} $\sigma$ and \textit{function type} $\sigma$.\\


\textbf{Atomic type} $\sigma = \alpha $, $\alpha \in A$.

For atomic $\sigma$ there are as many successors as there are members of $\Gamma$ of a form 
$(f,\rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \boldsymbol{\alpha} )$,
where $m \geq 0 $. In other words all the members of $\Gamma$ of a type
$\alpha$ or of a type for function which returns $\alpha$.

For each 
$(f,\rho_1 \rightarrow \dots \rightarrow \rho_m \rightarrow \boldsymbol{\alpha} ) \in \Gamma$,
where $m > 0$ the new subtree which will replace $(\sigma,\Gamma)$ has following form: \\

\Tree
   [.$\alpha$
	[.f	
 		\text{$(\rho_1,\Gamma)$}
 		\text{$(\rho_2,\Gamma)$}
 		\text{$\dots$} 		
 		\text{$(\rho_m,\Gamma)$}		 				 			
	]   
   ]\\

And for m = 0:

\Tree [.$\alpha$ f ] \\


\textbf{Function type} 
$\sigma = \tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$
where $n \geq 1, \alpha \in A$.

For function type $\sigma$ there is exactly one successor.

For every  $i \in \{1,\dots,n\}$ we create new \textit{variable name} $x_i$ which is not yet used anywhere in the whole tree.
%included in context $\Gamma$ as variable or constant name.

The new subtree which will replace $(\sigma,\Gamma)$ has following form: \\

\Tree
   [.\text{$\tau_1 \rightarrow \dots \rightarrow \tau_n \rightarrow \alpha$}
	[.\text{$\lambda x_1 \dots x_n$}	
 		\text{$(\alpha,\Gamma \cup \{ (x_1,\tau_1) , \dots , (x_n,\tau_n) \})$}		 				
	]   
   ]\\[3em]

We should discuss that it is correct to have as successors of a state those
created by expansion of the leftmost unfinished leaf (in contrast with
more broad successor set where any unfinished leaf may be expanded).
What we need to show is that no final state will be omitted by the method 
and that there is no shorter path omitted. 

One can see that it is correct by considering that 
the order of expanding unfinished leafs is irrelevant 
since expansion of each unfinished leaf is 
independent from expansion of any other unfinished leaf
(up to renaming of variables which is irrelevant).

Every unfinished leaf of a state must be replaced by 
tree containing no unfinished leafs in order to be final state. 
But since expansion of an unfinished leaf has no impact 
(besides new variable names) on expansion of other unfinished
leafs we may choose the order of expansions arbitrarily. \\

We prefer this variant because:
 \begin{enum}
 	& It has smaller or equal number successors for each state, 
 	  i.e. smaller or equal branching factor.
 	& It makes the state space a tree, in contrast with
 	  the naive state space which is not a tree. 
 \end{enum}

 
%\begin{todo}
%Mužem na to koukat jako na takovou tu redukci simetrií, 
%najít to v AIAMA a pořádně popsat.
%\end{todo}



\subsection{Our heuristic function}

A heuristic function takes as input a state and returns as output an estimation of number
of steps needed to get to a final state by the shortest possible path.

Our heuristic function is a simple one: The number of unfinished leafs in the state tree.


\begin{todo}
& rozebrat to, ukázat že je optimistická atd...
\end{todo}

\subsection{Search strategy}

Now we will describe further modification of the A* algorithm 
which is generalizing it enough to enable use of \textit{ramped half-and-half}
term generating method and other term generating methods.\\


It is done by adding one more input argument for the algorithm.
This additional argument is a search strategy whose purpose is
to determine which successor states of all possible successor states 
will be added to the priority queue, i.e., a search strategy 
is there to filter out some successor states from $state.nexts()$.
This filtering may be non-deterministic. It is based on
the depth of expanded unfinished leaf and on properties of 
the newly added subtree, e.g., whether the newly added subtree
contains one or more unfinished leafs (so it corresponds to non-terminal) 
or not (so it corresponds to terminal). 

By \textit{depth} we really mean $\lfloor depth/2 \rfloor$, because our 
formal state trees contain those type nodes above each symbol node. 
\\


In previous situation where there is no filtering of states involved 
we know that if queue becomes empty, then we have exhaustively searched
all possible states. 

But since filtering is involved this is no longer true.
Therefore, if the queue becomes empty before the required number of
terms is generated, then the generating process is restarted and 
it continues to generate terms.

We also enable strategy to have some internal state which is 
initialized (possibly non-deterministically) each time the 
generating process restarts. \\

Hopefully those changes will be clarified by the following pseudocode:

\Pseudokod{A*-with-strategy( start , n , isGoal , heuristic , strategy )}{	

	$results \la$ empty set\;\;	
	

	\While{ $\vert results \vert < n$ }{ 		
		
		$strategy.init()$\;\;
		
		$open    \la$ empty priority queue \;\;
		
		
		$start$.G = 0 \;
		$start$.F = $heuristic$( $start$ ) \;\;
		
		$open$.insert( $start$ )\;\;	
		
		\While{$\neg$ $open$.isEmpty() }{
		
			$state \la$ $open$.popStateWithLowestF()\;\;
			
			\If{ isGoal( $state$ ) }{
				
				$results$.insert( $state$ )\;
							
				\If{ $\vert results \vert = n$ }{
					\Return $results$ \;
				}
			}\;
			
			$nexts \la strategy.filter$( $state$.nexts() , $state$.depth() ) \;\;			
			
			\For{$next \in nexts$ }{
				$next$.G $\la $state$.G + 1$ \;
				$next$.F $\la $next$.G + heuristic$( $next$ )  \;
				$open$.insert( $next$ )\;			
			}
		}
	}\;
	
	\Return results
}

\Lets show examples of search strategies. 
They are described by describing 
their $filter(nexts,depth)$ 
and $init()$ methods. Again, in order to avoid confusion, 
\lets stress that $depth$ is really $\lfloor depth/2 \rfloor$ since
our formal states contain above each symbol node a type node.

\subsubsection{Systematic strategy}

The previous systematic behavior may be obtained by trivial 
$filter$ function which returns all $nexts$.
Systematic strategy needs no internal state, 
therefore $init$ is doing nothing.

\subsubsection{Ramped half-and-half strategy}

In the $init$ are randomly (uniformly) initialized two variables of the
strategy internal state:

\begin{enum}
 & \textit{isFull} - A boolean value, determining whether \textit{full}
                     or \textit{grow} method will be performed.
 & \textit{d} - A integer value from $\setDots{2}{D_{init}}$, where 
                $D_{init}$ is predefined maximal depth (e.g. 6).                    
\end{enum}~


The method $filter(nexts,depth)$ returns precisely one randomly
(uniformly) selected  element from a subset of $nexts$
(or zero elements if $nexts$ is empty). 
This means that the queue always contains only one (or zero) state.

The subset to select from is determined by $depth, d$ and $isFull$.\\


Those elements of $nexts$ whose newly added subtree contains one ore more 
unfinished leafs are regarded as \textit{non-terminals}, whereas 
those whose newly added subtree contains no unfinished leaf are regarded as 
\textit{terminals}.\\


If $depth = 0$, then the subset to select from is  
set of all \textit{non-terminals} of $nexts$.

If $depth = d$, then the subset to select from is
set of all \textit{terminals} of $nexts$.\\


In other cases of $depth$ it depends on value of $isFull$.

If $isFull = true$, then the subset to select from is 
set of all \textit{non-terminals} of $nexts$.

If $isFull = false$, then the subset to select from is 
whole $nexts$ set.

\begin{todo}
& eště se musí nějak prohrotit to, že tam je navíc ta lambda hlava
kerá ještě o něco kazí tu $depth$
& ukázat že je to fakt to samý co kozovský klasický
& zavýst tam pojem běhu pro ten jeden vnější while cyklus
\end{todo}



\subsubsection{Geometric strategy}


We can see those two previous strategies as two extremes on the spectrum of 
possible strategies.

\textit{Systematic strategy} filters no successor state thus performing
exhaustive search resulting in discovery of $n$ smallest terms in one run.

On the other hand, \textit{ramped half-and-half strategy} filters 
all but one successor states resulting in degradation of 
priority queue to "fancy variable". 
And each term is generated in its own run.\\

\textit{Geometric strategy} is simple yet fairly effective term generating 
strategy somewhere in the middle of this spectrum.

It is parameterized by parametr $q \in (0,1)$. 

For each element of $nexts$ it is probabilistically decided whether
it will be returned or omitted. A probability $p$ of returning is
same for all elements, but depends on $depth$. 
It is computed as follows:

$$ p = q^{depth} $$
   
This formula is motivated by idea that it is important to
explore all possible root symbols, but as the $depth$ 
increases it becomes less "dangerous" to omit 
an exploration branch. 

We can see this by considering that this strategy results in
somehow undisciplined A* search.
With each omission we make the search space smaller. But with
increasing depth these omissions have smaller impact on the search space,
i.e., they cut out lesser portion of the search space.

Another slightly esoteric argument supporting this formula is that "root 
parts" of a program usually stand for more crucial parts
with radical impact on global behavior of a program, 
whereas "leaf parts" of a program usually
stand for less important local parts (e.g. constants).  

It also plays nicely with the idea that "too big trees should be killed".\\



Systematic strategy does not have an internal state, thus $init$ does nothing.



\begin{todo}
 
& sjednotit terminologii nexts successors atd
 
& Odiskutovat nějak tohle:
přirozeně chceme při produkci velkýho množství termu obhospodařovávat nějaký
velký množství rozdělanejch termu v nějaký struktuře a postuipně je dodělávat.
Proto přichází jako tahle struktura na mysl nějaká prioritní fronta,
otázka je kde vzít ty priority a A* je jednoduchá ale přitom optimum hledající 
odpověď na tuhle otázku, dostatečně volná skrz tu heuristiku.  

& určitě někde zmínit ospravedlnění proč si napsat dokazovač sám
a né požít nějakej už hotovej - de o to, že dokazovače jsou delaný na to aby
udělali jeden dukaz typicky složitýho tvrzení, my sme v situaci kdy
chceme co nejvíc (a co možná co nejvíc rozmanitejch) dukazu jednoduchýho problému. 

& nekde zmínit že s těma nexts nezacházíme takle dementně, že samozřejmě vybíráme 
jen z těch přidávanejch podstromu ale že tim nechcem komplikovat popis 
 
\end{todo}


\subsection{Other tried approach to generating individuals}

\begin{todo}
 & popsat tu ten puvodní přístup, kde se bralo přímo 
   zavedení a eliminace pro každej typovej konstrukt 
   a podle toho se pak udělali příslušný term generating grammar pravidla
   kerý odpovídaj manipulacímse strommem
 & ale má nevýhody
   && hlavně to neni v normální formě, takže generujeme spoustu 
      ruzně dopočítanejch beta-rovnáse-ekvivalentních termu
   && u pravidla zavedení implikace neni uplně zřejmý
      co tam použít za typ
   && takže sice to na to ze začátku běhalo ale nakonec to bylo vyměněno
      za IM metodu since ta je jednoduší, líp se přemejšlí o ní,
      konstrukty jako and a or nemusíme zavádět na urovni pravidel
      ale stačí tam dat vhodný polymorfní konstruktory/destruktory 
 & nějak tu metodu pomenovat a přemenovat podle toho nadpis
\end{todo}



\newpage
\section{Crossover}

The crossover operation in classical GP is performed 
by swapping randomly selected subtrees in each parent 
\sexprTree (see \ref{GPxover}).

For typed lambda terms two difficulties arise: Types and variables.

We will show how to crossover typed \lterm trees in both 
\atTree and \sexprTree notation.\\

As in classical GP our crossover will be performed by swapping
two subtrees. But now with constraint that both subtrees have
the same type. This is discussed in grater detail in \ref{typed-swapping}.\\

Variables bring more difficulties then types do.
This problem arises from variables that are free in subterms corresponding 
to swapped subtrees. 

Following example illustrates the problem. \Lets have these two
parent trees with selected nodes in bold.\\

\newcommand{\lh}[1]{\lambda #1}

\Tree [.$\lh{x_1}$ [.f [.$\lh{x_2}$ [.\textbf{g} $x_2$ c ] ] $x_1$ ] ]
\Tree [.$\lh{x_1}$ [.h $x_1$ $\mathbf{x_1}$ ] ]

~\\The swap of subtrees results in following trees:\\

\Tree [.$\lh{x_1}$ [.f [.$\lh{x_2}$ $\mathbf{x_1}$ ] $x_1$ ] ]
\Tree [.$\lh{x_1}$ [.h $x_1$ [.\textbf{g} $\mathbf{x_2}$ \textbf{c} ] ] ]
 
~\\The problem is that variable $x_2$ in second tree
is not bound by any $\lambda$-head and since
it is not element of $\Gamma$, the second tree is not well-typed \lterm.  

% - This is only one example of a problem caused by swapping subtrees with variables,
% - Resolve problems with free variables or avoid variables completely. 


In \ref{repairing-method} is presented more detailed discussion of problems with
variables and method which repairs such "broken" terms. 

But we can avoid dealing with this problem by avoiding use of variables.
This can be achieved by process called abstraction elimination.
 

\subsection{Abstraction elimination}
\label{toSKI}
%... Conversion to SKI combinators

\textit{Abstraction elimination} is a process of transforming 
an arbitrary \lterm into \lterm witch contains no lambda abstractions
and no bound variables.
The newly produced \lterm may contain function applications, 
free symbols from former \lterm and some new symbols standing for 
combinators $\Scomb$, $\Kcomb$ and $\Icomb$. \\

Those combinators are defined as:
\begin{align*}
\Scomb &= \lam{f\,g\,x}{f\,x\,(g\,x)} \\
\Kcomb &= \lam{x\,y}{x} \\
\Icomb &= \lam{x}{x} 
\end{align*}

\newcommand{\Ae}{\mathop{\mathrm{\AE}}}

\Lets describe transformation $\Ae$ performing this process.
\begin{align*}
\Ae[x]           &= x &\\[0.4em]
\Ae[\,(M\,N)\,]  &= (\Ae[M]\;\;\Ae[N]) &\\[0.4em]
\Ae[\lam{x}{x}]  &= \Icomb &\\
\Ae[\lam{x}{M}]  &= (\Kcomb~\Ae[M]) &\textbf{if } x \not\in \FV(M)\\
\Ae[\lam{x}{\lamb{y}{M}}] &= \Ae[\lam{x}{\Ae[\lam{y}{M}]}]  
&\textbf{if } x \in \FV(M)\\
\Ae[\lam{x}{(M\,N)}] &= (\Scomb~\Ae[\lam{x}{M}]~\Ae[\lam{x}{N}])  
&\textbf{if } x \in \FV(M) \vee x \in \FV(N)\\
\end{align*}


\begin{todo}
& přidat (ale i do kódu) B a C
& říct že my musíme řešit i typy 
& říct o složitosti , a že je to docela
disadventič vzhledem k bloatu
& citovat Simon Peyton Jones implementaition of FP languages
jako source of further reading on this subject.
\end{todo}


\subsection{Typed subtree swapping}
\label{typed-swapping}


\subsection{ Repairing method }
\label{repairing-method}

\begin{todo}
& !!!
\end{todo}


\subsection{ Comparison of the two methods }
\label{comarison-ski-repairing}

\begin{todo}
& !!!
& říct že nakonec sme přilnuli k zbavit se proměnejch
\end{todo}

\newpage
\section{Mutation}

\begin{todo}
& říct že na tu sme se moc nesoustředili ze dvou duvodu
  ktery to ospravedlňujou 
	&& koza taky
	&& ta s vygereovanim podstromu je v zásadě použití
	   algoritmu na generovaní jedince a tak nám přišlo
	   lepší pořádně se soustředit na generování jedince a
	   začít do toho mutaci tahat až bude generování pořádně
	   prozkoumáno.
& To ale neznamená že jí nemáme radi, naopak se těšíme
  že její vhodný zavedení celou metodu ještě zlepší.
  
& Nápad na to jak to dělat:
  && nutno ošetřit - co všechno do gammy alemyslim že v phodě když je uplne 
     stejná jako ta generujicí, pač ve vy SKI-ovanym termu stejne nejsou zadny 
     proměný 
  && mít nějakej předem danej počet pohybu na frontě, abych mohl efektivně
     kontrolovat že to přidání mutací moc nespomalý
  && Po tom počtu pohybu končim generování
  && mam nějakej počet vygenerovanejch termu
  && pro chytrou mutaci mužu chtít třeba 10 termu
     &&& pokud se mi jich vygenerovalo míň, tak beru míň
     &&& pokud se mi jich vygenerovalo víc, tak vemu 10 náhodně
     &&& těch 10 vyhodnotim (jakože dosazenejch na jejich místo)
     &&& nejlepší si vezmu
     &&& dobrý udělat to zase najednou pro všechny v tý generaci mutovaný
         pánčto je nákladný spouštět Hint
& Další mutace - beta eta redukce (?? asi moc naroční viz dale) 
  && zmenšující mutace
  && udělaná že převedem SKIBC do lambda termu a to redukujem? to asi ne pač
     pak převod to spíš prodlouží, spíš asi mít redukční pravidla pro SKIBC
     a těma redukovat, ale je otázka jak moc tam budou vznikat SKIBC-redexi
     - ty mohou vznikat jen při @-tree reprezentaci
  && to otevírá otázku toho, že by elementy gammy mohla mít k sobě redukční pravidla
& Další mutace - jednoduše náhrada podstromu/uzlu za nějakej z gammy
  && nezvětšující mutace
& Další mutace - mutace číselnejch-like konstant
& To mě přivádí k myšlence sice pro generování ale myslim podstatný:
  jakmile jsou v problému číselný-like konstanty tak mužu udělat pro 
  jeden tvar hodně jedincu - což šetří jak čas na generování, tak to prozkomává 
  víc danej tvar


  
\end{todo}



\subsection{Using term generation}




\section{ Complexity discussion }

\begin{todo}
& !!!
& zmínit že mi moc neřešíme formální/asymptotický složitosti
  && de nám hlavně o realný časy
& vyjít z komplexnosti A* asi
  && složitost výrazně ovlivněná kvalitou heuristický funkce
\end{todo}


\chapter{Polymorphic variant}

\begin{todo}
 & poznamky o čem se mimojiné zmínit:
   && $(\$) : (\alpha \ar \beta) \ar \alpha \ar \beta$ 
      jako alternativa k ADF ! Myslim že hodně zajimavý
\end{todo}

\chapter{ Implementation }	

Předběžný náhodně uspořádaný body osnovy ..\\

\begin{easylist}[itemize]
& že je to v haskellu
&& Jak moc hrotit vysvětlení haskellu?
& že je to servrově řešený
& architektura evolučního algoritmu 
&& ty tři typový třídy a ta čtvrtá typová třída
& evaluace
&& vychytavka s vyhodnocenim celýho seznamu aby s eto nemuselo furt inicializovat - inicializace vyhodnocovače jednou za generaci 
& hlavní soubory
&& Eva.hs     - Eva monada
&& GP-Core.hs - Evoluční algoritmus
&& GP-Data.hs - Instance tříd z GP-Core
&& IMx.hs - Term generating
&& TTerm.hs   - typed lambda term
&& TTree.hs   - CTT
&& Server.hs  - main serveru
&& Job.hs     - Registrace problemu
&& Heval.hs   - Evaluator 
&& složka Problems
&&& pro každej problém složka 
&&& ta typicky strukturu
&&&& Problem.hs
&&&& Funs.hs
&& Slozka server
&&& index.html
&&& slozka js/Problems
&&& .js pro kazdej problem
\end{easylist}




\section{Top level view}
\section{Comments about main source files}
 \subsubsection{ Eva.hs }
 \subsubsection{ GP\_{}Core.hs }
\section{Term generating}
\section{Crossover}
\section{Mutation}




\chapter{Problems}
	In this section will be presented usage of the system in order to solve specific problems.
		
	
		\section{Even Parity Problem}
		\section{Big Context}
		\section{Fly}
		\section{Simple Symbolic Regression}
		\section{Artificial Ant}
		\section{Boolean Alternate}
		
\chapter{Comparison with others}

\chapter{Ideas yet not implemented}

\section{Roadmap}
\section{Tree ants}
\section{[Propojení těch dvou barandrechtskejch odvozovacích pravidel do jedinýho]}
\section{[Chytřejší heuristika pro A* která si to předpočítá na $\overline{\Gamma}$ ]}
\section{[šlechtění těch strategií na generování termů]}
\section{[šlechtění fitness funkcí a nápad s "turnajem olympioniků"]}
\section{[šlechtění search strategií pro prohledávání]}




%\chapter{Conclusion}
\chapter*{Conclusion}

\textbf{Poznámky :\\
bylo by asi mimojiný dobrý 
ospravedlnit/okomentovat tak fancy (=javascript frontend) 
implementaci při 
dost nedodělanejch jinejch věcech (heuristika by 
mohla být lepší, mutace by mohli bejt, atd)
ale věřil sem, že je to proces iterativní
a že je po celou dobu dobrý mít na čem to testovat.
Javascript byl vybranej pač HTML je nejpromáklejší
UI jaký vubec je na zemi teď (state of dzí art)
a tak to snad nebyla stráta času.
navíc se naskitla díky tomu další pěkná příležitost
k ukázání toho, že to neni omezený na haskell
ale že se kombinatorový konstrukty daj v zásadě přeložit do libovolnýho jazyka celkem bez problémů
nebo minimalně do JS. Ale todle možná nepsat tolik do závěru
spíš nějaky podrobnosti přehodit do implementačních detailu
a tady to jen tak líznout.}



\addcontentsline{toc}{chapter}{Conclusion}	
	

\begin{thebibliography}{9}


\bibitem{koza92}

  John R. Koza,
  \emph{Genetic Programming: On the Programming of Computers by Means of Natural Selection}.
  MIT Press, Cambridge, MA,
  1992. 

\bibitem{barendregt10}

  Henk Barendregt, Wil Dekkers, Richard Statman,
  \emph{Lambda Calculus With Types}.
  Cambridge University Press,
  2010. \\
  \url{http://www.cs.ru.nl/~henk/book.pdf}


\bibitem{AIAMA}

	Stuart J. Russell, Peter Norvig,
	\emph{Artificial Intelligence: A Modern Approach}.
	Pearson Education,
	2003. 

\end{thebibliography}

	
	
\end{document}
